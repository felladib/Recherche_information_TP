{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.1.3 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\asyncio\\base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\asyncio\\base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\ryan\\AppData\\Local\\Temp\\ipykernel_10408\\2440574508.py\", line 1, in <module>\n",
      "    import nltk\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\nltk\\__init__.py\", line 133, in <module>\n",
      "    from nltk.collocations import *\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\nltk\\collocations.py\", line 36, in <module>\n",
      "    from nltk.metrics import (\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\nltk\\metrics\\__init__.py\", line 18, in <module>\n",
      "    from nltk.metrics.association import (\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\nltk\\metrics\\association.py\", line 26, in <module>\n",
      "    from scipy.stats import fisher_exact\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\scipy\\stats\\__init__.py\", line 606, in <module>\n",
      "    from ._stats_py import *\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\scipy\\stats\\_stats_py.py\", line 37, in <module>\n",
      "    from scipy import sparse\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\scipy\\__init__.py\", line 134, in __getattr__\n",
      "    return _importlib.import_module(f'scipy.{name}')\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\importlib\\__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\scipy\\sparse\\__init__.py\", line 295, in <module>\n",
      "    from ._csr import *\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\scipy\\sparse\\_csr.py\", line 11, in <module>\n",
      "    from ._sparsetools import (csr_tocsc, csr_tobsr, csr_count_blocks,\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.1.3 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\asyncio\\base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\asyncio\\base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\ryan\\AppData\\Local\\Temp\\ipykernel_10408\\2440574508.py\", line 1, in <module>\n",
      "    import nltk\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\nltk\\__init__.py\", line 133, in <module>\n",
      "    from nltk.collocations import *\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\nltk\\collocations.py\", line 36, in <module>\n",
      "    from nltk.metrics import (\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\nltk\\metrics\\__init__.py\", line 38, in <module>\n",
      "    from nltk.metrics.scores import (\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\nltk\\metrics\\scores.py\", line 15, in <module>\n",
      "    from scipy.stats.stats import betai\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\scipy\\stats\\__init__.py\", line 606, in <module>\n",
      "    from ._stats_py import *\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\scipy\\stats\\_stats_py.py\", line 37, in <module>\n",
      "    from scipy import sparse\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\scipy\\__init__.py\", line 134, in __getattr__\n",
      "    return _importlib.import_module(f'scipy.{name}')\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\importlib\\__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\scipy\\sparse\\__init__.py\", line 295, in <module>\n",
      "    from ._csr import *\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\scipy\\sparse\\_csr.py\", line 11, in <module>\n",
      "    from ._sparsetools import (csr_tocsc, csr_tobsr, csr_count_blocks,\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.1.3 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\asyncio\\base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\asyncio\\base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\ryan\\AppData\\Local\\Temp\\ipykernel_10408\\2440574508.py\", line 1, in <module>\n",
      "    import nltk\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\nltk\\__init__.py\", line 146, in <module>\n",
      "    from nltk.chunk import *\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\nltk\\chunk\\__init__.py\", line 155, in <module>\n",
      "    from nltk.chunk.api import ChunkParserI\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\nltk\\chunk\\api.py\", line 15, in <module>\n",
      "    from nltk.parse import ParserI\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\nltk\\parse\\__init__.py\", line 100, in <module>\n",
      "    from nltk.parse.transitionparser import TransitionParser\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\nltk\\parse\\transitionparser.py\", line 17, in <module>\n",
      "    from scipy import sparse\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\scipy\\__init__.py\", line 134, in __getattr__\n",
      "    return _importlib.import_module(f'scipy.{name}')\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\importlib\\__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\scipy\\sparse\\__init__.py\", line 295, in <module>\n",
      "    from ._csr import *\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\scipy\\sparse\\_csr.py\", line 11, in <module>\n",
      "    from ._sparsetools import (csr_tocsc, csr_tobsr, csr_count_blocks,\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import os\n",
    "import math\n",
    "from collections import defaultdict\n",
    "from nltk import FreqDist\n",
    "import numpy as np\n",
    "\n",
    "STOPWORDS = set(nltk.corpus.stopwords.words('english'))\n",
    "PORTER_STEMMER = nltk.PorterStemmer()\n",
    "LANCASTER_STEMMER = nltk.LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../Collections'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcule de poids "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_processing_args():\n",
    "    tokenization = \"Split\"\n",
    "    normalization = \"None\",\n",
    "    file_type = \"TPD\"\n",
    "    return tokenization, normalization, file_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(doc_path, tokenization, normalization):\n",
    "    with open(doc_path, 'r') as file:\n",
    "        text = file.read()\n",
    "        \n",
    "    # Tokenization\n",
    "    if tokenization == \"Split\":\n",
    "        tokens = text.split()\n",
    "    else:\n",
    "        exp_reg = nltk.RegexpTokenizer(r'\\d+(?:\\.\\d+)?x\\d+|\\d+(?:\\.\\d+)|\\w+(?:-\\w+)*|(?:[A-Z]\\.)+|\\w+')\n",
    "        tokens = exp_reg.tokenize(text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    tokens = [term for term in tokens if term.lower() not in STOPWORDS]\n",
    "\n",
    "    # Normalization\n",
    "    if normalization == \"Porter\":\n",
    "        tokens = [PORTER_STEMMER.stem(term) for term in tokens]\n",
    "    elif normalization == \"Lancaster\":\n",
    "        tokens = [LANCASTER_STEMMER.stem(term) for term in tokens]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_global_term_frequencies(tokenization, normalization):\n",
    "    global_term_frequencies = defaultdict(int)\n",
    "\n",
    "    for doc_name in os.listdir(path):\n",
    "        doc_path = os.path.join(path, doc_name)\n",
    "        tokens = preprocessing(doc_path, tokenization, normalization)\n",
    "        unique_terms = set(tokens)\n",
    "\n",
    "        for term in unique_terms:\n",
    "            global_term_frequencies[term] += 1\n",
    "            \n",
    "    return global_term_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TPD_result(query, terms_freq, global_term_frequencies, N):\n",
    "    max_freq = max(terms_freq.values())\n",
    "    results=[]\n",
    "    for idx, (term, freq) in enumerate(terms_freq.items(), start=1):\n",
    "        poids = (freq / max_freq) * math.log10((N / global_term_frequencies[term]) + 1)\n",
    "        results.append((idx, term, query, freq, round(poids, 4)))\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_input(query, normalization):\n",
    "    # appliquer le traitement sur la requete\n",
    "    if normalization == \"Porter\":\n",
    "        query = PORTER_STEMMER.stem(query) \n",
    "    elif normalization == \"Lancaster\":\n",
    "        query = LANCASTER_STEMMER.stem(query) \n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nb_termes_glob(tokenization, normalization):\n",
    "    nb_termes_global = []\n",
    "    for doc_name in os.listdir(path):\n",
    "        doc_path = os.path.join(path, doc_name)\n",
    "        # Appliquer le prétraitement pour obtenir les tokens du document\n",
    "        tokens = preprocessing(doc_path, tokenization, normalization)\n",
    "        \n",
    "        # Ajouter les tokens du document à la liste globale\n",
    "        nb_termes_global.extend(tokens)\n",
    "\n",
    "    # Obtenir le nombre de termes uniques\n",
    "    termes_uniques = np.unique(nb_termes_global)\n",
    "    print(\"Termes uniques : \", termes_uniques)  # Optionnel : pour visualiser les termes uniques\n",
    "    \n",
    "    return len(termes_uniques)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nb_termes_glob(tokenization, normalization):\n",
    "    nb_termes_global = []\n",
    "    for doc_name in os.listdir('Collections'):\n",
    "        doc_path = os.path.join('Collections', doc_name)\n",
    "        # Appliquer le prétraitement pour obtenir les tokens du document\n",
    "        tokens = preprocessing(doc_path, tokenization, normalization)\n",
    "        \n",
    "        # Ajouter les tokens du document à la liste globale\n",
    "        nb_termes_global.extend(tokens)\n",
    "\n",
    "    # Obtenir le nombre de termes uniques\n",
    "    termes_uniques = np.unique(nb_termes_global)\n",
    "    print(\"Termes uniques : \", termes_uniques)  # Optionnel : pour visualiser les termes uniques\n",
    "    \n",
    "    return len(termes_uniques)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_processing(query,tokenization, normalization, file_type):\n",
    "    \n",
    "    # tokenization, normalization, file_type = get_processing_args()\n",
    "    nb_terms = 0\n",
    "    global_term_frequencies = build_global_term_frequencies(tokenization, normalization)  # Calculate global term frequencies\n",
    "    N = len(os.listdir(path))\n",
    "    results =[]\n",
    "    if file_type == \"TPD\":\n",
    "        doc_path = os.path.join(path, f\"{query}.txt\")\n",
    "        tokens = preprocessing(doc_path, tokenization, normalization)\n",
    "        nb_terms = len(np.unique(tokens))\n",
    "        terms_freq = FreqDist(tokens)\n",
    "        \n",
    "        result = TPD_result(query, terms_freq, global_term_frequencies, N)\n",
    "        \n",
    "        return result , nb_terms\n",
    "        \n",
    "    else :\n",
    "        query = process_input(query, normalization)\n",
    "        i=0\n",
    "        for doc_name in os.listdir(path):\n",
    "            doc_path = os.path.join(path, doc_name)\n",
    "            Tokens = preprocessing(doc_path, tokenization, normalization)\n",
    "            terms_freq = FreqDist(Tokens)\n",
    "\n",
    "            max_freq = max(terms_freq.values())\n",
    "            for term, freq in terms_freq.items():  \n",
    "                if term == query:  # Check if the term is the specific query term\n",
    "                    poids = ((freq / max_freq) * math.log10((N / global_term_frequencies[term]) + 1))\n",
    "                    i+=1\n",
    "                    results.append((i, term, os.path.splitext(doc_name)[0], freq, round(poids, 4)))\n",
    "        \n",
    "        return results , nb_terms\n",
    "   \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(query):\n",
    "    # if raw:\n",
    "    doc_path = os.path.join(path, f\"{query}.txt\")\n",
    "    with open(doc_path, 'r') as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "    # elif processed:\n",
    "    #     text_processing(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*le produit scalaire ( Le modele vectoriel)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* D1 : {'effet' : 0,0795, 'distribution' : 0,0663}\n",
    "* D3 : {'effet' : 0,1193, 'distribution' : 0,0995}\n",
    "* D4 : {'distribution': 0,0332}\n",
    "* D6 : {'effet' : 0,1193, 'distribution' : 0,0995}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculer_relevance(query, tokenization, normalization, file_type , path ):\n",
    "    # Créer une liste des documents à partir des fichiers dans le dossier\n",
    "    docs = [doc.split('.')[0] for doc in os.listdir(path)]\n",
    "    \n",
    "\n",
    "    relevance_dict = {doc: 0 for doc in docs}\n",
    "    for term in query.split():\n",
    "        occurrence, _ = text_processing(term, tokenization, normalization, file_type)\n",
    "        print(\"Occurrences:\", occurrence)\n",
    "        # exemple de occurrence : [(1, 'effect', 'D1', 1, 0.0795), (2, 'effect', 'D3', 1, 0.1193), (3, 'effect', 'D6', 1, 0.1193)]\n",
    "        \n",
    "        for occ in occurrence:\n",
    "            doc_name = occ[2]  # Nom du document contenant le terme\n",
    "            poids_terme = occ[4]  # Poids du terme dans ce document\n",
    "            relevance_dict[doc_name] += poids_terme\n",
    "            \n",
    "    return relevance_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Occurrences: [(1, 'effect', 'D1', 2, 0.1326), (2, 'effect', 'D2', 1, 0.0398), (3, 'effect', 'D3', 1, 0.0995), (4, 'effect', 'D6', 3, 0.2985)]\n",
      "Occurrences: [(1, 'distribut', 'D1', 1, 0.0663), (2, 'distribut', 'D3', 1, 0.0995), (3, 'distribut', 'D4', 1, 0.0332), (4, 'distribut', 'D6', 1, 0.0995)]\n",
      "Occurrences: [(1, 'wing', 'D1', 3, 0.301), (2, 'wing', 'D6', 2, 0.301)]\n",
      "Occurrences: [(1, 'slipstream', 'D1', 5, 0.7042)]\n",
      "Occurrences: [(1, 'experi', 'D1', 1, 0.1408)]\n",
      "Occurrences: [(1, 'investig', 'D1', 1, 0.1003), (2, 'investig', 'D2', 1, 0.0602)]\n",
      "Occurrences: [(1, 'aerodynam', 'D1', 1, 0.1003), (2, 'aerodynam', 'D6', 1, 0.1505)]\n"
     ]
    }
   ],
   "source": [
    "relevance = calculer_relevance('effect distribution wing slipstream experiment investig aerodynam','Split', 'Porter', 'DPT',path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'D1': 1.5455, 'D2': 0.1, 'D3': 0.199, 'D4': 0.0332, 'D5': 0, 'D6': 0.8495}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modèle basé sur la Similarité Cosinus (Cosine Measure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "\n",
    "def calculer_relevance_cosinus(query, tokenization, normalization, file_type, path):\n",
    "    # Créer une liste des documents à partir des fichiers dans le dossier\n",
    "    docs = [doc.split('.')[0] for doc in os.listdir(path)]\n",
    "    \n",
    "    # Initialiser un dictionnaire pour stocker les poids des termes pour chaque document\n",
    "    doc_vectors = {doc: {} for doc in docs}\n",
    "\n",
    "    # Remplir les vecteurs des documents\n",
    "    for term in query.split():\n",
    "        # Récupérer les occurrences du terme dans les documents\n",
    "        occurrence, _ = text_processing(term, tokenization, normalization, file_type)\n",
    "        print(f\"Occurrences pour le terme '{term}':\", occurrence)\n",
    "        \n",
    "        # Ajouter les poids des termes aux vecteurs des documents\n",
    "        for occ in occurrence:\n",
    "            doc_name = occ[2]  # Nom du document\n",
    "            poids_terme = occ[4]  # Poids du terme dans ce document\n",
    "            doc_vectors[doc_name][term] = poids_terme\n",
    "            \n",
    "            \n",
    "    print(doc_vectors)\n",
    "    # exemple : {'D1': {'effect': 0.0795, 'distribution': 0.0663}, 'D2': {}, 'D3': {'effect': 0.1193, 'distribution': 0.0995}, 'D4': {'distribution': 0.0332}, 'D5': {}, 'D6': {'effect': 0.1193, 'distribution': 0.0995}}\n",
    "    \n",
    "    # Calculer la similarité cosinus entre la requête et chaque document\n",
    "    relevance_dict = {}\n",
    "    for doc_name, doc_vector in doc_vectors.items():\n",
    "     \n",
    "        # Produit scalaire (query_vector · doc_vector), où chaque poids de query est 1 ( somme (vi *wi))\n",
    "        \n",
    "        dot_product = sum(doc_vector.get(term, 0) for term in doc_vector)\n",
    "        \n",
    "        # Norme du vecteur de la requête (sqrt(nombre de termes))\n",
    "        # utilisation de len parceque le vecteur norm est un vecteur boolean\n",
    "        # sqrt(som wi)\n",
    "        norm_query = math.sqrt(len(doc_vector))\n",
    "        print(norm_query)\n",
    "        \n",
    "        # Norme du vecteur du document (somme poidsi++2)\n",
    "        # for weight in doc_vector.values():\n",
    "        #     print(weight)\n",
    "        norm_doc = math.sqrt(sum(weight ** 2 for weight in doc_vector.values()))\n",
    "        # print(norm_doc)\n",
    "        # Similarité cosinus (éviter la division par zéro)\n",
    "        if norm_query > 0 and norm_doc > 0:\n",
    "            similarity = dot_product / (norm_query * norm_doc)\n",
    "\n",
    "        else:\n",
    "            similarity = 0\n",
    "        \n",
    "        # Ajouter le résultat à la pertinence pour ce document\n",
    "        relevance_dict[doc_name] = similarity\n",
    "\n",
    "    return relevance_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Occurrences pour le terme 'effect': [(1, 'effect', 'D1', 1, 0.0795), (2, 'effect', 'D3', 1, 0.1193), (3, 'effect', 'D6', 1, 0.1193)]\n",
      "Occurrences pour le terme 'distribution': [(1, 'distribution', 'D1', 1, 0.0663), (2, 'distribution', 'D3', 1, 0.0995), (3, 'distribution', 'D4', 1, 0.0332), (4, 'distribution', 'D6', 1, 0.0995)]\n",
      "{'D1': {'effect': 0.0795, 'distribution': 0.0663}, 'D2': {}, 'D3': {'effect': 0.1193, 'distribution': 0.0995}, 'D4': {'distribution': 0.0332}, 'D5': {}, 'D6': {'effect': 0.1193, 'distribution': 0.0995}}\n",
      "1.4142135623730951\n",
      "0.0\n",
      "1.4142135623730951\n",
      "1.0\n",
      "0.0\n",
      "1.4142135623730951\n"
     ]
    }
   ],
   "source": [
    "relevance_dict= calculer_relevance_cosinus('effect distribution','Split', 'None', 'DPT',path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'D1': 0.9959267318751248,\n",
       " 'D2': 0,\n",
       " 'D3': 0.9959304316119252,\n",
       " 'D4': 1.0,\n",
       " 'D5': 0,\n",
       " 'D6': 0.9959304316119252}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevance_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modèle basé sur l'Indice de Jaccard (Jaccard Measure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Occurrences: [(1, 'effect', 'D1', 1, 0.0795), (2, 'effect', 'D3', 1, 0.1193), (3, 'effect', 'D6', 1, 0.1193)]\n",
      "Occurrences: [(1, 'distribution', 'D1', 1, 0.0663), (2, 'distribution', 'D3', 1, 0.0995), (3, 'distribution', 'D4', 1, 0.0332), (4, 'distribution', 'D6', 1, 0.0995)]\n",
      " doc vectors : {'D1': {'effect': 0.0795, 'distribution': 0.0663}, 'D2': {}, 'D3': {'effect': 0.1193, 'distribution': 0.0995}, 'D4': {'distribution': 0.0332}, 'D5': {}, 'D6': {'effect': 0.1193, 'distribution': 0.0995}}\n",
      "vecteur query {'effect': 0.3181, 'distribution': 0.2985}\n",
      "{'D1': 0.11398376765566398, 'D2': 0, 'D3': 0.17941098224334792, 'D4': 0.0343009847771403, 'D5': 0, 'D6': 0.17941098224334792}\n"
     ]
    }
   ],
   "source": [
    "def calculer_jaccard_similarity(query, tokenization, normalization, file_type, path):\n",
    "    docs = [doc.split('.')[0] for doc in os.listdir(path)]\n",
    "    relevance_dict = {doc: 0 for doc in docs}\n",
    "    \n",
    "    # Construction des vecteurs de poids pour chaque document et pour la requête\n",
    "    doc_vectors = {doc: {} for doc in docs}\n",
    "    query_vector = {}\n",
    "\n",
    "    for term in query.split():\n",
    "        occurrence, _ = text_processing(term, tokenization, normalization, file_type)\n",
    "        print(\"Occurrences:\", occurrence)\n",
    "        \n",
    "        # Remplissage des vecteurs des documents et de la requête\n",
    "        for occ in occurrence:\n",
    "            doc_name = occ[2]\n",
    "            poids_terme = occ[4]\n",
    "            doc_vectors[doc_name][term] = poids_terme\n",
    "            query_vector[term] = query_vector.get(term, 0) + poids_terme\n",
    "            \n",
    "    print(f\" doc vectors : {doc_vectors}\")\n",
    "    print(f\"vecteur query {query_vector}\")\n",
    "    # Calcul de la mesure de Jaccard pour chaque document\n",
    "    jaccard_similarity = {}\n",
    "    for doc, term_weights in doc_vectors.items():\n",
    "        # intersection = sum(query_vector.get(term, 0) * weight for term, weight in term_weights.items())\n",
    "        intersection = sum( weight for term, weight in term_weights.items())\n",
    "        \n",
    "        # sum_query_squares = sum(weight**2 for weight in query_vector.values())\n",
    "        sum_query_squares = math.sqrt(len(term_weights))\n",
    "        sum_doc_squares = sum(weight**2 for weight in term_weights.values())\n",
    "        union = sum_query_squares + sum_doc_squares - intersection\n",
    "\n",
    "        if union == 0:\n",
    "            jaccard_similarity[doc] = 0  # Pour éviter une division par zéro\n",
    "        else:\n",
    "            jaccard_similarity[doc] = intersection / union\n",
    "    \n",
    "    return jaccard_similarity\n",
    "\n",
    "jaccard_relevance = calculer_jaccard_similarity('effect distribution', 'Split', 'None', 'DPT', path)\n",
    "print(jaccard_relevance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'D1': 0.11398376765566398,\n",
       " 'D2': 0,\n",
       " 'D3': 0.17941098224334792,\n",
       " 'D4': 0.0343009847771403,\n",
       " 'D5': 0,\n",
       " 'D6': 0.17941098224334792}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jaccard_relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
