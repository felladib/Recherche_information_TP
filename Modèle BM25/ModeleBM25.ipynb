{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.1.3 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\asyncio\\base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\asyncio\\base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\ryan\\AppData\\Local\\Temp\\ipykernel_1680\\2440574508.py\", line 1, in <module>\n",
      "    import nltk\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\nltk\\__init__.py\", line 133, in <module>\n",
      "    from nltk.collocations import *\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\nltk\\collocations.py\", line 36, in <module>\n",
      "    from nltk.metrics import (\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\nltk\\metrics\\__init__.py\", line 18, in <module>\n",
      "    from nltk.metrics.association import (\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\nltk\\metrics\\association.py\", line 26, in <module>\n",
      "    from scipy.stats import fisher_exact\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\scipy\\stats\\__init__.py\", line 606, in <module>\n",
      "    from ._stats_py import *\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\scipy\\stats\\_stats_py.py\", line 37, in <module>\n",
      "    from scipy import sparse\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\scipy\\__init__.py\", line 134, in __getattr__\n",
      "    return _importlib.import_module(f'scipy.{name}')\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\importlib\\__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\scipy\\sparse\\__init__.py\", line 295, in <module>\n",
      "    from ._csr import *\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\scipy\\sparse\\_csr.py\", line 11, in <module>\n",
      "    from ._sparsetools import (csr_tocsc, csr_tobsr, csr_count_blocks,\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.1.3 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\asyncio\\base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\asyncio\\base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\ryan\\AppData\\Local\\Temp\\ipykernel_1680\\2440574508.py\", line 1, in <module>\n",
      "    import nltk\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\nltk\\__init__.py\", line 133, in <module>\n",
      "    from nltk.collocations import *\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\nltk\\collocations.py\", line 36, in <module>\n",
      "    from nltk.metrics import (\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\nltk\\metrics\\__init__.py\", line 38, in <module>\n",
      "    from nltk.metrics.scores import (\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\nltk\\metrics\\scores.py\", line 15, in <module>\n",
      "    from scipy.stats.stats import betai\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\scipy\\stats\\__init__.py\", line 606, in <module>\n",
      "    from ._stats_py import *\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\scipy\\stats\\_stats_py.py\", line 37, in <module>\n",
      "    from scipy import sparse\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\scipy\\__init__.py\", line 134, in __getattr__\n",
      "    return _importlib.import_module(f'scipy.{name}')\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\importlib\\__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\scipy\\sparse\\__init__.py\", line 295, in <module>\n",
      "    from ._csr import *\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\scipy\\sparse\\_csr.py\", line 11, in <module>\n",
      "    from ._sparsetools import (csr_tocsc, csr_tobsr, csr_count_blocks,\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.1.3 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\asyncio\\base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\asyncio\\base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\ryan\\AppData\\Local\\Temp\\ipykernel_1680\\2440574508.py\", line 1, in <module>\n",
      "    import nltk\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\nltk\\__init__.py\", line 146, in <module>\n",
      "    from nltk.chunk import *\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\nltk\\chunk\\__init__.py\", line 155, in <module>\n",
      "    from nltk.chunk.api import ChunkParserI\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\nltk\\chunk\\api.py\", line 15, in <module>\n",
      "    from nltk.parse import ParserI\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\nltk\\parse\\__init__.py\", line 100, in <module>\n",
      "    from nltk.parse.transitionparser import TransitionParser\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\nltk\\parse\\transitionparser.py\", line 17, in <module>\n",
      "    from scipy import sparse\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\scipy\\__init__.py\", line 134, in __getattr__\n",
      "    return _importlib.import_module(f'scipy.{name}')\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\importlib\\__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\scipy\\sparse\\__init__.py\", line 295, in <module>\n",
      "    from ._csr import *\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\scipy\\sparse\\_csr.py\", line 11, in <module>\n",
      "    from ._sparsetools import (csr_tocsc, csr_tobsr, csr_count_blocks,\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import os\n",
    "import math\n",
    "from collections import defaultdict\n",
    "from nltk import FreqDist\n",
    "import numpy as np\n",
    "import json\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "STOPWORDS = set(nltk.corpus.stopwords.words('english'))\n",
    "PORTER_STEMMER = nltk.PorterStemmer()\n",
    "LANCASTER_STEMMER = nltk.LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../Collections'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_processing_args():\n",
    "    tokenization = \"Split\"\n",
    "    normalization = \"None\",\n",
    "    file_type = \"TPD\"\n",
    "    return tokenization, normalization, file_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(doc_path, tokenization, normalization):\n",
    "    # read all fils \n",
    "    with open(doc_path, 'r') as file:\n",
    "        text = file.read()\n",
    "        \n",
    "    # Tokenization\n",
    "    if tokenization == \"Split\":\n",
    "        tokens = text.split()\n",
    "    else:\n",
    "        exp_reg = nltk.RegexpTokenizer(r'\\d+(?:\\.\\d+)?x\\d+|\\d+(?:\\.\\d+)|\\w+(?:-\\w+)*|(?:[A-Z]\\.)+|\\w+')\n",
    "        tokens = exp_reg.tokenize(text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    tokens = [term for term in tokens if term.lower() not in STOPWORDS]\n",
    "\n",
    "    # Normalization\n",
    "    if normalization == \"Porter\":\n",
    "        tokens = [PORTER_STEMMER.stem(term) for term in tokens]\n",
    "    elif normalization == \"Lancaster\":\n",
    "        tokens = [LANCASTER_STEMMER.stem(term) for term in tokens]\n",
    "    print(\"------Tokens:------------------\",tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_global_term_frequencies(tokenization, normalization):\n",
    "    global_term_frequencies = defaultdict(int)\n",
    "\n",
    "    for doc_name in os.listdir(path):\n",
    "        doc_path = os.path.join(path, doc_name)\n",
    "        tokens = preprocessing(doc_path, tokenization, normalization)\n",
    "        unique_terms = set(tokens)\n",
    "\n",
    "        for term in unique_terms:\n",
    "            global_term_frequencies[term] += 1\n",
    "            \n",
    "            \n",
    "    print(\"------Global term frequencies:------------------\",global_term_frequencies)\n",
    "    \n",
    "    return global_term_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TPD_result(query, terms_freq, global_term_frequencies, N):\n",
    "    max_freq = max(terms_freq.values())\n",
    "    results=[]\n",
    "    for idx, (term, freq) in enumerate(terms_freq.items(), start=1):\n",
    "        poids = (freq / max_freq) * math.log10((N / global_term_frequencies[term]) + 1)\n",
    "        results.append((idx, term, query, freq, round(poids, 4)))\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_input(query, normalization):\n",
    "    # appliquer le traitement sur la requete\n",
    "    if normalization == \"Porter\":\n",
    "        query = PORTER_STEMMER.stem(query) \n",
    "    elif normalization == \"Lancaster\":\n",
    "        query = LANCASTER_STEMMER.stem(query) \n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nb_termes_glob(tokenization, normalization):\n",
    "    nb_termes_global = []\n",
    "    for doc_name in os.listdir(path):\n",
    "        doc_path = os.path.join(path, doc_name)\n",
    "        # Appliquer le prétraitement pour obtenir les tokens du document\n",
    "        tokens = preprocessing(doc_path, tokenization, normalization)\n",
    "        \n",
    "        # Ajouter les tokens du document à la liste globale\n",
    "        nb_termes_global.extend(tokens)\n",
    "\n",
    "    # Obtenir le nombre de termes uniques\n",
    "    termes_uniques = np.unique(nb_termes_global)\n",
    "    print(\"Termes uniques : \", termes_uniques)  # Optionnel : pour visualiser les termes uniques\n",
    "    \n",
    "    return len(termes_uniques)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_processing(query,tokenization, normalization, file_type):\n",
    "    \n",
    "    # tokenization, normalization, file_type = get_processing_args()\n",
    "    nb_terms = 0\n",
    "    global_term_frequencies = build_global_term_frequencies(tokenization, normalization)  # Calculate global term frequencies\n",
    "    N = len(os.listdir(path))\n",
    "    results =[]\n",
    "    if file_type == \"TPD\":\n",
    "        doc_path = os.path.join(path, f\"{query}.txt\")\n",
    "        tokens = preprocessing(doc_path, tokenization, normalization)\n",
    "        nb_terms = len(np.unique(tokens))\n",
    "        terms_freq = FreqDist(tokens)\n",
    "        \n",
    "        result = TPD_result(query, terms_freq, global_term_frequencies, N)\n",
    "        \n",
    "        return result , nb_terms\n",
    "        \n",
    "    else :\n",
    "        query = process_input(query, normalization)\n",
    "        i=0\n",
    "        for doc_name in os.listdir(path):\n",
    "            doc_path = os.path.join(path, doc_name)\n",
    "            Tokens = preprocessing(doc_path, tokenization, normalization)\n",
    "            terms_freq = FreqDist(Tokens)\n",
    "\n",
    "            max_freq = max(terms_freq.values())\n",
    "            for term, freq in terms_freq.items():  \n",
    "                if term == query:  # Check if the term is the specific query term\n",
    "                    poids = ((freq / max_freq) * math.log10((N / global_term_frequencies[term]) + 1))\n",
    "                    i+=1\n",
    "                    results.append((i, term, os.path.splitext(doc_name)[0], freq, round(poids, 4)))\n",
    "        \n",
    "        return results , nb_terms\n",
    "   \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(query):\n",
    "    # if raw:\n",
    "    doc_path = os.path.join(path, f\"{query}.txt\")\n",
    "    with open(doc_path, 'r') as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "    # elif processed:\n",
    "    #     text_processing(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_descriptor_and_inverse_files_with_weights(path, tokenization, normalization, output_path=\"output\"):\n",
    "    # Créer un nom unique pour les fichiers en fonction des choix de tokenization et normalization\n",
    "    descriptor_filename = f\"descripteur_{tokenization}_{normalization}.json\"\n",
    "    inverse_index_filename = f\"inverse_index_{tokenization}_{normalization}.json\"\n",
    "    \n",
    "    descriptor_path = os.path.join(output_path, descriptor_filename)\n",
    "    inverse_index_path = os.path.join(output_path, inverse_index_filename)\n",
    "    \n",
    "    # Vérifier si les fichiers existent déjà\n",
    "    if os.path.exists(descriptor_path) and os.path.exists(inverse_index_path):\n",
    "        print(f\"Les fichiers descripteur et inverse existent déjà : {descriptor_path} et {inverse_index_path}\")\n",
    "        \n",
    "        # Charger les fichiers existants\n",
    "        with open(descriptor_path, \"r\", encoding=\"utf-8\") as desc_file:\n",
    "            descripteur = json.load(desc_file)\n",
    "        with open(inverse_index_path, \"r\", encoding=\"utf-8\") as inverse_file:\n",
    "            inverse_index = json.load(inverse_file)\n",
    "        \n",
    "        return descripteur, inverse_index\n",
    "    \n",
    "    # Si les fichiers n'existent pas, les créer\n",
    "    print(\"Création des fichiers descripteur et inverse...\")\n",
    "    \n",
    "    # Initialiser les fichiers descripteur et inverse\n",
    "    descripteur = {}\n",
    "    inverse_index = defaultdict(lambda: defaultdict(lambda: {\"freq\": 0, \"poids\": 0}))\n",
    "    \n",
    "    # Nombre total de documents\n",
    "    documents = os.listdir(path)\n",
    "    N = len(documents)\n",
    "    \n",
    "    # Calcul des fréquences globales pour les poids\n",
    "    global_term_frequencies = defaultdict(int)\n",
    "    for doc_name in documents:\n",
    "        doc_path = os.path.join(path, doc_name)\n",
    "        tokens = preprocessing(doc_path, tokenization, normalization)\n",
    "        unique_terms = set(tokens)\n",
    "        for term in unique_terms:\n",
    "            global_term_frequencies[term] += 1\n",
    "    \n",
    "    # Construire les fichiers descripteur et inverse\n",
    "    for doc_name in documents:\n",
    "        doc_path = os.path.join(path, doc_name)\n",
    "        tokens = preprocessing(doc_path, tokenization, normalization)\n",
    "        terms_freq = FreqDist(tokens)  # Fréquence des termes\n",
    "        max_freq = max(terms_freq.values())  # Fréquence maximale dans le document\n",
    "        doc_key = os.path.splitext(doc_name)[0]\n",
    "        \n",
    "        # Ajouter les termes au fichier descripteur\n",
    "        descripteur[doc_key] = {}\n",
    "        for term, freq in terms_freq.items():\n",
    "            poids = (freq / max_freq) * math.log10((N / global_term_frequencies[term]) + 1)\n",
    "            descripteur[doc_key][term] = {\"freq\": freq, \"poids\": round(poids, 4)}\n",
    "            \n",
    "            # Ajouter les termes au fichier inverse\n",
    "            inverse_index[term][doc_key][\"freq\"] = freq\n",
    "            inverse_index[term][doc_key][\"poids\"] = round(poids, 4)\n",
    "    \n",
    "    # Sauvegarder les fichiers en JSON\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    \n",
    "    with open(descriptor_path, \"w\", encoding=\"utf-8\") as desc_file:\n",
    "        json.dump(descripteur, desc_file, indent=4, ensure_ascii=False)\n",
    "        \n",
    "    with open(inverse_index_path, \"w\", encoding=\"utf-8\") as inverse_file:\n",
    "        json.dump(inverse_index, inverse_file, indent=4, ensure_ascii=False)\n",
    "    \n",
    "\n",
    "    return descripteur, inverse_index\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les fichiers descripteur et inverse existent déjà : output\\descripteur_Split_Porter.json et output\\inverse_index_Split_Porter.json\n"
     ]
    }
   ],
   "source": [
    "# Exemple d'utilisation\n",
    "path = \"../Collections\"  # Remplacez par le chemin de votre dossier contenant les documents\n",
    "tokenization = \"Split\"  # Ou \"Regex\" selon votre choix\n",
    "normalization = \"Porter\"  # Ou \"Lancaster\" selon votre choix\n",
    "descripteur, inverse_index = create_descriptor_and_inverse_files_with_weights(path, tokenization, normalization)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculer_relevance_BM25(query, tokenization, normalization, path, k=1.5, b=0.75, output_path=\"output\"):\n",
    "    # Générer les noms des fichiers\n",
    "    descriptor_filename = f\"descripteur_{tokenization}_{normalization}.json\"\n",
    "    inverse_index_filename = f\"inverse_index_{tokenization}_{normalization}.json\"\n",
    "\n",
    "    descriptor_path = os.path.join(output_path, descriptor_filename)\n",
    "    inverse_index_path = os.path.join(output_path, inverse_index_filename)\n",
    "\n",
    "    # Vérifier si les fichiers existent, sinon les créer\n",
    "    if not os.path.exists(descriptor_path) or not os.path.exists(inverse_index_path):\n",
    "        print(f\"Les fichiers pour {tokenization} et {normalization} n'existent pas. Création en cours...\")\n",
    "        create_descriptor_and_inverse_files_with_weights(path, tokenization, normalization, output_path)\n",
    "\n",
    "    # Charger les fichiers descripteur et inverse\n",
    "    with open(descriptor_path, \"r\", encoding=\"utf-8\") as desc_file:\n",
    "        descripteur = json.load(desc_file)\n",
    "    with open(inverse_index_path, \"r\", encoding=\"utf-8\") as inverse_file:\n",
    "        inverse_index = json.load(inverse_file)\n",
    "        \n",
    "    \n",
    "\n",
    "    # Calcul des longueurs de documents et de la taille moyenne\n",
    "    doc_lengths = {doc: sum(term_data[\"freq\"] for term_data in terms.values()) for doc, terms in descripteur.items()}\n",
    "\n",
    "    # Nbre total des terms\n",
    "    total_terms = sum(doc_lengths.values())\n",
    "    # Nombre total de documents\n",
    "    N = len(descripteur) \n",
    "    # la taille moyenne des documents \n",
    "    avdl = total_terms / N if N > 0 else 1  # Eviter division par zéro\n",
    "    \n",
    "    # Initialiser les scores de pertinence\n",
    "    relevance_dict = {doc: 0 for doc in descripteur}\n",
    "\n",
    "    # Traiter chaque terme de la requête\n",
    "    for term in query.split():\n",
    "        term = process_input(term, normalization)\n",
    "        \n",
    "        # dans le cas ou le terme n'exist pas dans le fichier inverse\n",
    "        if term not in inverse_index:\n",
    "            continue  # Ignorer les termes absents du fichier inverse\n",
    "\n",
    "        # Nombre de documents contenant le terme\n",
    "        ni = len(inverse_index[term])\n",
    "        # print(f\"nombre de document contenant {term} : {ni}\")\n",
    "\n",
    "        # Calculer l'IDF avec un seuil pour éviter des valeurs négatives ou extrêmes\n",
    "        idf = math.log10(((N - ni + 0.5) / (ni + 0.5)) ) if ni + 0.5 != 0 else 0\n",
    "        # print(idf)\n",
    "        for doc_name, data in inverse_index[term].items():\n",
    "            freq_ti_d = data[\"freq\"]  # Fréquence du terme dans le document\n",
    "            # print(f\"frequant terme {term} dans le document {doc_name} est :{freq_ti_d}\")\n",
    "            dl = doc_lengths.get(doc_name, 0)  # Taille du document d\n",
    "            \n",
    "            if dl == 0:\n",
    "                continue  # Éviter division par zéro\n",
    "\n",
    "            # Calcul du score BM25\n",
    "            # numerator = freq_ti_d * (k + 1)\n",
    "            numerator = freq_ti_d \n",
    "            denominator = freq_ti_d + (k * ((1 - b) + b * (dl / avdl)))\n",
    "            RSV = idf * (numerator / denominator)\n",
    "\n",
    "            \n",
    "            # Ajouter au score total du document\n",
    "            relevance_dict[doc_name] += RSV\n",
    "            \n",
    "    # print(relevance_dict)\n",
    "    \n",
    "    filtered_relevances = {doc: score for doc, score in relevance_dict.items() if score != 0}\n",
    "\n",
    "    # Trier les documents par pertinence décroissante\n",
    "    sorted_relevance = dict(sorted(filtered_relevances.items(), key=lambda item: item[1], reverse=True))\n",
    "    \n",
    "    return sorted_relevance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores de pertinence : {'D4': -0.08823075947891026, 'D1': -0.11465481799179207, 'D6': -0.11847597133343354, 'D3': -0.13669933538797593}\n"
     ]
    }
   ],
   "source": [
    "tokenization = \"Split\"  # Ou \"Regex\" selon votre choix\n",
    "normalization = \"None\"  # Ou \"Lancaster\" selon votre choix\n",
    "query = \"effect distribution \"  # Exemple de requête\n",
    "path = \"../Collections\"  # Chemin vers le dossier contenant les documents\n",
    "output_path = \"output\"\n",
    "\n",
    "\n",
    "relevance_scores = calculer_relevance_BM25(query, tokenization, normalization, path, k=1.5, b=0.75, output_path=output_path)\n",
    "\n",
    "if relevance_scores:\n",
    "    print(\"Scores de pertinence :\", relevance_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
