{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os\n",
    "import math\n",
    "from collections import defaultdict\n",
    "from nltk import FreqDist\n",
    "\n",
    "STOPWORDS = set(nltk.corpus.stopwords.words('english'))\n",
    "PORTER_STEMMER = nltk.PorterStemmer()\n",
    "LANCASTER_STEMMER = nltk.LancasterStemmer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_processing_args():\n",
    "    tokenization = \"Split\"\n",
    "    normalization = \"None\",\n",
    "    file_type = \"TPD\"\n",
    "    return tokenization, normalization, file_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(doc_path, tokenization, normalization):\n",
    "    with open(doc_path, 'r') as file:\n",
    "        text = file.read()\n",
    "        \n",
    "    # Tokenization\n",
    "    if tokenization == \"Split\":\n",
    "        tokens = text.split()\n",
    "    else:\n",
    "        exp_reg = nltk.RegexpTokenizer(r'\\d+(?:\\.\\d+)?x\\d+|\\d+(?:\\.\\d+)|\\w+(?:-\\w+)*|(?:[A-Z]\\.)+|\\w+')\n",
    "        tokens = exp_reg.tokenize(text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    tokens = [term for term in tokens if term.lower() not in STOPWORDS]\n",
    "\n",
    "    # Normalization\n",
    "    if normalization == \"Porter\":\n",
    "        tokens = [PORTER_STEMMER.stem(term) for term in tokens]\n",
    "    elif normalization == \"Lancaster\":\n",
    "        tokens = [LANCASTER_STEMMER.stem(term) for term in tokens]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_global_term_frequencies(tokenization, normalization):\n",
    "    global_term_frequencies = defaultdict(int)\n",
    "\n",
    "    for doc_name in os.listdir('Collections'):\n",
    "        doc_path = os.path.join('Collections', doc_name)\n",
    "        tokens = preprocessing(doc_path, tokenization, normalization)\n",
    "        unique_terms = set(tokens)\n",
    "\n",
    "        for term in unique_terms:\n",
    "            global_term_frequencies[term] += 1\n",
    "            \n",
    "    return global_term_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Display_TPD_result(query, terms_freq, global_term_frequencies, N):\n",
    "    max_freq = max(terms_freq.values())\n",
    "\n",
    "    for idx, (term, freq) in enumerate(terms_freq.items(), start=1):\n",
    "        poids = (freq / max_freq) * math.log10((N / global_term_frequencies[term]) + 1)\n",
    "        print(idx, query, term, freq, format(poids, '.4f'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_processing(query):\n",
    "    tokenization, normalization, file_type = get_processing_args()\n",
    "   \n",
    "    global_term_frequencies = build_global_term_frequencies(tokenization, normalization)  # Calculate global term frequencies\n",
    "    N = len(os.listdir('Collections'))\n",
    "    \n",
    "    if file_type == \"TPD\":\n",
    "        doc_path = os.path.join('Collections', f\"{query}.txt\")\n",
    "        tokens = preprocessing(doc_path, tokenization, normalization)\n",
    "        terms_freq = FreqDist(tokens)\n",
    "\n",
    "        Display_TPD_result(query, terms_freq, global_term_frequencies, N)\n",
    "        \n",
    "    else :\n",
    "        i=0\n",
    "        for doc_name in os.listdir('Collections'):\n",
    "            doc_path = os.path.join('Collections', doc_name)\n",
    "            Tokens = preprocessing(doc_path, tokenization, normalization)\n",
    "            terms_freq = FreqDist(Tokens)\n",
    "\n",
    "            max_freq = max(terms_freq.values())\n",
    "            for term, freq in terms_freq.items():  \n",
    "                if term == query:  # Check if the term is the specific query term\n",
    "                    poids = ((freq / max_freq) * math.log10((N / global_term_frequencies[term]) + 1))\n",
    "                    i+=1\n",
    "                    print(f\"{i} Term: {term}, Document: {os.path.splitext(doc_name)[0]}, Frequency: {freq}, Weight: {format(poids, '.4f')}\")\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_args(query, raw, processed):\n",
    "    if raw:\n",
    "        doc_path = os.path.join('Collections', f\"{query}.txt\")\n",
    "        with open(doc_path, 'r') as file:\n",
    "            text = file.read()\n",
    "        print(text)\n",
    "    elif processed:\n",
    "        text_processing(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 D1 experimental 2 0.2817\n",
      "2 D1 investigation 1 0.1408\n",
      "3 D1 aerodynamics 1 0.1408\n",
      "4 D1 wing 3 0.3010\n",
      "5 D1 slipstream 5 0.7042\n",
      "6 D1 . 6 0.3010\n",
      "7 D1 study 1 0.1003\n",
      "8 D1 propeller 1 0.1408\n",
      "9 D1 made 2 0.2007\n",
      "10 D1 order 1 0.1408\n",
      "11 D1 determine 1 0.1408\n",
      "12 D1 spanwise 1 0.1408\n",
      "13 D1 distribution 1 0.0663\n",
      "14 D1 lift 3 0.4225\n",
      "15 D1 increase 1 0.1408\n",
      "16 D1 due 2 0.2817\n",
      "17 D1 different 3 0.3010\n",
      "18 D1 angles 1 0.1408\n",
      "19 D1 attack 1 0.1408\n",
      "20 D1 free 1 0.1003\n",
      "21 D1 stream 1 0.1003\n",
      "22 D1 velocity 1 0.1003\n",
      "23 D1 ratios 1 0.1408\n",
      "24 D1 results 1 0.1003\n",
      "25 D1 intended 1 0.1408\n",
      "26 D1 part 2 0.2817\n",
      "27 D1 evaluation 2 0.2817\n",
      "28 D1 basis 1 0.1003\n",
      "29 D1 theoretical 1 0.1408\n",
      "30 D1 treatments 1 0.1408\n",
      "31 D1 problem 1 0.0663\n",
      "32 D1 comparative 1 0.1408\n",
      "33 D1 span 1 0.1408\n",
      "34 D1 loading 1 0.1408\n",
      "35 D1 curves, 1 0.1408\n",
      "36 D1 together 1 0.1408\n",
      "37 D1 supporting 1 0.1408\n",
      "38 D1 evidence, 1 0.1408\n",
      "39 D1 showed 1 0.1408\n",
      "40 D1 substantial 1 0.1408\n",
      "41 D1 increment 1 0.1408\n",
      "42 D1 produced 1 0.1408\n",
      "43 D1 /destalling/ 1 0.1408\n",
      "44 D1 boundary-layer-control 1 0.1408\n",
      "45 D1 effect 1 0.0795\n",
      "46 D1 integrated 1 0.1003\n",
      "47 D1 remaining 1 0.1408\n",
      "48 D1 increment, 1 0.1408\n",
      "49 D1 subtracting 1 0.1408\n",
      "50 D1 destalling 2 0.2817\n",
      "51 D1 lift, 1 0.1003\n",
      "52 D1 found 1 0.1408\n",
      "53 D1 agree 1 0.1408\n",
      "54 D1 well 1 0.1408\n",
      "55 D1 potential 1 0.1408\n",
      "56 D1 flow 1 0.0571\n",
      "57 D1 theory 1 0.1003\n",
      "58 D1 empirical 1 0.1408\n",
      "59 D1 effects 1 0.0795\n",
      "60 D1 specific 1 0.1408\n",
      "61 D1 configuration 1 0.1408\n",
      "62 D1 experiment 1 0.1408\n"
     ]
    }
   ],
   "source": [
    "get_text_args(\"D1\", False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
