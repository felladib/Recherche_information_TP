{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import BIB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyQt5.QtWidgets import (\n",
    "    QApplication, QMainWindow, QWidget, QVBoxLayout, QHBoxLayout,\n",
    "    QLineEdit, QPushButton, QRadioButton, QLabel, QGroupBox,\n",
    "    QTableWidget, QTableWidgetItem, QScrollArea, QTextEdit, QStackedWidget, QGridLayout,\n",
    "    QMessageBox,QHeaderView,QComboBox\n",
    ")\n",
    "from PyQt5.QtGui import QDoubleValidator\n",
    "from PyQt5.QtGui import QIcon\n",
    "from PyQt5.QtCore import Qt\n",
    "import sys\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os\n",
    "import math\n",
    "from collections import defaultdict\n",
    "from nltk import FreqDist\n",
    "\n",
    "\n",
    "STOPWORDS = set(nltk.corpus.stopwords.words('english'))\n",
    "PORTER_STEMMER = nltk.PorterStemmer()\n",
    "LANCASTER_STEMMER = nltk.LancasterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../Collestions'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_processing_args():\n",
    "#     tokenization = \"Split\"\n",
    "#     normalization = \"None\",\n",
    "#     file_type = \"TPD\"\n",
    "#     return tokenization, normalization, file_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(doc_path, tokenization, normalization):\n",
    "    with open(doc_path, 'r') as file:\n",
    "        text = file.read()\n",
    "        \n",
    "    # Tokenization\n",
    "    if tokenization == \"Split\":\n",
    "        tokens = text.split()\n",
    "    else:\n",
    "        exp_reg = nltk.RegexpTokenizer(r'\\d+(?:\\.\\d+)?x\\d+|\\d+(?:\\.\\d+)|\\w+(?:-\\w+)*|(?:[A-Z]\\.)+|\\w+')\n",
    "        tokens = exp_reg.tokenize(text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    tokens = [term for term in tokens if term.lower() not in STOPWORDS]\n",
    "\n",
    "    # Normalization\n",
    "    if normalization == \"Porter\":\n",
    "        tokens = [PORTER_STEMMER.stem(term) for term in tokens]\n",
    "    elif normalization == \"Lancaster\":\n",
    "        tokens = [LANCASTER_STEMMER.stem(term) for term in tokens]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_global_term_frequencies(tokenization, normalization):\n",
    "#     global_term_frequencies = defaultdict(int)\n",
    "\n",
    "#     for doc_name in os.listdir('Collections'):\n",
    "#         doc_path = os.path.join('Collections', doc_name)\n",
    "#         tokens = preprocessing(doc_path, tokenization, normalization)\n",
    "#         unique_terms = set(tokens)\n",
    "\n",
    "#         for term in unique_terms:\n",
    "#             global_term_frequencies[term] += 1\n",
    "            \n",
    "#     return global_term_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def TPD_result(query, terms_freq, global_term_frequencies, N):\n",
    "#     max_freq = max(terms_freq.values())\n",
    "#     results=[]\n",
    "#     for idx, (term, freq) in enumerate(terms_freq.items(), start=1):\n",
    "#         poids = (freq / max_freq) * math.log10((N / global_term_frequencies[term]) + 1)\n",
    "#         results.append((idx, term, query, freq, round(poids, 4)))\n",
    "        \n",
    "#     return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def nb_termes_glob(tokenization, normalization):\n",
    "#     nb_termes_global = []\n",
    "#     for doc_name in os.listdir('Collections'):\n",
    "#         doc_path = os.path.join('Collections', doc_name)\n",
    "#         # Appliquer le prétraitement pour obtenir les tokens du document\n",
    "#         tokens = preprocessing(doc_path, tokenization, normalization)\n",
    "        \n",
    "#         # Ajouter les tokens du document à la liste globale\n",
    "#         nb_termes_global.extend(tokens)\n",
    "\n",
    "#     # Obtenir le nombre de termes uniques\n",
    "#     termes_uniques = np.unique(nb_termes_global)\n",
    "#     print(\"Termes uniques : \", termes_uniques)  # Optionnel : pour visualiser les termes uniques\n",
    "    \n",
    "#     return len(termes_uniques)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_input(query, normalization):\n",
    "    # appliquer le traitement sur la requete\n",
    "    if normalization == \"Porter\":\n",
    "        query = PORTER_STEMMER.stem(query) \n",
    "    elif normalization == \"Lancaster\":\n",
    "        query = LANCASTER_STEMMER.stem(query) \n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def text_processing(query,tokenization, normalization, file_type):\n",
    "#     # tokenization, normalization, file_type = get_processing_args()\n",
    "#     nb_terms = 0\n",
    "#     # calculer la frequence de chaque element\n",
    "#     global_term_frequencies = build_global_term_frequencies(tokenization, normalization)  # Calculate global term frequencies\n",
    "#     N = len(os.listdir('Collections'))\n",
    "#     results =[]\n",
    "#     if file_type == \"TPD\":\n",
    "#         doc_path = os.path.join('Collections', f\"{query}.txt\")\n",
    "#         tokens = preprocessing(doc_path, tokenization, normalization)\n",
    "#         nb_terms = len(np.unique(tokens))\n",
    "#         terms_freq = FreqDist(tokens)\n",
    "        \n",
    "#         results = TPD_result(query, terms_freq, global_term_frequencies, N)\n",
    "#         print(f\"la resultat de chaque requete type {results}\")\n",
    "#         return results , nb_terms\n",
    "        \n",
    "#     else :\n",
    "#         query = process_input(query, normalization)\n",
    "#         i=0\n",
    "#         for doc_name in os.listdir('Collections'):\n",
    "#             doc_path = os.path.join('Collections', doc_name)\n",
    "#             Tokens = preprocessing(doc_path, tokenization, normalization)\n",
    "#             terms_freq = FreqDist(Tokens)\n",
    "\n",
    "#             max_freq = max(terms_freq.values())\n",
    "#             for term, freq in terms_freq.items():  \n",
    "#                 if term == query:  # Check if the term is the specific query term\n",
    "#                     poids = ((freq / max_freq) * math.log10((N / global_term_frequencies[term]) + 1))\n",
    "#                     i+=1\n",
    "#                     results.append((i, term, os.path.splitext(doc_name)[0], freq, round(poids, 4)))\n",
    "#         print(f\"la resultat de chaque requete type {results}\")\n",
    "#         return results , nb_terms\n",
    "   \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(query):\n",
    "    # if raw:\n",
    "    doc_path = os.path.join('Collections', f\"{query}.txt\")\n",
    "    with open(doc_path, 'r') as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "    # elif processed:\n",
    "    #     text_processing(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def create_descriptor_and_inverse_files_with_weights(path, tokenization, normalization, output_path=\"output\"):\n",
    "    # Créer un nom unique pour les fichiers en fonction des choix de tokenization et normalization\n",
    "    descriptor_filename = f\"descripteur_{tokenization}_{normalization}.json\"\n",
    "    inverse_index_filename = f\"inverse_index_{tokenization}_{normalization}.json\"\n",
    "    \n",
    "    descriptor_path = os.path.join(output_path, descriptor_filename)\n",
    "    inverse_index_path = os.path.join(output_path, inverse_index_filename)\n",
    "    \n",
    "    # Vérifier si les fichiers existent déjà\n",
    "    if os.path.exists(descriptor_path) and os.path.exists(inverse_index_path):\n",
    "        print(f\"Les fichiers descripteur et inverse existent déjà : {descriptor_path} et {inverse_index_path}\")\n",
    "        \n",
    "        # Charger les fichiers existants\n",
    "        with open(descriptor_path, \"r\", encoding=\"utf-8\") as desc_file:\n",
    "            descripteur = json.load(desc_file)\n",
    "        with open(inverse_index_path, \"r\", encoding=\"utf-8\") as inverse_file:\n",
    "            inverse_index = json.load(inverse_file)\n",
    "        \n",
    "        return descripteur, inverse_index\n",
    "    \n",
    "    # Si les fichiers n'existent pas, les créer\n",
    "    print(\"Création des fichiers descripteur et inverse...\")\n",
    "    \n",
    "    # Initialiser les fichiers descripteur et inverse\n",
    "    descripteur = {}\n",
    "    inverse_index = defaultdict(lambda: defaultdict(lambda: {\"freq\": 0, \"poids\": 0}))\n",
    "    \n",
    "    # Nombre total de documents\n",
    "    documents = os.listdir(path)\n",
    "    N = len(documents)\n",
    "    \n",
    "    # Calcul des fréquences globales pour les poids\n",
    "    global_term_frequencies = defaultdict(int)\n",
    "    for doc_name in documents:\n",
    "        doc_path = os.path.join(path, doc_name)\n",
    "        tokens = preprocessing(doc_path, tokenization, normalization)\n",
    "        unique_terms = set(tokens)\n",
    "        for term in unique_terms:\n",
    "            global_term_frequencies[term] += 1\n",
    "    \n",
    "    # Construire les fichiers descripteur et inverse\n",
    "    for doc_name in documents:\n",
    "        doc_path = os.path.join(path, doc_name)\n",
    "        tokens = preprocessing(doc_path, tokenization, normalization)\n",
    "        terms_freq = FreqDist(tokens)  # Fréquence des termes\n",
    "        max_freq = max(terms_freq.values())  # Fréquence maximale dans le document\n",
    "        doc_key = os.path.splitext(doc_name)[0]\n",
    "        \n",
    "        # Ajouter les termes au fichier descripteur\n",
    "        descripteur[doc_key] = {}\n",
    "        for term, freq in terms_freq.items():\n",
    "            poids = (freq / max_freq) * math.log10((N / global_term_frequencies[term]) + 1)\n",
    "            descripteur[doc_key][term] = {\"freq\": freq, \"poids\": round(poids, 4)}\n",
    "            \n",
    "            # Ajouter les termes au fichier inverse\n",
    "            inverse_index[term][doc_key][\"freq\"] = freq\n",
    "            inverse_index[term][doc_key][\"poids\"] = round(poids, 4)\n",
    "    \n",
    "    # Sauvegarder les fichiers en JSON\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    \n",
    "    with open(descriptor_path, \"w\", encoding=\"utf-8\") as desc_file:\n",
    "        json.dump(descripteur, desc_file, indent=4, ensure_ascii=False)\n",
    "        \n",
    "    with open(inverse_index_path, \"w\", encoding=\"utf-8\") as inverse_file:\n",
    "        json.dump(inverse_index, inverse_file, indent=4, ensure_ascii=False)\n",
    "    \n",
    "\n",
    "    return descripteur, inverse_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_terme_per_doc(query, descripteur):\n",
    "    if query in descripteur:\n",
    "        terms = descripteur[query]\n",
    "        total = len(terms)  # Nombre de termes distincts dans le document\n",
    "        return total\n",
    "    else:\n",
    "        return 0  # Si le document n'existe pas dans le fichier descripteur\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_termes_descripteur(descripteur):\n",
    "    total = 0\n",
    "    for terms in descripteur.values():  # Parcourt chaque document\n",
    "        total += len(terms)  # Ajoute le nombre de termes distincts dans chaque document\n",
    "    return total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_descripteur_invers(normalization , tokenization ,output_path):\n",
    "    # Générer les noms des fichiers\n",
    "    descriptor_filename = f\"descripteur_{tokenization}_{normalization}.json\"\n",
    "    inverse_index_filename = f\"inverse_index_{tokenization}_{normalization}.json\"\n",
    "\n",
    "    descriptor_path = os.path.join(output_path, descriptor_filename)\n",
    "    inverse_index_path = os.path.join(output_path, inverse_index_filename)\n",
    "\n",
    "    # Vérifier si les fichiers existent, sinon les créer\n",
    "    if not os.path.exists(descriptor_path) or not os.path.exists(inverse_index_path):\n",
    "        print(f\"Les fichiers pour {tokenization} et {normalization} n'existent pas. Création en cours...\")\n",
    "        create_descriptor_and_inverse_files_with_weights(path, tokenization, normalization, output_path)\n",
    "\n",
    "    # Charger les fichiers descripteur et inverse\n",
    "    with open(descriptor_path, \"r\", encoding=\"utf-8\") as desc_file:\n",
    "        descripteur = json.load(desc_file)\n",
    "    with open(inverse_index_path, \"r\", encoding=\"utf-8\") as inverse_file:\n",
    "        inverse_index = json.load(inverse_file)\n",
    "    return descripteur , inverse_index\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing(query, tokenization, normalization, path, output_path=\"output\",methode = 'TPD'):\n",
    "    \n",
    "    descripteur , inverse_index = open_descripteur_invers(normalization , tokenization ,output_path)\n",
    "   \n",
    "    nb_total_terme_per_doc = total_terme_per_doc(query, descripteur)\n",
    "    nb_total_per_collection = total_termes_descripteur(descripteur)\n",
    "    \n",
    "    formatted_results = []\n",
    "    if methode == 'TPD':\n",
    "        if query in descripteur:\n",
    "            terms = descripteur[query]\n",
    "            for i, (term, data) in enumerate(terms.items(), start=1):\n",
    "                formatted_results.append((i, term, query, data['freq'], data['poids']))\n",
    "        \n",
    "    else:\n",
    "        if query in inverse_index:\n",
    "            docs = inverse_index[query]\n",
    "            for i, (doc, data) in enumerate(docs.items(), start=1):\n",
    "                formatted_results.append((i, query, doc, data['freq'], data['poids']))\n",
    "    \n",
    "    \n",
    "   \n",
    "\n",
    "    return formatted_results , nb_total_terme_per_doc , nb_total_per_collection\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'experimental', 'D1', 2, 0.2817),\n",
       " (2, 'investigation', 'D1', 1, 0.1408),\n",
       " (3, 'aerodynamics', 'D1', 1, 0.1408),\n",
       " (4, 'wing', 'D1', 3, 0.301),\n",
       " (5, 'slipstream', 'D1', 5, 0.7042),\n",
       " (6, '.', 'D1', 6, 0.301),\n",
       " (7, 'study', 'D1', 1, 0.1003),\n",
       " (8, 'propeller', 'D1', 1, 0.1408),\n",
       " (9, 'made', 'D1', 2, 0.2007),\n",
       " (10, 'order', 'D1', 1, 0.1408),\n",
       " (11, 'determine', 'D1', 1, 0.1408),\n",
       " (12, 'spanwise', 'D1', 1, 0.1408),\n",
       " (13, 'distribution', 'D1', 1, 0.0663),\n",
       " (14, 'lift', 'D1', 3, 0.4225),\n",
       " (15, 'increase', 'D1', 1, 0.1408),\n",
       " (16, 'due', 'D1', 2, 0.2817),\n",
       " (17, 'different', 'D1', 3, 0.301),\n",
       " (18, 'angles', 'D1', 1, 0.1408),\n",
       " (19, 'attack', 'D1', 1, 0.1408),\n",
       " (20, 'free', 'D1', 1, 0.1003),\n",
       " (21, 'stream', 'D1', 1, 0.1003),\n",
       " (22, 'velocity', 'D1', 1, 0.1003),\n",
       " (23, 'ratios', 'D1', 1, 0.1408),\n",
       " (24, 'results', 'D1', 1, 0.1003),\n",
       " (25, 'intended', 'D1', 1, 0.1408),\n",
       " (26, 'part', 'D1', 2, 0.2817),\n",
       " (27, 'evaluation', 'D1', 2, 0.2817),\n",
       " (28, 'basis', 'D1', 1, 0.1003),\n",
       " (29, 'theoretical', 'D1', 1, 0.1408),\n",
       " (30, 'treatments', 'D1', 1, 0.1408),\n",
       " (31, 'problem', 'D1', 1, 0.0663),\n",
       " (32, 'comparative', 'D1', 1, 0.1408),\n",
       " (33, 'span', 'D1', 1, 0.1408),\n",
       " (34, 'loading', 'D1', 1, 0.1408),\n",
       " (35, 'curves,', 'D1', 1, 0.1408),\n",
       " (36, 'together', 'D1', 1, 0.1408),\n",
       " (37, 'supporting', 'D1', 1, 0.1408),\n",
       " (38, 'evidence,', 'D1', 1, 0.1408),\n",
       " (39, 'showed', 'D1', 1, 0.1408),\n",
       " (40, 'substantial', 'D1', 1, 0.1408),\n",
       " (41, 'increment', 'D1', 1, 0.1408),\n",
       " (42, 'produced', 'D1', 1, 0.1408),\n",
       " (43, '/destalling/', 'D1', 1, 0.1408),\n",
       " (44, 'boundary-layer-control', 'D1', 1, 0.1408),\n",
       " (45, 'effect', 'D1', 1, 0.0795),\n",
       " (46, 'integrated', 'D1', 1, 0.1003),\n",
       " (47, 'remaining', 'D1', 1, 0.1408),\n",
       " (48, 'increment,', 'D1', 1, 0.1408),\n",
       " (49, 'subtracting', 'D1', 1, 0.1408),\n",
       " (50, 'destalling', 'D1', 2, 0.2817),\n",
       " (51, 'lift,', 'D1', 1, 0.1003),\n",
       " (52, 'found', 'D1', 1, 0.1408),\n",
       " (53, 'agree', 'D1', 1, 0.1408),\n",
       " (54, 'well', 'D1', 1, 0.1408),\n",
       " (55, 'potential', 'D1', 1, 0.1408),\n",
       " (56, 'flow', 'D1', 1, 0.0571),\n",
       " (57, 'theory', 'D1', 1, 0.1003),\n",
       " (58, 'empirical', 'D1', 1, 0.1408),\n",
       " (59, 'effects', 'D1', 1, 0.0795),\n",
       " (60, 'specific', 'D1', 1, 0.1408),\n",
       " (61, 'configuration', 'D1', 1, 0.1408),\n",
       " (62, 'experiment', 'D1', 1, 0.1408)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_results , nb_total_terme_per_doc , nb_total_per_collection = processing(\"D1\", \"split\", None, path, output_path=\"output\",methode = 'TPD')\n",
    "formatted_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculer_relevance_BM25(query, tokenization, normalization, path, k=1.5, b=0.75, output_path=\"output\"):\n",
    "    \n",
    "        \n",
    "    descripteur , inverse_index = open_descripteur_invers(normalization , tokenization ,output_path)\n",
    "\n",
    "    # Calcul des longueurs de documents et de la taille moyenne\n",
    "    doc_lengths = {doc: sum(term_data[\"freq\"] for term_data in terms.values()) for doc, terms in descripteur.items()}\n",
    "\n",
    "    # Nbre total des terms\n",
    "    total_terms = sum(doc_lengths.values())\n",
    "    # Nombre total de documents\n",
    "    N = len(descripteur) \n",
    "    # la taille moyenne des documents \n",
    "    avdl = total_terms / N if N > 0 else 1  # Eviter division par zéro\n",
    "    \n",
    "    # Initialiser les scores de pertinence\n",
    "    relevance_dict = {doc: 0 for doc in descripteur}\n",
    "\n",
    "    # Traiter chaque terme de la requête\n",
    "    for term in query.split():\n",
    "        term = process_input(term, normalization)\n",
    "        \n",
    "        # dans le cas ou le terme n'exist pas dans le fichier inverse\n",
    "        if term not in inverse_index:\n",
    "            continue  # Ignorer les termes absents du fichier inverse\n",
    "\n",
    "        # Nombre de documents contenant le terme\n",
    "        ni = len(inverse_index[term])\n",
    "        # print(f\"nombre de document contenant {term} : {ni}\")\n",
    "\n",
    "        # Calculer l'IDF avec un seuil pour éviter des valeurs négatives ou extrêmes\n",
    "        idf = math.log10(((N - ni + 0.5) / (ni + 0.5)) ) if ni + 0.5 != 0 else 0\n",
    "        # print(idf)\n",
    "        for doc_name, data in inverse_index[term].items():\n",
    "            freq_ti_d = data[\"freq\"]  # Fréquence du terme dans le document\n",
    "            # print(f\"frequant terme {term} dans le document {doc_name} est :{freq_ti_d}\")\n",
    "            dl = doc_lengths.get(doc_name, 0)  # Taille du document d\n",
    "            \n",
    "            if dl == 0:\n",
    "                continue  # Éviter division par zéro\n",
    "\n",
    "            # Calcul du score BM25\n",
    "            # numerator = freq_ti_d * (k + 1)\n",
    "            numerator = freq_ti_d \n",
    "            denominator = freq_ti_d + (k * ((1 - b) + b * (dl / avdl)))\n",
    "            RSV = idf * (numerator / denominator)\n",
    "\n",
    "            \n",
    "            # Ajouter au score total du document\n",
    "            relevance_dict[doc_name] += RSV\n",
    "            \n",
    "    # print(relevance_dict)\n",
    "    \n",
    "    filtered_relevances = {doc: score for doc, score in relevance_dict.items() if score != 0}\n",
    "\n",
    "    # Trier les documents par pertinence décroissante\n",
    "    sorted_relevance = dict(sorted(filtered_relevances.items(), key=lambda item: item[1], reverse=True))\n",
    "    print(sorted_relevance)\n",
    "    return sorted_relevance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'D4': -0.08823075947891026, 'D2': -0.09795556690944766, 'D1': -0.2728925083902616, 'D3': -0.27339867077595187, 'D6': -0.3028044304368359}\n",
      "Scores de pertinence : {'D4': -0.08823075947891026, 'D2': -0.09795556690944766, 'D1': -0.2728925083902616, 'D3': -0.27339867077595187, 'D6': -0.3028044304368359}\n"
     ]
    }
   ],
   "source": [
    "tokenization = \"Split\"  # Ou \"Regex\" selon votre choix\n",
    "normalization = \"Lancaster\"  # Ou \"Lancaster\" selon votre choix\n",
    "query = \"effect distribution \"  # Exemple de requête\n",
    "path = \"../Collections\"  # Chemin vers le dossier contenant les documents\n",
    "output_path = \"output\"\n",
    "\n",
    "\n",
    "relevance_scores = calculer_relevance_BM25(query, tokenization, normalization, path, k=1.5, b=0.75, output_path=output_path)\n",
    "\n",
    "if relevance_scores:\n",
    "    print(\"Scores de pertinence :\", relevance_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculer_relevance(query, tokenization, normalization, file_type , path ):\n",
    "    # Créer une liste des documents à partir des fichiers dans le dossier\n",
    "    docs = [doc.split('.')[0] for doc in os.listdir(path)]\n",
    "    \n",
    "\n",
    "    relevance_dict = {doc: 0 for doc in docs}\n",
    "    for term in query.split():\n",
    "        occurrence, _ ,_= processing(term, tokenization, normalization, path, output_path=\"output\",methode =file_type)\n",
    "        print(occurrence)\n",
    "        # print(\"Occurrences:\", occurrence)\n",
    "        # exemple de occurrence : [(1, 'effect', 'D1', 1, 0.0795), (2, 'effect', 'D3', 1, 0.1193), (3, 'effect', 'D6', 1, 0.1193)]\n",
    "        \n",
    "        for occ in occurrence:\n",
    "            doc_name = occ[2]  # Nom du document contenant le terme\n",
    "            poids_terme = occ[4]  # Poids du terme dans ce document\n",
    "            relevance_dict[doc_name] += poids_terme\n",
    "            \n",
    "    return relevance_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 'effect', 'D1', 2, 0.1326), (2, 'effect', 'D2', 1, 0.0398), (3, 'effect', 'D3', 1, 0.0995), (4, 'effect', 'D6', 3, 0.2985)]\n",
      "[]\n",
      "[(1, 'wing', 'D1', 3, 0.301), (2, 'wing', 'D6', 2, 0.301)]\n",
      "[(1, 'slipstream', 'D1', 5, 0.7042)]\n",
      "[(1, 'experiment', 'D1', 2, 0.2817)]\n",
      "[(1, 'investig', 'D1', 1, 0.1003), (2, 'investig', 'D2', 1, 0.0602)]\n",
      "[(1, 'aerodynam', 'D1', 1, 0.1003), (2, 'aerodynam', 'D6', 1, 0.1505)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'D1': 1.6201,\n",
       " 'D2': 0.1,\n",
       " 'D3': 0.0995,\n",
       " 'D4': 0,\n",
       " 'D5': 0,\n",
       " 'D6': 0.7499999999999999}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevance = calculer_relevance('effect distribution wing slipstream experiment investig aerodynam','Split', 'Porter', 'DPT',path)\n",
    "relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "\n",
    "def calculer_relevance_cosinus(query, tokenization, normalization, file_type, path):\n",
    "    descripteur, _ = open_descripteur_invers(normalization, tokenization, output_path)\n",
    "    \n",
    "    # Calcul de la norme des vecteurs des documents (somme des poids au carré)\n",
    "    poids = {doc: sum((term_data[\"poids\"])**2 for term_data in terms.values()) for doc, terms in descripteur.items()}\n",
    "  \n",
    "    \n",
    "    # Création de la liste des documents\n",
    "    docs = [doc.split('.')[0] for doc in os.listdir(path)]\n",
    "    \n",
    "    # Initialisation des vecteurs de documents\n",
    "    doc_vectors = {doc: {} for doc in docs}\n",
    "    \n",
    "    # Remplir les vecteurs des documents\n",
    "    for term in query.split():\n",
    "        occurrence, _, _ = processing(term, tokenization, normalization, path, output_path=\"output\", methode=file_type)\n",
    "        for occ in occurrence:\n",
    "            doc_name = occ[2]  # Nom du document\n",
    "            poids_terme = occ[4]  # Poids du terme\n",
    "            doc_vectors[doc_name][term] = poids_terme\n",
    "    # doc_vectors =>   {'D1': {'effect': 0.0795, 'distribution': 0.0663}, 'D2': {}, 'D3': {'effect': 0.1193, 'distribution': 0.0995}, 'D4': {'distribution': 0.0332}, 'D5': {}, 'D6': {'effect': 0.1193, 'distribution': 0.0995}}\n",
    "    print(f\"doc vectors {doc_vectors}\")\n",
    "    \n",
    "    # Calcul de la similarité cosinus\n",
    "    relevance_dict = {}\n",
    "    for doc_name, doc_vector in doc_vectors.items():\n",
    "        # doc_name, doc_vector => 'D1': {'effect': 0.0795, 'distribution': 0.0663}       \n",
    "        dot_product = sum(doc_vector.get(term, 0) for term in query.split())  # Produit scalaire\n",
    "        norm_query = math.sqrt(len(query.split()))  # Norme de la requête (poids uniformes)\n",
    "        norm_doc = math.sqrt(poids.get(doc_name, 0))  # Norme du document\n",
    "        similarity = dot_product / (norm_query * norm_doc) if norm_query > 0 and norm_doc > 0 else 0\n",
    "        \n",
    "        relevance_dict[doc_name] = similarity\n",
    "\n",
    "    return relevance_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc vectors {'D1': {'effect': 0.0795, 'distribution': 0.0663}, 'D2': {}, 'D3': {'effect': 0.1193, 'distribution': 0.0995}, 'D4': {'distribution': 0.0332}, 'D5': {}, 'D6': {'effect': 0.1193, 'distribution': 0.0995}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'D1': 0.06900726067964397,\n",
       " 'D2': 0.0,\n",
       " 'D3': 0.12320148746796511,\n",
       " 'D4': 0.024022928446014405,\n",
       " 'D5': 0.0,\n",
       " 'D6': 0.08426610754279582}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevance_dict = calculer_relevance_cosinus('effect distribution','Split', 'None', 'DPT',path)\n",
    "relevance_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculer_jaccard_similarity(query, tokenization, normalization, file_type, path):\n",
    "    \n",
    "    descripteur, _ = open_descripteur_invers(normalization, tokenization, output_path)\n",
    "    \n",
    "    poids = {doc: sum((term_data[\"poids\"])**2 for term_data in terms.values()) for doc, terms in descripteur.items()}\n",
    "    \n",
    "    \n",
    "    docs = [doc.split('.')[0] for doc in os.listdir(path)]\n",
    "    \n",
    "    # Initialisation des vecteurs de documents\n",
    "    doc_vectors = {doc: {} for doc in docs}\n",
    "    \n",
    "    # Remplir les vecteurs des documents\n",
    "    for term in query.split():\n",
    "        occurrence, _, _ = processing(term, tokenization, normalization, path, output_path=\"output\", methode=file_type)\n",
    "        for occ in occurrence:\n",
    "            doc_name = occ[2]  # Nom du document\n",
    "            poids_terme = occ[4]  # Poids du terme\n",
    "            doc_vectors[doc_name][term] = poids_terme\n",
    "            \n",
    "\n",
    "    # Calcul de la mesure de Jaccard pour chaque document\n",
    "    relevance_dict = {}\n",
    "    for doc_name, doc_vector in doc_vectors.items():\n",
    "        print(f\"doc_name, doc_vector : {doc_name, doc_vector}\")\n",
    "        # doc_name, doc_vector => 'D1': {'effect': 0.0795, 'distribution': 0.0663}       \n",
    "        dot_product = sum(doc_vector.get(term, 0) for term in query.split())  # Produit scalaire\n",
    "        \n",
    "        norm_query = len(query.split())  # Norme de la requête (poids uniformes)\n",
    "        norm_doc = poids.get(doc_name, 0) # Norme du document\n",
    "        union = norm_doc+norm_query-dot_product\n",
    "        similarity =  (dot_product)/(union)if union > 0 else 0\n",
    "        \n",
    "        relevance_dict[doc_name] = similarity\n",
    "\n",
    "    return relevance_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_name, doc_vector : ('D1', {'effect': 0.0795, 'distribution': 0.0663})\n",
      "doc_name, doc_vector : ('D2', {})\n",
      "doc_name, doc_vector : ('D3', {'effect': 0.1193, 'distribution': 0.0995})\n",
      "doc_name, doc_vector : ('D4', {'distribution': 0.0332})\n",
      "doc_name, doc_vector : ('D5', {})\n",
      "doc_name, doc_vector : ('D6', {'effect': 0.1193, 'distribution': 0.0995})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'D1': 0.035681015160415504,\n",
       " 'D2': 0.0,\n",
       " 'D3': 0.0651538779902336,\n",
       " 'D4': 0.011362936292260204,\n",
       " 'D5': 0.0,\n",
       " 'D6': 0.0424672942054332}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jaccard_relevance = calculer_jaccard_similarity('effect distribution', 'Split', 'None', 'DPT', path='../Collections')\n",
    "jaccard_relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyparsing import infixNotation, opAssoc, Word, alphas, ParseException  # Import des modules nécessaires\n",
    "# Une bibliothèque pour analyser du texte et vérifier s'il convient à une certaine grammaire (comme ici pour des requêtes logiques).\n",
    "# infixNotation : Permet de définir une grammaire avec des opérateurs comme AND, OR, NOT.\n",
    "# opAssoc : Définit l'ordre dans lequel les opérateurs sont évalués.\n",
    "# Word, alphas : Permet de reconnaître des mots composés uniquement de lettres (comme \"chien\").\n",
    "\n",
    "# Définition des mots-clés logiques\n",
    "AND = \"AND\"\n",
    "OR = \"OR\"\n",
    "NOT = \"NOT\"\n",
    "\n",
    "# Classe représentant un terme simple\n",
    "class Term:\n",
    "    def __init__(self, tokens):\n",
    "        self.term = tokens[0]  # Le terme est le premier élément de la liste des tokens\n",
    "\n",
    "    def eval(self, doc):\n",
    "        return self.term in doc  # Le terme doit être présent dans le document\n",
    "\n",
    "\n",
    "class BooleanOp:\n",
    "    def __init__(self, tokens):\n",
    "        self.args = tokens[0][0::2]  # Extrait les arguments\n",
    "        # Transforme les arguments en instances de `Term` si ce n'est pas déjà fait\n",
    "        self.args = [arg if isinstance(arg, Term) else Term([arg]) for arg in self.args]\n",
    "\n",
    "class AndOp(BooleanOp):\n",
    "    def eval(self, doc):\n",
    "        return all(arg.eval(doc) for arg in self.args)\n",
    "\n",
    "class OrOp(BooleanOp):\n",
    "    def eval(self, doc):\n",
    "        return any(arg.eval(doc) for arg in self.args)\n",
    "\n",
    "class NotOp(BooleanOp):\n",
    "    def eval(self, doc):\n",
    "        return not self.args[0].eval(doc)\n",
    "\n",
    "# Définition de la grammaire des termes et des opérateurs\n",
    "term = Word(alphas).setParseAction(Term)  # Un terme est simplement un mot composé de lettres\n",
    "    # Word(alphas): Reconnaît des mots composés uniquement de lettres (comme \"chien\").\n",
    "    # setParseAction(Term): Chaque mot reconnu est transformé en un objet de la classe Term.\n",
    "expr = infixNotation(term, [\n",
    "    (NOT, 1, opAssoc.RIGHT, NotOp),  # Le NOT est évalué de droite à gauche\n",
    "    (AND, 2, opAssoc.LEFT, AndOp),   # Le AND est évalué de gauche à droite\n",
    "    (OR, 2, opAssoc.LEFT, OrOp),     # Le OR est évalué de gauche à droite\n",
    "])\n",
    "\n",
    "# Fonction pour vérifier la requête booléenne\n",
    "def verifier_requete_booleenne(query, doc_vectors):\n",
    "    try:\n",
    "        # Analyser la requête avec la grammaire définie\n",
    "        parsed_query = expr.parseString(query, parseAll=True)[0]\n",
    "    except ParseException as e:\n",
    "        # Si la requête n'est pas valide, lever une exception\n",
    "        raise ValueError(f\"Requête booléenne invalide : {str(e)}\")\n",
    "\n",
    "    # Dictionnaire pour stocker les résultats de l'évaluation\n",
    "    pertinence_dict = {}\n",
    "\n",
    "    # Évaluer la requête pour chaque document\n",
    "    for doc_name, doc_vector in doc_vectors.items():\n",
    "        pertinence_dict[doc_name] = parsed_query.eval(doc_vector)\n",
    "\n",
    "    # Retourner le dictionnaire des résultats\n",
    "    return pertinence_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_booleen(query, tokenization, normalization, file_type, path,output_path=\"output\"):\n",
    "    # query => effect and ditribtion or snow \n",
    "    \n",
    "   \n",
    "    docs = [doc.split('.')[0] for doc in os.listdir(path)]\n",
    "    \n",
    "    # Initialisation des vecteurs de documents\n",
    "    doc_vectors = {doc: {} for doc in docs}\n",
    "    \n",
    "    # Mots-clés logiques\n",
    "    logical_operators = {\"AND\", \"OR\", \"NOT\"}\n",
    "    \n",
    "    # Séparer les termes des opérateurs logiques\n",
    "    terms = [term for term in query.split() if term not in logical_operators]\n",
    "    \n",
    "    # Remplir les vecteurs des documents\n",
    "    for term in terms:\n",
    "        occurrence, _, _ = processing(term, tokenization, normalization, path, output_path=\"output\", methode=file_type)\n",
    "        for occ in occurrence:\n",
    "            doc_name = occ[2]  # Nom du document\n",
    "            poids_terme = occ[4]  # Poids du terme\n",
    "            doc_vectors[doc_name][term] = poids_terme\n",
    "    print(doc_vectors)\n",
    "   \n",
    "    resultats = verifier_requete_booleenne(query, doc_vectors)\n",
    "    resultats = [(doc , val) for doc , val in resultats.items()]\n",
    "    return resultats\n",
    "    \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'D1': {'effect': 0.0795, 'distribution': 0.0663}, 'D2': {}, 'D3': {'effect': 0.1193, 'distribution': 0.0995}, 'D4': {'distribution': 0.0332}, 'D5': {}, 'D6': {'effect': 0.1193, 'distribution': 0.0995}}\n"
     ]
    }
   ],
   "source": [
    "res = model_booleen(\"NOT effect AND NOT distribution\", \"Split\", \"None\", \"DPT\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('D1', False), ('D2', False), ('D3', False), ('D4', False), ('D5', False), ('D6', False)]\n"
     ]
    }
   ],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SearchApp(QMainWindow):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.path = 'Collections'\n",
    "        self.setWindowTitle(\"Document Search and Processing\")\n",
    "        self.setGeometry(100, 100, 1170, 800) #8,6 => 900?700 \n",
    "        self.setWindowIcon(QIcon(\"./icons/interface_icon.png\")) \n",
    "        self.setFixedSize(1170, 800) #900:700\n",
    "        \n",
    "        \n",
    "        # Layout principal\n",
    "        central_widget = QWidget()\n",
    "        self.setCentralWidget(central_widget)\n",
    "        self.main_layout = QVBoxLayout(central_widget)\n",
    "\n",
    "        # Barre de recherche\n",
    "        search_layout = QHBoxLayout()\n",
    "        query_label = QLabel(\"Query: \", self)\n",
    "        search_layout.addWidget(query_label)\n",
    "        \n",
    "        self.search_bar = QLineEdit(self)\n",
    "        self.search_bar.setPlaceholderText(\"Enter document name...\")\n",
    "        self.search_button = QPushButton(\"Search\", self)\n",
    "        \n",
    "        search_layout.addWidget(self.search_bar)\n",
    "        search_layout.addWidget(self.search_button)\n",
    "        self.main_layout.addLayout(search_layout)\n",
    "\n",
    "        # Options de radio\n",
    "        radio_layout = QHBoxLayout()\n",
    "        self.raw_text_radio = QRadioButton(\"Raw Text\", self)\n",
    "        self.processed_text_radio = QRadioButton(\"Processed Text\", self)\n",
    "        radio_layout.addWidget(self.raw_text_radio)\n",
    "        radio_layout.addWidget(self.processed_text_radio)\n",
    "        self.main_layout.addLayout(radio_layout)\n",
    "        \n",
    "        # Section Tokenization\n",
    "        tokenization_box = QGroupBox(\"Tokenization\")\n",
    "        tokenization_layout = QVBoxLayout()\n",
    "        self.split_radio = QRadioButton(\"Split\", self)\n",
    "        self.regex_radio = QRadioButton(\"Regex\", self)\n",
    "        tokenization_layout.addWidget(self.split_radio)\n",
    "        tokenization_layout.addWidget(self.regex_radio)\n",
    "        tokenization_box.setLayout(tokenization_layout)\n",
    "        \n",
    "        # Section Normalization\n",
    "        normalization_box = QGroupBox(\"Normalization\")\n",
    "        normalization_layout = QVBoxLayout()\n",
    "        self.no_stem_radio = QRadioButton(\"No Stem\", self)\n",
    "        self.porter_radio = QRadioButton(\"Porter\", self)\n",
    "        self.lancaster_radio = QRadioButton(\"Lancaster\", self)\n",
    "        normalization_layout.addWidget(self.no_stem_radio)\n",
    "        normalization_layout.addWidget(self.porter_radio)\n",
    "        normalization_layout.addWidget(self.lancaster_radio)\n",
    "        normalization_box.setLayout(normalization_layout)\n",
    "        \n",
    "        # Section Indexation\n",
    "        indexation_box = QGroupBox(\"Indexation\")\n",
    "        indexation_layout = QVBoxLayout()\n",
    "        self.doc_per_term_radio = QRadioButton(\"Documents per Term\", self)\n",
    "        self.term_per_doc_radio = QRadioButton(\"Terms per Document\", self)\n",
    "        indexation_layout.addWidget(self.doc_per_term_radio)\n",
    "        indexation_layout.addWidget(self.term_per_doc_radio)\n",
    "        indexation_box.setLayout(indexation_layout)\n",
    "        \n",
    "        \n",
    "        matching_box = QGroupBox(\"Matching\")\n",
    "        matching_layout = QGridLayout()\n",
    "\n",
    "        # Boutons radio\n",
    "        self.vector_space_radio = QRadioButton(\"Vector Space Model\")\n",
    "        self.probability_model_radio = QRadioButton(\"Probabilistic Model (BM25)\")\n",
    "        self.boolean_model_radio = QRadioButton(\"Boolean Model\")\n",
    "        self.data_mining_model_radio = QRadioButton(\"Data Mining Model\")\n",
    "\n",
    "        # Menu déroulant pour \"Vector Space Model\"\n",
    "        self.matching_options = QComboBox()\n",
    "        self.matching_options.addItems([\"Scalar Product\", \"Cosine Similarity\", \"Jaccard Index\"])\n",
    "\n",
    "        # Champs de saisie pour k et b sous \"Probabilistic Model\"\n",
    "        self.k_input = QLineEdit()\n",
    "        validator = QDoubleValidator()\n",
    "        validator.setNotation(QDoubleValidator.StandardNotation)  # Autorise les notations standard (pas scientifiques)\n",
    "        validator.setRange(-1000.0, 1000.0, 3)  # Plage de valeurs entre -1000 et 1000 avec 3 décimales max\n",
    "        self.k_input.setValidator(validator)\n",
    "        self.k_input.setPlaceholderText(\"K\")\n",
    "\n",
    "        self.b_input = QLineEdit()\n",
    "        self.b_input.setValidator(validator)  # Utilisez le même validateur pour `b_input`\n",
    "        self.b_input.setPlaceholderText(\"B\")\n",
    "\n",
    "        # Ajout des widgets au layout (organisé par colonnes et lignes)\n",
    "        matching_layout.addWidget(self.vector_space_radio, 0, 0)  # Ligne 0, Colonne 0\n",
    "        matching_layout.addWidget(self.matching_options, 0, 1)     # Ligne 0, Colonne 1\n",
    "        matching_layout.addWidget(self.probability_model_radio, 1, 0)  # Ligne 1, Colonne 0\n",
    "        matching_layout.addWidget(self.k_input, 1, 1)             # Ligne 1, Colonne 1\n",
    "        matching_layout.addWidget(self.b_input, 1, 2)             # Ligne 1, Colonne 2\n",
    "        matching_layout.addWidget(self.boolean_model_radio, 2, 0) # Ligne 2, Colonne 0\n",
    "        matching_layout.addWidget(self.data_mining_model_radio, 3, 0)  # Ligne 3, Colonne 0\n",
    "\n",
    "        # Appliquer le layout au QGroupBox\n",
    "        matching_box.setLayout(matching_layout)\n",
    "\n",
    "       \n",
    "       \n",
    "        \n",
    "        # Disposition des sections\n",
    "        sections_layout = QHBoxLayout()\n",
    "        sections_layout.addWidget(tokenization_box)\n",
    "        sections_layout.addWidget(normalization_box)\n",
    "        sections_layout.addWidget(indexation_box)\n",
    "        sections_layout.addWidget(matching_box) \n",
    "        self.main_layout.addLayout(sections_layout)\n",
    "        \n",
    "        # Zone de résultats (QStackedWidget pour alterner entre texte et tableau)\n",
    "        self.result_label = QLabel(\"Result: \", self)\n",
    "        self.main_layout.addWidget(self.result_label)\n",
    "\n",
    "        self.result_area = QStackedWidget(self)\n",
    "        self.result_area.setFixedHeight(400)  # Taille fixe pour éviter d'étendre la mise en page\n",
    "        self.result_area.setFixedWidth(600)  # Ajustez selon la largeur désirée\n",
    "        \n",
    "        \n",
    "        # Widget pour afficher le texte brut\n",
    "        self.raw_text_widget = QTextEdit(self)\n",
    "        self.raw_text_widget.setReadOnly(True)  # Rendre le texte en lecture seule\n",
    "        self.result_area.addWidget(self.raw_text_widget)\n",
    "        \n",
    "        # Widget pour afficher le tableau\n",
    "        self.table = QTableWidget(0, 5, self)  # 5 colonnes pour N°, N° doc, terme, fréquence, poids\n",
    "        self.table.setHorizontalHeaderLabels([\"N°\", \"N° doc\", \"Term\", \"Frequency\", \"Weight\"])\n",
    "        self.table.setShowGrid(False)  # Masquer la grille du tableau\n",
    "        \n",
    "        # Faire en sorte que les colonnes s'étendent pour couvrir toute la largeur\n",
    "        header = self.table.horizontalHeader()\n",
    "        header.setSectionResizeMode(QHeaderView.Stretch)\n",
    "        self.result_area.addWidget(self.table)\n",
    "        \n",
    "        self.main_layout.addWidget(self.result_area)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #    //////////////////////////////////////\n",
    "        Total_terms_layout = QHBoxLayout()\n",
    "        \n",
    "        # Création et configuration des QLabel\n",
    "        self.terms_per_doc = QLabel(self)\n",
    "        self.terms_all_doc = QLabel(self)\n",
    "        \n",
    "        # Appliquer les styles pour enlever le fond et les bordures\n",
    "        style = \"\"\"\n",
    "            QLabel {\n",
    "                margin-left: 20px;\n",
    "                background-color: transparent;\n",
    "                border: none;\n",
    "                font-size: 14px;\n",
    "                font-family: Arial, sans-serif;\n",
    "            }\n",
    "        \"\"\"\n",
    "        self.terms_all_doc.setStyleSheet(style)\n",
    "        self.terms_per_doc.setStyleSheet(style)\n",
    "        \n",
    "        \n",
    "        # Ajout des QLabel au layout horizontal\n",
    "        Total_terms_layout.addWidget(self.terms_per_doc)\n",
    "        Total_terms_layout.addWidget(self.terms_all_doc)\n",
    "        \n",
    "        # Ajout du layout horizontal dans le layout principal\n",
    "        self.main_layout.addLayout(Total_terms_layout)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # Ajustements dans le code principal\n",
    "        self.main_layout.setContentsMargins(2, 2, 2, 2)  # Réduire les marges globales\n",
    "        self.main_layout.setSpacing(8)  # Diminuer l'espace entre les sections\n",
    "        self.result_area.setContentsMargins(2, 0, 2, 0)  # Marges gauche et droite de 2px pour le tableau\n",
    "        self.result_area.setFixedWidth(self.width() - 4) \n",
    "                \n",
    "                \n",
    "        # events\n",
    "        self.search_button.clicked.connect(self.process_search)\n",
    "        self.raw_text_radio.clicked.connect(self.raw_text_radio_process)\n",
    "        self.processed_text_radio.clicked.connect(self.processed_text_radio_process)\n",
    "        self.vector_space_radio.toggled.connect(self.toggle_radio_buttons)\n",
    "        \n",
    "    def toggle_radio_buttons(self,state):\n",
    "        self.doc_per_term_radio.setEnabled(not state) \n",
    "        self.term_per_doc_radio.setEnabled(not state)\n",
    "        \n",
    "    def raw_text_radio_process(self):\n",
    "        self.vector_space_radio.setEnabled(False)\n",
    "        self.matching_options.setEnabled(False)\n",
    "        self.split_radio.setEnabled(False) \n",
    "        self.regex_radio.setEnabled(False)\n",
    "        self.lancaster_radio.setEnabled(False)\n",
    "        self.porter_radio.setEnabled(False)\n",
    "        self.doc_per_term_radio.setEnabled(False)\n",
    "        self.term_per_doc_radio.setEnabled(False)\n",
    "        self.no_stem_radio.setEnabled(False)\n",
    "        self.terms_per_doc.setText(\"\")\n",
    "        self.terms_all_doc.setText(\"\")\n",
    "       \n",
    "       \n",
    "    def processed_text_radio_process(self):\n",
    "        self.vector_space_radio.setEnabled(True)\n",
    "        self.matching_options.setEnabled(True)\n",
    "        self.split_radio.setEnabled(True) \n",
    "        self.regex_radio.setEnabled(True)\n",
    "        self.lancaster_radio.setEnabled(True)\n",
    "        self.porter_radio.setEnabled(True)\n",
    "        self.doc_per_term_radio.setEnabled(True)\n",
    "        self.term_per_doc_radio.setEnabled(True)\n",
    "        self.no_stem_radio.setEnabled(True)\n",
    "        \n",
    "         \n",
    "    def display_Total_Terms(self, termes_global, nb_termes ,index):\n",
    "        if nb_termes != 0 and index == 'TPD':\n",
    "            # Afficher le nombre de termes par document\n",
    "            self.terms_per_doc.setText(f\"Terms per document : {nb_termes}\")\n",
    "        else :\n",
    "            self.terms_per_doc.setText(\"\")\n",
    "        self.terms_all_doc.setText(f\"Total terms  : {termes_global}\")\n",
    "\n",
    "        \n",
    "        \n",
    "    def process_search(self):\n",
    "        # Obtenir le numéro de document\n",
    "        document_number = self.search_bar.text()\n",
    "        \n",
    "        if not document_number:\n",
    "            self.show_error(\"Veuillez entrer un numéro de document valide.\")\n",
    "            return\n",
    "\n",
    "        # \n",
    "        # Vérifier le type de texte sélectionné\n",
    "        if self.raw_text_radio.isChecked():\n",
    "            # verification de nom_document\n",
    "            result = get_text(document_number)\n",
    "            self.show_raw_text(result)\n",
    "        \n",
    "        else:\n",
    "            # Obtenir les méthodes sélectionnées\n",
    "            tokenization_method = \"Split\" if self.split_radio.isChecked() else \"Regex\"\n",
    "            if self.porter_radio.isChecked() :\n",
    "                normalization_method = \"Porter\" \n",
    "            elif self.no_stem_radio.isChecked():\n",
    "                normalization_method = \"None\" \n",
    "            else :\n",
    "                normalization_method =\"Lancaster\"\n",
    "            indexation_method = \"DPT\" if self.doc_per_term_radio.isChecked() else \"TPD\"\n",
    "            \n",
    "            if self.vector_space_radio.isChecked():\n",
    "            # Récupération de la méthode de matching sélectionnée\n",
    "                matching_method = self.matching_options.currentText()\n",
    "                # Utilisation\n",
    "                \n",
    "                if matching_method == \"Scalar Product\":\n",
    "                    print(\"produit scalaire\")\n",
    "                    relevances = calculer_relevance(document_number, tokenization_method, normalization_method, 'DPT',self.path)\n",
    "                    print(relevances)\n",
    "\n",
    "                elif matching_method == \"Cosine Similarity\":\n",
    "                    print(\"produit cosine\")\n",
    "                    relevances = calculer_relevance_cosinus(document_number, tokenization_method, normalization_method, 'DPT',self.path)\n",
    "                    print(relevances)\n",
    "\n",
    "                elif matching_method == \"Jaccard Index\":\n",
    "                    print(\"jacard\")\n",
    "                    relevances = calculer_jaccard_similarity(document_number, tokenization_method, normalization_method, 'DPT',self.path)\n",
    "                    print(relevances)\n",
    "                filtered_relevances = {doc: score for doc, score in relevances.items() if score > 0}\n",
    "                sorted_relevances = sorted(filtered_relevances.items(), key=lambda item: item[1], reverse=True)\n",
    "                print(sorted_relevances)\n",
    "                # Afficher les relevances triées\n",
    "                self.display_relevance(sorted_relevances)\n",
    "            elif self.probability_model_radio.isChecked():\n",
    "                try:\n",
    "                    text = self.k_input.text()\n",
    "                    corrected_text_k = text.replace(\",\", \".\")\n",
    "                    text = self.b_input.text()\n",
    "                    corrected_text_b = text.replace(\",\", \".\")\n",
    "                    k = float(corrected_text_k)  # Convertit en flottant\n",
    "                    b = float(corrected_text_b)\n",
    "                except ValueError:\n",
    "                    QMessageBox.warning(self, \"Invalid Input\", \"Please enter valid numeric values for k and b.\")\n",
    "                    k = 1.5  # Valeur par défaut si erreur\n",
    "                    b = 0.75  # Valeur par défaut si erreur\n",
    "                print(k,b)\n",
    "                relevances = calculer_relevance_BM25(document_number, tokenization_method, normalization_method, self.path ,k ,b,'output')\n",
    "                # filtered_relevances = {doc: score for doc, score in relevances.items() if score > 0}\n",
    "                # sorted_relevances = sorted(filtered_relevances.items(), key=lambda item: item[1], reverse=True)\n",
    "                print(\"BM25\")\n",
    "                print(relevances)\n",
    "                relevances = list(relevances.items())\n",
    "                self.display_relevance(relevances)\n",
    "            elif self.boolean_model_radio.isChecked():\n",
    "                try :\n",
    "                    print(document_number, tokenization_method, normalization_method)\n",
    "                    relevances = model_booleen(document_number, tokenization_method, normalization_method, \"DPT\", self.path,output_path=\"output\")\n",
    "                    self.display_relevance(relevances)\n",
    "                except Exception as e :\n",
    "                    print(e)\n",
    "                    QMessageBox.warning(self, \"Invalid Input\", \"Please enter valid Expression.\")\n",
    "                    \n",
    "            else :\n",
    "                # termes_global = nb_termes_glob(tokenization_method, normalization_method)\n",
    "                # Appeler la fonction pour obtenir les données\n",
    "                data , nb_termes , termes_global=processing(document_number, tokenization_method,  normalization_method, path='../Collections', output_path=\"output\",methode = indexation_method)\n",
    "                self.display_results(data)\n",
    "                self.display_Total_Terms(termes_global , nb_termes , indexation_method)\n",
    "                \n",
    "                     \n",
    "\n",
    "    def reset_table(self, column_headers):\n",
    "        \"\"\"\n",
    "        Réinitialise complètement le tableau avec de nouvelles colonnes et leurs en-têtes.\n",
    "\n",
    "        Parameters:\n",
    "            column_headers (list of str): Liste des noms des colonnes.\n",
    "        \"\"\"\n",
    "        self.table.clear()  # Efface tout le contenu (cellules et en-têtes)\n",
    "        self.table.setRowCount(0)  # Réinitialise le nombre de lignes\n",
    "        self.table.setColumnCount(len(column_headers))  # Définir le nombre de colonnes\n",
    "        self.table.setHorizontalHeaderLabels(column_headers)  # Définir les en-têtes de colonnes\n",
    "\n",
    "        # Ajuster les colonnes pour s'étendre uniformément\n",
    "        header = self.table.horizontalHeader()\n",
    "        header.setSectionResizeMode(QHeaderView.Stretch)\n",
    "\n",
    "\n",
    "    def show_raw_text(self, text):\n",
    "        self.raw_text_widget.setText(text)\n",
    "        self.result_area.setCurrentWidget(self.raw_text_widget)  # Afficher le widget de texte brut\n",
    "\n",
    "        \n",
    "    def display_results(self, data):\n",
    "    # Supprimer l'affichage de l'index de ligne\n",
    "        self.table.verticalHeader().setVisible(False)\n",
    "        # Réinitialiser le tableau avec les colonnes spécifiques\n",
    "        column_headers = [\"N°\", \"N° doc\", \"Term\", \"Frequency\", \"Weight\"]\n",
    "        self.reset_table(column_headers)\n",
    "        \n",
    "        # Nettoyer le tableau et ajouter les résultats\n",
    "        self.table.setRowCount(0)\n",
    "        for index, row_data in enumerate(data):\n",
    "            row_position = self.table.rowCount()\n",
    "            self.table.insertRow(row_position)\n",
    "            for column, value in enumerate(row_data):\n",
    "                item = QTableWidgetItem(str(value))\n",
    "                item.setTextAlignment(Qt.AlignCenter)  # Centrer le texte dans chaque cellule\n",
    "                self.table.setItem(row_position, column, item)\n",
    "        \n",
    "        # Afficher le widget de tableau\n",
    "        self.result_area.setCurrentWidget(self.table)\n",
    "    \n",
    "\n",
    "\n",
    "    def display_relevance(self, sorted_relevances):\n",
    "        \"\"\"\n",
    "        Affiche les relevances triées dans le tableau.\n",
    "\n",
    "        Parameters:\n",
    "            sorted_relevances (list of tuples): Une liste de tuples (document, score de pertinence).\n",
    "        \"\"\"\n",
    "        # Réinitialiser le tableau avec deux colonnes\n",
    "        column_headers = [\"Document\", \"Score de Pertinence\"]\n",
    "        self.reset_table(column_headers)\n",
    "\n",
    "        # Ajouter les données triées dans le tableau\n",
    "        for doc_id, score in sorted_relevances:\n",
    "            row_position = self.table.rowCount()\n",
    "            self.table.insertRow(row_position)\n",
    "\n",
    "            # Ajouter l'identifiant du document\n",
    "            doc_item = QTableWidgetItem(doc_id)\n",
    "            doc_item.setTextAlignment(Qt.AlignCenter)\n",
    "            self.table.setItem(row_position, 0, doc_item)\n",
    "\n",
    "            # Ajouter le score de pertinence\n",
    "            if isinstance(score, bool):\n",
    "                score_item = QTableWidgetItem(\"True\" if score else \"False\")\n",
    "            elif isinstance(score, (int, float)):\n",
    "                score_item = QTableWidgetItem(f\"{score:.4f}\")\n",
    "            else:\n",
    "                raise ValueError(f\"Type non pris en charge : {type(score)}\")\n",
    "\n",
    "            score_item.setTextAlignment(Qt.AlignCenter)\n",
    "            self.table.setItem(row_position, 1, score_item)\n",
    "\n",
    "        # Afficher le tableau\n",
    "        self.result_area.setCurrentWidget(self.table)\n",
    "\n",
    "        \n",
    "    \n",
    "        \n",
    "        \n",
    "    def show_error(self, message):\n",
    "        error_dialog = QMessageBox(self)\n",
    "        error_dialog.setIcon(QMessageBox.Critical)\n",
    "        error_dialog.setWindowTitle(\"Erreur\")\n",
    "        error_dialog.setText(message)\n",
    "        error_dialog.exec_()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "produit scalaire\n",
      "[(1, 'effect', 'D1', 1, 0.0795), (2, 'effect', 'D3', 1, 0.1193), (3, 'effect', 'D6', 1, 0.1193)]\n",
      "[(1, 'distribution', 'D1', 1, 0.0663), (2, 'distribution', 'D3', 1, 0.0995), (3, 'distribution', 'D4', 1, 0.0332), (4, 'distribution', 'D6', 1, 0.0995)]\n",
      "{'D1': 0.14579999999999999, 'D2': 0, 'D3': 0.2188, 'D4': 0.0332, 'D5': 0, 'D6': 0.2188}\n",
      "[('D3', 0.2188), ('D6', 0.2188), ('D1', 0.14579999999999999), ('D4', 0.0332)]\n",
      "produit cosine\n",
      "doc vectors {'D1': {'effect': 0.0795, 'distribution': 0.0663}, 'D2': {}, 'D3': {'effect': 0.1193, 'distribution': 0.0995}, 'D4': {'distribution': 0.0332}, 'D5': {}, 'D6': {'effect': 0.1193, 'distribution': 0.0995}}\n",
      "{'D1': 0.06900726067964397, 'D2': 0.0, 'D3': 0.12320148746796511, 'D4': 0.024022928446014405, 'D5': 0.0, 'D6': 0.08426610754279582}\n",
      "[('D3', 0.12320148746796511), ('D6', 0.08426610754279582), ('D1', 0.06900726067964397), ('D4', 0.024022928446014405)]\n",
      "jacard\n",
      "doc_name, doc_vector : ('D1', {'effect': 0.0795, 'distribution': 0.0663})\n",
      "doc_name, doc_vector : ('D2', {})\n",
      "doc_name, doc_vector : ('D3', {'effect': 0.1193, 'distribution': 0.0995})\n",
      "doc_name, doc_vector : ('D4', {'distribution': 0.0332})\n",
      "doc_name, doc_vector : ('D5', {})\n",
      "doc_name, doc_vector : ('D6', {'effect': 0.1193, 'distribution': 0.0995})\n",
      "{'D1': 0.035681015160415504, 'D2': 0.0, 'D3': 0.0651538779902336, 'D4': 0.011362936292260204, 'D5': 0.0, 'D6': 0.0424672942054332}\n",
      "[('D3', 0.0651538779902336), ('D6', 0.0424672942054332), ('D1', 0.035681015160415504), ('D4', 0.011362936292260204)]\n",
      "1.0 0.75\n",
      "{'D4': -0.11284468146320148, 'D1': -0.14044256551670187, 'D6': -0.14424157007984148, 'D3': -0.16174223241018335}\n",
      "BM25\n",
      "{'D4': -0.11284468146320148, 'D1': -0.14044256551670187, 'D6': -0.14424157007984148, 'D3': -0.16174223241018335}\n",
      "effect AND distribution Split None\n",
      "effect OR distribution Split None\n",
      "effect OR distribution AND snow Split None\n",
      "effect OR distribution AND NOT snow Split None\n",
      "'str' object has no attribute 'eval'\n",
      "NOT effect OR distribution AND snow Split None\n",
      "'str' object has no attribute 'eval'\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    app = QApplication(sys.argv)\n",
    "    app.setStyleSheet(\"\"\"\n",
    "    QMainWindow {\n",
    "        background-color: #f5f5f5;\n",
    "    }\n",
    "    QLabel {\n",
    "        color: #333333;\n",
    "        font-size: 14px;\n",
    "    }\n",
    "    QLineEdit {\n",
    "        border: 1px solid #CCCCCC;\n",
    "        border-radius: 5px;\n",
    "        padding: 5px;\n",
    "    }\n",
    "    QPushButton {\n",
    "        background-color: #4CAF50;\n",
    "        color: white;\n",
    "        font-size: 14px;\n",
    "        padding: 5px 10px;\n",
    "        border-radius: 5px;\n",
    "    }\n",
    "    QPushButton:hover {\n",
    "        background-color: #45a049;\n",
    "    }\n",
    "    \n",
    "    QRadioButton {\n",
    "        font-size: 13px;\n",
    "    }\n",
    "    QGroupBox {\n",
    "        font-size: 15px;\n",
    "        color: #333333;\n",
    "        border: 1px solid #CCCCCC;\n",
    "        border-radius: 8px;\n",
    "        margin-top: 10px;\n",
    "        padding: 10px;\n",
    "    }\n",
    "    QTextEdit {\n",
    "        background-color: #f0f0f0;\n",
    "        border: 1px solid #CCCCCC;\n",
    "        border-radius: 5px;\n",
    "        padding: 5px;\n",
    "    }\n",
    "    QTableWidget {\n",
    "        background-color: #FFFFFF;\n",
    "        border: 1px solid #CCCCCC;\n",
    "        border-radius: 5px;\n",
    "        padding: 2px;\n",
    "        gridline-color: #E0E0E0;\n",
    "    }\n",
    "    QTableWidget::item {\n",
    "        padding: 5px;\n",
    "        border-bottom: 1px solid #E0E0E0;\n",
    "    }\n",
    "    QHeaderView::section {\n",
    "        background-color: #f0f0f0;\n",
    "        padding: 5px;\n",
    "        border: 1px solid #CCCCCC;\n",
    "        font-weight: bold;\n",
    "    }\n",
    "\n",
    "    QLabel {\n",
    "        margin-left: 20px;\n",
    "        background-color: transparent;\n",
    "        border: none;\n",
    "        font-size: 14px;\n",
    "        font-family: Arial, sans-serif;\n",
    "    }\n",
    "    \n",
    "\"\"\")\n",
    "\n",
    "    window = SearchApp()\n",
    "    window.show()\n",
    "    sys.exit(app.exec_())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('D4', -0.08823075947891026)\n",
      "('D2', -0.09795556690944766)\n",
      "('D1', -0.2728925083902616)\n",
      "('D3', -0.27339867077595187)\n",
      "('D6', -0.3028044304368359)\n",
      "[('D4', -0.08823075947891026), ('D2', -0.09795556690944766), ('D1', -0.2728925083902616), ('D3', -0.27339867077595187), ('D6', -0.3028044304368359)]\n",
      "[('D4', -0.08823075947891026), ('D2', -0.09795556690944766), ('D1', -0.2728925083902616), ('D3', -0.27339867077595187), ('D6', -0.3028044304368359)]\n"
     ]
    }
   ],
   "source": [
    "data = {'D4': -0.08823075947891026, 'D2': -0.09795556690944766, 'D1': -0.2728925083902616, 'D3': -0.27339867077595187, 'D6': -0.3028044304368359}\n",
    "sorted_relevances = sorted(data.items(), key=lambda item: item[1], reverse=True)\n",
    "for item in data.items():\n",
    "    print(item)\n",
    "print(sorted_relevances)\n",
    "relevances = list(data.items())\n",
    "print(relevances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
