{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import BIB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyQt5.QtWidgets import (\n",
    "    QApplication, QMainWindow, QWidget, QVBoxLayout, QHBoxLayout,\n",
    "    QLineEdit, QPushButton, QRadioButton, QLabel, QGroupBox,\n",
    "    QTableWidget, QTableWidgetItem, QScrollArea, QTextEdit, QStackedWidget, QGridLayout,\n",
    "    QMessageBox,QHeaderView\n",
    ")\n",
    "from PyQt5.QtGui import QIcon\n",
    "from PyQt5.QtCore import Qt\n",
    "import sys\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.1.3 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\asyncio\\base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\asyncio\\base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\ryan\\AppData\\Local\\Temp\\ipykernel_14544\\2050946007.py\", line 1, in <module>\n",
      "    import nltk\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\nltk\\__init__.py\", line 133, in <module>\n",
      "    from nltk.collocations import *\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\nltk\\collocations.py\", line 36, in <module>\n",
      "    from nltk.metrics import (\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\nltk\\metrics\\__init__.py\", line 18, in <module>\n",
      "    from nltk.metrics.association import (\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\nltk\\metrics\\association.py\", line 26, in <module>\n",
      "    from scipy.stats import fisher_exact\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\scipy\\stats\\__init__.py\", line 606, in <module>\n",
      "    from ._stats_py import *\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\scipy\\stats\\_stats_py.py\", line 37, in <module>\n",
      "    from scipy import sparse\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\scipy\\__init__.py\", line 134, in __getattr__\n",
      "    return _importlib.import_module(f'scipy.{name}')\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\importlib\\__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\scipy\\sparse\\__init__.py\", line 295, in <module>\n",
      "    from ._csr import *\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\scipy\\sparse\\_csr.py\", line 11, in <module>\n",
      "    from ._sparsetools import (csr_tocsc, csr_tobsr, csr_count_blocks,\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.1.3 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\asyncio\\base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\asyncio\\base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\ryan\\AppData\\Local\\Temp\\ipykernel_14544\\2050946007.py\", line 1, in <module>\n",
      "    import nltk\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\nltk\\__init__.py\", line 133, in <module>\n",
      "    from nltk.collocations import *\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\nltk\\collocations.py\", line 36, in <module>\n",
      "    from nltk.metrics import (\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\nltk\\metrics\\__init__.py\", line 38, in <module>\n",
      "    from nltk.metrics.scores import (\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\nltk\\metrics\\scores.py\", line 15, in <module>\n",
      "    from scipy.stats.stats import betai\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\scipy\\stats\\__init__.py\", line 606, in <module>\n",
      "    from ._stats_py import *\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\scipy\\stats\\_stats_py.py\", line 37, in <module>\n",
      "    from scipy import sparse\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\scipy\\__init__.py\", line 134, in __getattr__\n",
      "    return _importlib.import_module(f'scipy.{name}')\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\importlib\\__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\scipy\\sparse\\__init__.py\", line 295, in <module>\n",
      "    from ._csr import *\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\scipy\\sparse\\_csr.py\", line 11, in <module>\n",
      "    from ._sparsetools import (csr_tocsc, csr_tobsr, csr_count_blocks,\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.1.3 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\asyncio\\base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\asyncio\\base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\ryan\\AppData\\Local\\Temp\\ipykernel_14544\\2050946007.py\", line 1, in <module>\n",
      "    import nltk\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\nltk\\__init__.py\", line 146, in <module>\n",
      "    from nltk.chunk import *\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\nltk\\chunk\\__init__.py\", line 155, in <module>\n",
      "    from nltk.chunk.api import ChunkParserI\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\nltk\\chunk\\api.py\", line 15, in <module>\n",
      "    from nltk.parse import ParserI\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\nltk\\parse\\__init__.py\", line 100, in <module>\n",
      "    from nltk.parse.transitionparser import TransitionParser\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\nltk\\parse\\transitionparser.py\", line 17, in <module>\n",
      "    from scipy import sparse\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\scipy\\__init__.py\", line 134, in __getattr__\n",
      "    return _importlib.import_module(f'scipy.{name}')\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\importlib\\__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\scipy\\sparse\\__init__.py\", line 295, in <module>\n",
      "    from ._csr import *\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\scipy\\sparse\\_csr.py\", line 11, in <module>\n",
      "    from ._sparsetools import (csr_tocsc, csr_tobsr, csr_count_blocks,\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import os\n",
    "import math\n",
    "from collections import defaultdict\n",
    "from nltk import FreqDist\n",
    "\n",
    "\n",
    "STOPWORDS = set(nltk.corpus.stopwords.words('english'))\n",
    "PORTER_STEMMER = nltk.PorterStemmer()\n",
    "LANCASTER_STEMMER = nltk.LancasterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_processing_args():\n",
    "    tokenization = \"Split\"\n",
    "    normalization = \"None\",\n",
    "    file_type = \"TPD\"\n",
    "    return tokenization, normalization, file_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(doc_path, tokenization, normalization):\n",
    "    with open(doc_path, 'r') as file:\n",
    "        text = file.read()\n",
    "        \n",
    "    # Tokenization\n",
    "    if tokenization == \"Split\":\n",
    "        tokens = text.split()\n",
    "    else:\n",
    "        exp_reg = nltk.RegexpTokenizer(r'\\d+(?:\\.\\d+)?x\\d+|\\d+(?:\\.\\d+)|\\w+(?:-\\w+)*|(?:[A-Z]\\.)+|\\w+')\n",
    "        tokens = exp_reg.tokenize(text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    tokens = [term for term in tokens if term.lower() not in STOPWORDS]\n",
    "\n",
    "    # Normalization\n",
    "    if normalization == \"Porter\":\n",
    "        tokens = [PORTER_STEMMER.stem(term) for term in tokens]\n",
    "    elif normalization == \"Lancaster\":\n",
    "        tokens = [LANCASTER_STEMMER.stem(term) for term in tokens]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_global_term_frequencies(tokenization, normalization):\n",
    "    global_term_frequencies = defaultdict(int)\n",
    "\n",
    "    for doc_name in os.listdir('Collections'):\n",
    "        doc_path = os.path.join('Collections', doc_name)\n",
    "        tokens = preprocessing(doc_path, tokenization, normalization)\n",
    "        unique_terms = set(tokens)\n",
    "\n",
    "        for term in unique_terms:\n",
    "            global_term_frequencies[term] += 1\n",
    "            \n",
    "    return global_term_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TPD_result(query, terms_freq, global_term_frequencies, N):\n",
    "    max_freq = max(terms_freq.values())\n",
    "    results=[]\n",
    "    for idx, (term, freq) in enumerate(terms_freq.items(), start=1):\n",
    "        poids = (freq / max_freq) * math.log10((N / global_term_frequencies[term]) + 1)\n",
    "        results.append((idx, term, query, freq, round(poids, 4)))\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nb_termes_glob(tokenization, normalization):\n",
    "    nb_termes_global = []\n",
    "    for doc_name in os.listdir('Collections'):\n",
    "        doc_path = os.path.join('Collections', doc_name)\n",
    "        # Appliquer le prétraitement pour obtenir les tokens du document\n",
    "        tokens = preprocessing(doc_path, tokenization, normalization)\n",
    "        \n",
    "        # Ajouter les tokens du document à la liste globale\n",
    "        nb_termes_global.extend(tokens)\n",
    "\n",
    "    # Obtenir le nombre de termes uniques\n",
    "    termes_uniques = np.unique(nb_termes_global)\n",
    "    print(\"Termes uniques : \", termes_uniques)  # Optionnel : pour visualiser les termes uniques\n",
    "    \n",
    "    return len(termes_uniques)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Termes uniques :  ['0.1' '1' '1.5' '1.6x10' '10' '101' '3' '3.2x10' '300' '5' '704'\n",
      " 'accuracy' 'accurate' 'actual' 'aerodynamic' 'aerodynamic-centre'\n",
      " 'aerodynamics' 'agree' 'algebraic' 'almost' 'also' 'angles' 'another'\n",
      " 'applicability' 'applied' 'approximate' 'approximation' 'arbitrary'\n",
      " 'area' 'areas' 'arises' 'attack' 'axial' 'axially' 'based' 'basis'\n",
      " 'behind' 'blunt-nosed' 'bodies' 'body' 'boundaries' 'boundary'\n",
      " 'boundary-layer' 'boundary-layer-control' 'boundary-value' 'bow'\n",
      " 'calculated' 'calculation' 'capable' 'case' 'cent' 'center'\n",
      " 'characteristics' 'check' 'circulatory' 'classical' 'coefficient'\n",
      " 'coefficients' 'comparative' 'comparison' 'compression' 'computations'\n",
      " 'computed' 'concept' 'concluded' 'cone' 'cones' 'configuration'\n",
      " 'consequently' 'consider' 'considered' 'constant' 'conventional'\n",
      " 'coordinate' 'curved' 'curves' 'dealing' 'define' 'defined' 'depend'\n",
      " 'depending' 'destalling' 'determine' 'developed' 'differ' 'different'\n",
      " 'dimensional' 'diminish' 'discussed' 'discussion' 'distribution'\n",
      " 'disturbance' 'drag' 'due' 'dynamic' 'ease' 'edge' 'edpm' 'effect'\n",
      " 'effects' 'efficient' 'either' 'embedded' 'emitting' 'empirical' 'enough'\n",
      " 'entire' 'entropy' 'equation' 'equations' 'evaluation' 'evidence' 'exact'\n",
      " 'examination' 'example' 'examples' 'exists' 'expected' 'experiment'\n",
      " 'experimental' 'factors' 'feature' 'ferri' 'field' 'fields' 'first'\n",
      " 'flap' 'flare' 'flat' 'flight' 'flow' 'flows' 'fluid' 'found' 'fredholm'\n",
      " 'free' 'free-stream' 'friction' 'general' 'generates' 'give' 'given'\n",
      " 'greater' 'growth' 'high-speed' 'hours' 'hypersonic' 'ibm' 'impact'\n",
      " 'incident' 'incompressible' 'increase' 'increasing' 'increment' 'initial'\n",
      " 'inlet' 'instance' 'integral' 'integrated' 'intended' 'interesting'\n",
      " 'interference' 'internal' 'investigated' 'investigation' 'inviscid'\n",
      " 'involving' 'irrotational' 'karman-pohlhausen' 'kind' 'known' 'laminar'\n",
      " 'large-angled' 'layer' 'layers' 'leading' 'leads' 'libby' 'lift' 'linear'\n",
      " 'little' 'loading' 'local' 'locally' 'located' 'location' 'low'\n",
      " 'low-speed' 'lower' 'made' 'main' 'maximum' 'may' 'measurements'\n",
      " 'mentioned' 'method' 'methods' 'minutes' 'modified' 'mounted' 'must'\n",
      " 'necessary' 'need' 'needs' 'neumann' 'newtonian' 'non' 'nonuniformity'\n",
      " 'normal-force' 'nose' 'novel' 'number' 'numbers' 'obtained' 'occur' 'one'\n",
      " 'order' 'original' 'outside' 'paper' 'part' 'past' 'per'\n",
      " 'pitching-moment' 'plane' 'plate' 'points' 'possible' 'potential'\n",
      " 'prandtl' 'predict' 'predicts' 'presence' 'present' 'presented'\n",
      " 'pressure' 'pressures' 'previously' 'problem' 'problems' 'process'\n",
      " 'produced' 'profile' 'programed' 'propeller' 'protrude' 'provided'\n",
      " 'purely' 'rae' 'ramp' 'range' 'ratio' 'ratios' 'recently' 'refinement'\n",
      " 'region' 'remaining' 'require' 'restricted' 'result' 'results' 'revert'\n",
      " 'revolution' 'reynolds' 'rotational' 'satisfactorily' 'scope' 'second'\n",
      " 'secondary' 'section' 'seidel' 'set' 'several' 'shape' 'shear' 'shock'\n",
      " 'show' 'showed' 'shown' 'simple' 'simplest' 'single' 'situation' 'skin'\n",
      " 'slipstream' 'slopes' 'small' 'solid' 'solid-body' 'solution' 'solutions'\n",
      " 'solve' 'solved' 'solving' 'somewhat' 'source' 'span' 'spanwise'\n",
      " 'specific' 'speed' 'stabilizer' 'stabilizers' 'steady' 'still' 'stream'\n",
      " 'streams' 'study' 'substantial' 'subtracting' 'suction' 'suggests'\n",
      " 'supersonic' 'supporting' 'surface' 'symmetric' 'symmetry' 'takes'\n",
      " 'technique' 'theoretical' 'theory' 'thickness' 'thin' 'things' 'three'\n",
      " 'time' 'together' 'traverses' 'treated' 'treatments' 'turbulent' 'two'\n",
      " 'two-dimensional' 'uniform' 'upon' 'used' 'useful' 'usually' 'value'\n",
      " 'values' 'variable' 'vary' 'velocities' 'velocity' 'viewed' 'viscosity'\n",
      " 'viscous' 'vorticity' 'wave' 'wedges' 'well' 'whose' 'wing' 'within']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "353"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb = nb_termes_glob(\"split\" ,None)\n",
    "nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_input(query, normalization):\n",
    "    # appliquer le traitement sur la requete\n",
    "    if normalization == \"Porter\":\n",
    "        query = PORTER_STEMMER.stem(query) \n",
    "    elif normalization == \"Lancaster\":\n",
    "        query = LANCASTER_STEMMER.stem(query) \n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_processing(query,tokenization, normalization, file_type):\n",
    "    # tokenization, normalization, file_type = get_processing_args()\n",
    "    nb_terms = 0\n",
    "    global_term_frequencies = build_global_term_frequencies(tokenization, normalization)  # Calculate global term frequencies\n",
    "    N = len(os.listdir('Collections'))\n",
    "    results =[]\n",
    "    if file_type == \"TPD\":\n",
    "        doc_path = os.path.join('Collections', f\"{query}.txt\")\n",
    "        tokens = preprocessing(doc_path, tokenization, normalization)\n",
    "        nb_terms = len(np.unique(tokens))\n",
    "        terms_freq = FreqDist(tokens)\n",
    "        \n",
    "        result = TPD_result(query, terms_freq, global_term_frequencies, N)\n",
    "        \n",
    "        return result , nb_terms\n",
    "        \n",
    "    else :\n",
    "        query = process_input(query, normalization)\n",
    "        i=0\n",
    "        for doc_name in os.listdir('Collections'):\n",
    "            doc_path = os.path.join('Collections', doc_name)\n",
    "            Tokens = preprocessing(doc_path, tokenization, normalization)\n",
    "            terms_freq = FreqDist(Tokens)\n",
    "\n",
    "            max_freq = max(terms_freq.values())\n",
    "            for term, freq in terms_freq.items():  \n",
    "                if term == query:  # Check if the term is the specific query term\n",
    "                    poids = ((freq / max_freq) * math.log10((N / global_term_frequencies[term]) + 1))\n",
    "                    i+=1\n",
    "                    results.append((i, term, os.path.splitext(doc_name)[0], freq, round(poids, 4)))\n",
    "        \n",
    "        return results , nb_terms\n",
    "   \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(query):\n",
    "    # if raw:\n",
    "    doc_path = os.path.join('Collections', f\"{query}.txt\")\n",
    "    with open(doc_path, 'r') as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "    # elif processed:\n",
    "    #     text_processing(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SearchApp(QMainWindow):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.setWindowTitle(\"Document Search and Processing\")\n",
    "        self.setGeometry(100, 100, 900, 700) #8,6\n",
    "        self.setWindowIcon(QIcon(\"./icons/interface_icon.png\")) \n",
    "        self.setFixedSize(900, 700)\n",
    "        \n",
    "        \n",
    "        # Layout principal\n",
    "        central_widget = QWidget()\n",
    "        self.setCentralWidget(central_widget)\n",
    "        self.main_layout = QVBoxLayout(central_widget)\n",
    "\n",
    "        # Barre de recherche\n",
    "        search_layout = QHBoxLayout()\n",
    "        query_label = QLabel(\"Query: \", self)\n",
    "        search_layout.addWidget(query_label)\n",
    "        \n",
    "        self.search_bar = QLineEdit(self)\n",
    "        self.search_bar.setPlaceholderText(\"Enter document name...\")\n",
    "        self.search_button = QPushButton(\"Search\", self)\n",
    "        \n",
    "        search_layout.addWidget(self.search_bar)\n",
    "        search_layout.addWidget(self.search_button)\n",
    "        self.main_layout.addLayout(search_layout)\n",
    "\n",
    "        # Options de radio\n",
    "        radio_layout = QHBoxLayout()\n",
    "        self.raw_text_radio = QRadioButton(\"Raw Text\", self)\n",
    "        self.processed_text_radio = QRadioButton(\"Processed Text\", self)\n",
    "        radio_layout.addWidget(self.raw_text_radio)\n",
    "        radio_layout.addWidget(self.processed_text_radio)\n",
    "        self.main_layout.addLayout(radio_layout)\n",
    "        \n",
    "        # Section Tokenization\n",
    "        tokenization_box = QGroupBox(\"Tokenization\")\n",
    "        tokenization_layout = QVBoxLayout()\n",
    "        self.split_radio = QRadioButton(\"Split\", self)\n",
    "        self.regex_radio = QRadioButton(\"Regex\", self)\n",
    "        tokenization_layout.addWidget(self.split_radio)\n",
    "        tokenization_layout.addWidget(self.regex_radio)\n",
    "        tokenization_box.setLayout(tokenization_layout)\n",
    "        \n",
    "        # Section Normalization\n",
    "        normalization_box = QGroupBox(\"Normalization\")\n",
    "        normalization_layout = QVBoxLayout()\n",
    "        self.no_stem_radio = QRadioButton(\"No Stem\", self)\n",
    "        self.porter_radio = QRadioButton(\"Porter\", self)\n",
    "        self.lancaster_radio = QRadioButton(\"Lancaster\", self)\n",
    "        normalization_layout.addWidget(self.no_stem_radio)\n",
    "        normalization_layout.addWidget(self.porter_radio)\n",
    "        normalization_layout.addWidget(self.lancaster_radio)\n",
    "        normalization_box.setLayout(normalization_layout)\n",
    "        \n",
    "        # Section Indexation\n",
    "        indexation_box = QGroupBox(\"Indexation\")\n",
    "        indexation_layout = QVBoxLayout()\n",
    "        self.doc_per_term_radio = QRadioButton(\"Documents per Term\", self)\n",
    "        self.term_per_doc_radio = QRadioButton(\"Terms per Document\", self)\n",
    "        indexation_layout.addWidget(self.doc_per_term_radio)\n",
    "        indexation_layout.addWidget(self.term_per_doc_radio)\n",
    "        indexation_box.setLayout(indexation_layout)\n",
    "        \n",
    "        # Disposition des sections\n",
    "        sections_layout = QHBoxLayout()\n",
    "        sections_layout.addWidget(tokenization_box)\n",
    "        sections_layout.addWidget(normalization_box)\n",
    "        sections_layout.addWidget(indexation_box)\n",
    "        self.main_layout.addLayout(sections_layout)\n",
    "        \n",
    "        # Zone de résultats (QStackedWidget pour alterner entre texte et tableau)\n",
    "        self.result_label = QLabel(\"Result: \", self)\n",
    "        self.main_layout.addWidget(self.result_label)\n",
    "\n",
    "        self.result_area = QStackedWidget(self)\n",
    "        self.result_area.setFixedHeight(400)  # Taille fixe pour éviter d'étendre la mise en page\n",
    "        self.result_area.setFixedWidth(600)  # Ajustez selon la largeur désirée\n",
    "        \n",
    "        # Widget pour afficher le texte brut\n",
    "        self.raw_text_widget = QTextEdit(self)\n",
    "        self.raw_text_widget.setReadOnly(True)  # Rendre le texte en lecture seule\n",
    "        self.result_area.addWidget(self.raw_text_widget)\n",
    "        \n",
    "        # Widget pour afficher le tableau\n",
    "        self.table = QTableWidget(0, 5, self)  # 5 colonnes pour N°, N° doc, terme, fréquence, poids\n",
    "        self.table.setHorizontalHeaderLabels([\"N°\", \"N° doc\", \"Term\", \"Frequency\", \"Weight\"])\n",
    "        self.table.setShowGrid(False)  # Masquer la grille du tableau\n",
    "        \n",
    "        # Faire en sorte que les colonnes s'étendent pour couvrir toute la largeur\n",
    "        header = self.table.horizontalHeader()\n",
    "        header.setSectionResizeMode(QHeaderView.Stretch)\n",
    "        self.result_area.addWidget(self.table)\n",
    "        \n",
    "        self.main_layout.addWidget(self.result_area)\n",
    "\n",
    "    #    //////////////////////////////////////\n",
    "        Total_terms_layout = QHBoxLayout()\n",
    "        \n",
    "        # Création et configuration des QLabel\n",
    "        self.terms_per_doc = QLabel(self)\n",
    "        self.terms_all_doc = QLabel(self)\n",
    "        \n",
    "        # Appliquer les styles pour enlever le fond et les bordures\n",
    "        style = \"\"\"\n",
    "            QLabel {\n",
    "                margin-left: 20px;\n",
    "                background-color: transparent;\n",
    "                border: none;\n",
    "                font-size: 14px;\n",
    "                font-family: Arial, sans-serif;\n",
    "            }\n",
    "        \"\"\"\n",
    "        self.terms_all_doc.setStyleSheet(style)\n",
    "        self.terms_per_doc.setStyleSheet(style)\n",
    "        \n",
    "        \n",
    "        # Ajout des QLabel au layout horizontal\n",
    "        Total_terms_layout.addWidget(self.terms_per_doc)\n",
    "        Total_terms_layout.addWidget(self.terms_all_doc)\n",
    "        \n",
    "        # Ajout du layout horizontal dans le layout principal\n",
    "        self.main_layout.addLayout(Total_terms_layout)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # Ajustements dans le code principal\n",
    "        self.main_layout.setContentsMargins(2, 2, 2, 2)  # Réduire les marges globales\n",
    "        self.main_layout.setSpacing(8)  # Diminuer l'espace entre les sections\n",
    "        self.result_area.setContentsMargins(2, 0, 2, 0)  # Marges gauche et droite de 2px pour le tableau\n",
    "        self.result_area.setFixedWidth(self.width() - 4) \n",
    "                \n",
    "                \n",
    "        \n",
    "        self.search_button.clicked.connect(self.process_search)\n",
    "        self.raw_text_radio.clicked.connect(self.raw_text_radio_process)\n",
    "        self.processed_text_radio.clicked.connect(self.processed_text_radio_process)\n",
    "        \n",
    "\n",
    "    def raw_text_radio_process(self):\n",
    "        self.split_radio.setEnabled(False) \n",
    "        self.regex_radio.setEnabled(False)\n",
    "        self.lancaster_radio.setEnabled(False)\n",
    "        self.porter_radio.setEnabled(False)\n",
    "        self.doc_per_term_radio.setEnabled(False)\n",
    "        self.term_per_doc_radio.setEnabled(False)\n",
    "        self.no_stem_radio.setEnabled(False)\n",
    "        self.terms_per_doc.setText(\"\")\n",
    "        self.terms_all_doc.setText(\"\")\n",
    "       \n",
    "       \n",
    "    def processed_text_radio_process(self):\n",
    "        self.split_radio.setEnabled(True) \n",
    "        self.regex_radio.setEnabled(True)\n",
    "        self.lancaster_radio.setEnabled(True)\n",
    "        self.porter_radio.setEnabled(True)\n",
    "        self.doc_per_term_radio.setEnabled(True)\n",
    "        self.term_per_doc_radio.setEnabled(True)\n",
    "        self.no_stem_radio.setEnabled(True)\n",
    "        \n",
    "         \n",
    "    def display_Total_Terms(self, termes_global, nb_termes ,index):\n",
    "        if nb_termes != 0 and index == 'TPD':\n",
    "            # Afficher le nombre de termes par document\n",
    "            self.terms_per_doc.setText(f\"Terms per document : {nb_termes}\")\n",
    "        else :\n",
    "            self.terms_per_doc.setText(\"\")\n",
    "        self.terms_all_doc.setText(f\"Total terms  : {termes_global}\")\n",
    "\n",
    "        \n",
    "        \n",
    "    def process_search(self):\n",
    "        # Obtenir le numéro de document\n",
    "        document_number = self.search_bar.text()\n",
    "        \n",
    "        if not document_number:\n",
    "            self.show_error(\"Veuillez entrer un numéro de document valide.\")\n",
    "            return\n",
    "\n",
    "        # \n",
    "        # Vérifier le type de texte sélectionné\n",
    "        if self.raw_text_radio.isChecked():\n",
    "            # verification de nom_document\n",
    "            result = get_text(document_number)\n",
    "            self.show_raw_text(result)\n",
    "        else:\n",
    "            # Obtenir les méthodes sélectionnées\n",
    "            tokenization_method = \"Split\" if self.split_radio.isChecked() else \"Regex\"\n",
    "            if self.porter_radio.isChecked() :\n",
    "                normalization_method = \"Porter\" \n",
    "            elif self.no_stem_radio.isChecked():\n",
    "                normalization_method = \"None\" \n",
    "            else :\n",
    "                normalization_method =\"Lancaster\"\n",
    "            indexation_method = \"DPT\" if self.doc_per_term_radio.isChecked() else \"TPD\"\n",
    "            \n",
    "                \n",
    "                \n",
    "            termes_global = nb_termes_glob(tokenization_method, normalization_method)\n",
    "            # Appeler la fonction pour obtenir les données\n",
    "            data , nb_termes = text_processing(document_number, tokenization_method, normalization_method, indexation_method)\n",
    "            print(data)\n",
    "            self.display_results(data)\n",
    "            self.display_Total_Terms(termes_global , nb_termes , indexation_method)\n",
    "\n",
    " \n",
    "\n",
    "    def show_raw_text(self, text):\n",
    "        self.raw_text_widget.setText(text)\n",
    "        self.result_area.setCurrentWidget(self.raw_text_widget)  # Afficher le widget de texte brut\n",
    "\n",
    "        \n",
    "    def display_results(self, data):\n",
    "    # Supprimer l'affichage de l'index de ligne\n",
    "        self.table.verticalHeader().setVisible(False)\n",
    "\n",
    "        # Nettoyer le tableau et ajouter les résultats\n",
    "        self.table.setRowCount(0)\n",
    "        for index, row_data in enumerate(data):\n",
    "            row_position = self.table.rowCount()\n",
    "            self.table.insertRow(row_position)\n",
    "            for column, value in enumerate(row_data):\n",
    "                item = QTableWidgetItem(str(value))\n",
    "                item.setTextAlignment(Qt.AlignCenter)  # Centrer le texte dans chaque cellule\n",
    "                self.table.setItem(row_position, column, item)\n",
    "        \n",
    "        # Afficher le widget de tableau\n",
    "        self.result_area.setCurrentWidget(self.table)\n",
    "    \n",
    "    def show_error(self, message):\n",
    "        error_dialog = QMessageBox(self)\n",
    "        error_dialog.setIcon(QMessageBox.Critical)\n",
    "        error_dialog.setWindowTitle(\"Erreur\")\n",
    "        error_dialog.setText(message)\n",
    "        error_dialog.exec_()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Termes uniques :  ['.' '..' '/destalling/' '0.1' '1' '1.5' '1.6x10' '10' '101' '3,.'\n",
      " '3.2x10' '300' '5' '704' 'accur' 'accuraci' 'actual' 'aerodynam'\n",
      " 'aerodynamic-centr' 'agre' 'algebra' 'almost' 'also' 'angl' 'another,'\n",
      " 'appli' 'applic' 'approxim' 'approximation,' 'arbitrari' 'area' 'arises,'\n",
      " 'attack' 'axial' 'base' 'basi' 'behind' 'blunt-nos' 'bodi' 'boundari'\n",
      " 'boundary-' 'boundary-lay' 'boundary-layer-control' 'boundary-valu' 'bow'\n",
      " 'calcul' 'capabl' 'case' 'cent' 'center,.but' 'characterist'\n",
      " 'characteristics,' 'check' 'circulatori' 'classic' 'coeffici' 'compar'\n",
      " 'comparison' 'compress' 'comput' 'concept' 'conclud' 'cone' 'cone,'\n",
      " 'configur' 'consequ' 'consequently,' 'consid' 'constant' 'convent'\n",
      " 'coordin' 'curv' 'curves,' 'deal' 'defin' 'depend' 'destal' 'determin'\n",
      " 'develop' 'differ' 'dimension' 'diminish' 'discuss' 'distribut'\n",
      " 'distribution,' 'disturb' 'drag' 'due' 'dynam' 'eas' 'edg' 'edpm'\n",
      " 'effect' 'effici' 'either' 'embed' 'emit' 'empir' 'enough' 'entir'\n",
      " 'entropi' 'equat' 'equations,' 'evalu' 'evidence,' 'exact' 'examin'\n",
      " 'exampl' 'example,' 'exist' 'expect' 'experi' 'experiment' 'factor'\n",
      " 'featur' 'ferri' 'field' 'first,' 'flap' 'flare' 'flat' 'flight' 'flow'\n",
      " 'fluid' 'found' 'fredholm' 'free' 'free-stream' 'friction,' 'gener'\n",
      " 'give' 'given' 'greater' 'growth' 'high-spe' 'hours,' 'hyperson' 'ibm'\n",
      " 'impact' 'incid' 'incompress' 'increas' 'increment' 'increment,' 'initi'\n",
      " 'inlet,' 'instance,' 'integr' 'intend' 'interest' 'interfer' 'intern'\n",
      " 'investig' 'inviscid' 'involv' 'irrot' 'is,' 'karman-pohlhausen' 'kind,'\n",
      " 'known' 'laminar' 'large-angl' 'layer' 'lead' 'libbi' 'lift' 'lift,'\n",
      " 'linear' 'littl' 'load' 'local' 'locat' 'low' 'low-spe' 'lower' 'made'\n",
      " 'main' 'maximum' 'may' 'measur' 'mention' 'method' 'minut' 'modifi'\n",
      " 'mount' 'must' 'necessari' 'need' 'neumann' 'newtonian' 'non-'\n",
      " 'nonuniform' 'normal-forc' 'nose' 'novel' 'number' 'obtain' 'occur' 'one'\n",
      " 'order' 'origin' 'outsid' 'paper,' 'part' 'past' 'per' 'pitching-mo'\n",
      " 'plane' 'plate' 'point' 'possibl' 'potenti' \"prandtl'\" 'predict'\n",
      " 'presenc' 'present' 'presented,' 'pressur' 'previous' 'problem' 'process'\n",
      " 'produc' 'profil' 'program' 'propel' 'protrud' 'provid' 'pure' 'rae'\n",
      " 'ramp' 'rang' 'ratio' 'recent' 'refin' 'region' 'remain' 'requir'\n",
      " 'restrict' 'result' 'revert' 'revolution,' 'reynold' 'rotat'\n",
      " 'satisfactorili' 'scope' 'second' 'secondari' 'section' 'seidel' 'set'\n",
      " 'sever' 'shape' 'shape,' 'shear' 'shock' 'show' 'shown' 'simpl'\n",
      " 'simplest' 'singl' 'situat' 'skin' 'slipstream' 'slope' 'small' 'solid,'\n",
      " 'solid-body,' 'solut' 'solv' 'somewhat' 'sourc' 'span' 'spanwis' 'specif'\n",
      " 'speed' 'stabil' 'steadi' 'still' 'stream' 'stream,' 'studi' 'substanti'\n",
      " 'subtract' 'suction' 'suggest' 'superson' 'support' 'surfac' 'symmetr'\n",
      " 'symmetri' 'take' 'techniqu' 'theoret' 'theori' 'thickness,' 'thin'\n",
      " 'thing' 'three' 'time' 'togeth' 'travers' 'treat' 'treatment' 'turbul'\n",
      " 'two' 'two-dimension' 'uniform' 'upon' 'use' 'usual' 'valu' 'vari'\n",
      " 'variabl' 'veloc' 'view' 'viscos' 'viscou' 'vortic' 'wave' 'wave,' 'wedg'\n",
      " 'well' 'whose' 'wing' 'within']\n",
      "[(1, 'exact', 'D4', 2, 0.1408), (2, 'solut', 'D4', 4, 0.2007), (3, 'neumann', 'D4', 2, 0.1408), (4, 'problem', 'D4', 4, 0.1326), (5, '.', 'D4', 12, 0.301), (6, 'calcul', 'D4', 3, 0.1193), (7, 'non-', 'D4', 1, 0.0704), (8, 'circulatori', 'D4', 1, 0.0704), (9, 'plane', 'D4', 2, 0.1408), (10, 'axial', 'D4', 2, 0.1003), (11, 'symmetr', 'D4', 1, 0.0704), (12, 'flow', 'D4', 7, 0.1997), (13, 'within', 'D4', 2, 0.1408), (14, 'arbitrari', 'D4', 1, 0.0704), (15, 'boundari', 'D4', 2, 0.0663), (16, 'gener', 'D4', 1, 0.0502), (17, 'method', 'D4', 4, 0.2007), (18, 'solv', 'D4', 4, 0.2817), (19, 'second', 'D4', 2, 0.1408), (20, 'boundary-valu', 'D4', 1, 0.0704), (21, 'develop', 'D4', 1, 0.0502), (22, 'appli', 'D4', 1, 0.0704), (23, 'low-spe', 'D4', 1, 0.0704), (24, 'bodi', 'D4', 5, 0.1988), (25, 'almost', 'D4', 1, 0.0704), (26, 'shape,', 'D4', 1, 0.0704), (27, 'provid', 'D4', 1, 0.0704), (28, 'either', 'D4', 1, 0.0704), (29, 'symmetri', 'D4', 1, 0.0704), (30, 'solid-body,', 'D4', 1, 0.0704), (31, 'inlet,', 'D4', 1, 0.0704), (32, 'pure', 'D4', 1, 0.0704), (33, 'intern', 'D4', 1, 0.0704), (34, 'capabl', 'D4', 1, 0.0704), (35, 'deal', 'D4', 1, 0.0704), (36, 'sever', 'D4', 1, 0.0704), (37, 'presenc', 'D4', 1, 0.0704), (38, 'one', 'D4', 1, 0.0704), (39, 'another,', 'D4', 1, 0.0704), (40, 'consequ', 'D4', 1, 0.0704), (41, 'interfer', 'D4', 1, 0.0704), (42, 'treat', 'D4', 1, 0.0502), (43, 'eas', 'D4', 1, 0.0704), (44, 'need', 'D4', 1, 0.0502), (45, 'solid,', 'D4', 1, 0.0704), (46, 'is,', 'D4', 1, 0.0704), (47, 'involv', 'D4', 1, 0.0704), (48, 'area', 'D4', 1, 0.0502), (49, 'suction', 'D4', 1, 0.0704), (50, 'veloc', 'D4', 1, 0.0398), (51, 'comput', 'D4', 2, 0.1408), (52, 'point', 'D4', 3, 0.2113), (53, 'surfac', 'D4', 2, 0.1003), (54, 'entir', 'D4', 1, 0.0704), (55, 'field', 'D4', 1, 0.0502), (56, 'sourc', 'D4', 1, 0.0704), (57, 'distribut', 'D4', 1, 0.0332), (58, 'use', 'D4', 2, 0.0795), (59, 'basi', 'D4', 1, 0.0502), (60, 'lead', 'D4', 1, 0.0502), (61, 'fredholm', 'D4', 1, 0.0704), (62, 'integr', 'D4', 1, 0.0398), (63, 'equat', 'D4', 1, 0.0398), (64, 'kind,', 'D4', 1, 0.0704), (65, 'set', 'D4', 1, 0.0704), (66, 'linear', 'D4', 1, 0.0704), (67, 'algebra', 'D4', 1, 0.0704), (68, 'equations,', 'D4', 1, 0.0704), (69, 'usual', 'D4', 1, 0.0502), (70, 'modifi', 'D4', 1, 0.0704), (71, 'seidel', 'D4', 1, 0.0704), (72, 'present', 'D4', 1, 0.0502), (73, 'time', 'D4', 1, 0.0704), (74, 'program', 'D4', 1, 0.0704), (75, 'ibm', 'D4', 1, 0.0704), (76, '704', 'D4', 1, 0.0704), (77, 'edpm', 'D4', 1, 0.0704), (78, 'previous', 'D4', 1, 0.0704), (79, 'mention', 'D4', 1, 0.0704), (80, 'characterist', 'D4', 1, 0.0502), (81, 'whose', 'D4', 1, 0.0704), (82, 'profil', 'D4', 1, 0.0704), (83, 'defin', 'D4', 2, 0.1408), (84, 'satisfactorili', 'D4', 1, 0.0704), (85, '300', 'D4', 1, 0.0704), (86, 'coordin', 'D4', 1, 0.0704), (87, 'number', 'D4', 2, 0.0795), (88, 'presented,', 'D4', 1, 0.0704), (89, 'show', 'D4', 1, 0.0398), (90, 'scope', 'D4', 1, 0.0704), (91, 'accuraci', 'D4', 1, 0.0704), (92, 'requir', 'D4', 1, 0.0704), (93, 'three', 'D4', 1, 0.0704), (94, 'minut', 'D4', 1, 0.0704), (95, 'two', 'D4', 1, 0.0502), (96, 'hours,', 'D4', 1, 0.0704), (97, 'depend', 'D4', 1, 0.0502), (98, 'upon', 'D4', 1, 0.0704), (99, 'shape', 'D4', 1, 0.0704)]\n",
      "Termes uniques :  ['.' '..' '/destalling/' '0.1' '1' '1.5' '1.6x10' '10' '101' '3,.'\n",
      " '3.2x10' '300' '5' '704' 'accur' 'accuraci' 'actual' 'aerodynam'\n",
      " 'aerodynamic-centr' 'agre' 'algebra' 'almost' 'also' 'angl' 'another,'\n",
      " 'appli' 'applic' 'approxim' 'approximation,' 'arbitrari' 'area' 'arises,'\n",
      " 'attack' 'axial' 'base' 'basi' 'behind' 'blunt-nos' 'bodi' 'boundari'\n",
      " 'boundary-' 'boundary-lay' 'boundary-layer-control' 'boundary-valu' 'bow'\n",
      " 'calcul' 'capabl' 'case' 'cent' 'center,.but' 'characterist'\n",
      " 'characteristics,' 'check' 'circulatori' 'classic' 'coeffici' 'compar'\n",
      " 'comparison' 'compress' 'comput' 'concept' 'conclud' 'cone' 'cone,'\n",
      " 'configur' 'consequ' 'consequently,' 'consid' 'constant' 'convent'\n",
      " 'coordin' 'curv' 'curves,' 'deal' 'defin' 'depend' 'destal' 'determin'\n",
      " 'develop' 'differ' 'dimension' 'diminish' 'discuss' 'distribut'\n",
      " 'distribution,' 'disturb' 'drag' 'due' 'dynam' 'eas' 'edg' 'edpm'\n",
      " 'effect' 'effici' 'either' 'embed' 'emit' 'empir' 'enough' 'entir'\n",
      " 'entropi' 'equat' 'equations,' 'evalu' 'evidence,' 'exact' 'examin'\n",
      " 'exampl' 'example,' 'exist' 'expect' 'experi' 'experiment' 'factor'\n",
      " 'featur' 'ferri' 'field' 'first,' 'flap' 'flare' 'flat' 'flight' 'flow'\n",
      " 'fluid' 'found' 'fredholm' 'free' 'free-stream' 'friction,' 'gener'\n",
      " 'give' 'given' 'greater' 'growth' 'high-spe' 'hours,' 'hyperson' 'ibm'\n",
      " 'impact' 'incid' 'incompress' 'increas' 'increment' 'increment,' 'initi'\n",
      " 'inlet,' 'instance,' 'integr' 'intend' 'interest' 'interfer' 'intern'\n",
      " 'investig' 'inviscid' 'involv' 'irrot' 'is,' 'karman-pohlhausen' 'kind,'\n",
      " 'known' 'laminar' 'large-angl' 'layer' 'lead' 'libbi' 'lift' 'lift,'\n",
      " 'linear' 'littl' 'load' 'local' 'locat' 'low' 'low-spe' 'lower' 'made'\n",
      " 'main' 'maximum' 'may' 'measur' 'mention' 'method' 'minut' 'modifi'\n",
      " 'mount' 'must' 'necessari' 'need' 'neumann' 'newtonian' 'non-'\n",
      " 'nonuniform' 'normal-forc' 'nose' 'novel' 'number' 'obtain' 'occur' 'one'\n",
      " 'order' 'origin' 'outsid' 'paper,' 'part' 'past' 'per' 'pitching-mo'\n",
      " 'plane' 'plate' 'point' 'possibl' 'potenti' \"prandtl'\" 'predict'\n",
      " 'presenc' 'present' 'presented,' 'pressur' 'previous' 'problem' 'process'\n",
      " 'produc' 'profil' 'program' 'propel' 'protrud' 'provid' 'pure' 'rae'\n",
      " 'ramp' 'rang' 'ratio' 'recent' 'refin' 'region' 'remain' 'requir'\n",
      " 'restrict' 'result' 'revert' 'revolution,' 'reynold' 'rotat'\n",
      " 'satisfactorili' 'scope' 'second' 'secondari' 'section' 'seidel' 'set'\n",
      " 'sever' 'shape' 'shape,' 'shear' 'shock' 'show' 'shown' 'simpl'\n",
      " 'simplest' 'singl' 'situat' 'skin' 'slipstream' 'slope' 'small' 'solid,'\n",
      " 'solid-body,' 'solut' 'solv' 'somewhat' 'sourc' 'span' 'spanwis' 'specif'\n",
      " 'speed' 'stabil' 'steadi' 'still' 'stream' 'stream,' 'studi' 'substanti'\n",
      " 'subtract' 'suction' 'suggest' 'superson' 'support' 'surfac' 'symmetr'\n",
      " 'symmetri' 'take' 'techniqu' 'theoret' 'theori' 'thickness,' 'thin'\n",
      " 'thing' 'three' 'time' 'togeth' 'travers' 'treat' 'treatment' 'turbul'\n",
      " 'two' 'two-dimension' 'uniform' 'upon' 'use' 'usual' 'valu' 'vari'\n",
      " 'variabl' 'veloc' 'view' 'viscos' 'viscou' 'vortic' 'wave' 'wave,' 'wedg'\n",
      " 'well' 'whose' 'wing' 'within']\n",
      "[(1, 'solut', 'D3', 2, 0.301), (2, 'solut', 'D4', 4, 0.2007)]\n",
      "Termes uniques :  ['.' '..' '/destalling/' '0.1' '1' '1.5' '1.6x10' '10' '101' '3,.'\n",
      " '3.2x10' '300' '5' '704' 'acc' 'act' 'aerodynam' 'aerodynamic-centre'\n",
      " 'agr' 'algebra' 'almost' 'also' 'angl' 'another,' 'appl' 'apply'\n",
      " 'approxim' 'approximation,' 'arbit' 'are' 'area' 'arises,' 'attack' 'ax'\n",
      " 'bas' 'behind' 'blunt-nosed' 'body' 'bound' 'boundary-' 'boundary-layer'\n",
      " 'boundary-layer-control' 'boundary-value' 'bow' 'calc' 'cap' 'cas' 'cent'\n",
      " 'center,.but' 'charact' 'characteristics,' 'check' 'circ' 'class'\n",
      " 'coefficy' 'comp' 'comparison' 'compress' 'comput' 'con' 'conceiv'\n",
      " 'conclud' 'cone,' 'config' 'consequ' 'consequently,' 'consid' 'const'\n",
      " 'conv' 'coordin' 'curv' 'curves,' 'deal' 'defin' 'depend' 'destal'\n",
      " 'determin' 'develop' 'diff' 'dimend' 'dimin' 'discuss' 'distribut'\n",
      " 'distribution,' 'disturb' 'drag' 'due' 'dynam' 'eas' 'edg' 'edpm'\n",
      " 'effect' 'efficy' 'eith' 'embed' 'emit' 'empir' 'enough' 'entir' 'entrop'\n",
      " 'equ' 'equations,' 'evalu' 'evidence,' 'ex' 'exact' 'examin' 'exampl'\n",
      " 'example,' 'expect' 'expery' 'fact' 'feat' 'ferr' 'field' 'first,' 'flap'\n",
      " 'flar' 'flat' 'flight' 'flow' 'fluid' 'found' 'fre' 'fredholm'\n",
      " 'free-stream' 'friction,' 'gen' 'giv' 'gre' 'grow' 'high-speed' 'hours,'\n",
      " 'hyperson' 'ibm' 'impact' 'incid' 'incompress' 'incr' 'increas'\n",
      " 'increment,' 'init' 'inlet,' 'instance,' 'integr' 'intend' 'interest'\n",
      " 'interf' 'intern' 'investig' 'inviscid' 'involv' 'irrot' 'is,'\n",
      " 'karman-pohlhaus' 'kind,' 'known' 'lamin' 'large-angled' 'lay' 'lead'\n",
      " 'libby' 'lift' 'lift,' 'linear' 'littl' 'load' 'loc' 'low' 'low-speed'\n",
      " 'mad' 'main' 'maxim' 'may' 'meas' 'ment' 'method' 'minut' 'mod' 'mount'\n",
      " 'must' 'necess' 'nee' 'neuman' 'newton' 'non-' 'nonuniform'\n",
      " 'normal-force' 'nos' 'novel' 'numb' 'obtain' 'occ' 'on' 'ord' 'origin'\n",
      " 'outsid' 'paper,' 'part' 'past' 'per' 'pitching-moment' 'plan' 'plat'\n",
      " 'point' 'poss' 'pot' \"prandtl's\" 'predict' 'pres' 'presented,' 'press'\n",
      " 'prevy' 'problem' 'process' 'produc' 'profil' 'program' 'propel'\n",
      " 'protrud' 'provid' 'pur' 'rae' 'ramp' 'rang' 'ratio' 'rec' 'refin' 'reg'\n",
      " 'remain' 'requir' 'restrict' 'result' 'revert' 'revolution,' 'reynold'\n",
      " 'rot' 'satisfact' 'scop' 'second' 'sect' 'seidel' 'set' 'sev' 'shap'\n",
      " 'shape,' 'shear' 'shock' 'show' 'shown' 'simpl' 'simplest' 'singl' 'situ'\n",
      " 'skin' 'slipstream' 'slop' 'smal' 'solid,' 'solid-body,' 'solv' 'somewh'\n",
      " 'sourc' 'span' 'spanw' 'spec' 'spee' 'stabl' 'steady' 'stil' 'stream'\n",
      " 'stream,' 'streams' 'study' 'subst' 'subtract' 'suct' 'suggest'\n",
      " 'superson' 'support' 'surfac' 'symmet' 'symmetry' 'tak' 'techn' 'the'\n",
      " 'theoret' 'thickness,' 'thin' 'thing' 'three' 'tim' 'togeth' 'travers'\n",
      " 'tre' 'turb' 'two' 'two-dimensional' 'uniform' 'upon' 'us' 'valu' 'vary'\n",
      " 'veloc' 'view' 'visc' 'viscos' 'vort' 'wav' 'wave,' 'wedg' 'wel' 'whos'\n",
      " 'wing' 'within']\n",
      "[(1, 'solv', 'D3', 2, 0.301), (2, 'solv', 'D4', 8, 0.4014)]\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    app = QApplication(sys.argv)\n",
    "    app.setStyleSheet(\"\"\"\n",
    "    QMainWindow {\n",
    "        background-color: #f5f5f5;\n",
    "    }\n",
    "    QLabel {\n",
    "        color: #333333;\n",
    "        font-size: 14px;\n",
    "    }\n",
    "    QLineEdit {\n",
    "        border: 1px solid #CCCCCC;\n",
    "        border-radius: 5px;\n",
    "        padding: 5px;\n",
    "    }\n",
    "    QPushButton {\n",
    "        background-color: #4CAF50;\n",
    "        color: white;\n",
    "        font-size: 14px;\n",
    "        padding: 5px 10px;\n",
    "        border-radius: 5px;\n",
    "    }\n",
    "    QPushButton:hover {\n",
    "        background-color: #45a049;\n",
    "    }\n",
    "    \n",
    "    QRadioButton {\n",
    "        font-size: 13px;\n",
    "    }\n",
    "    QGroupBox {\n",
    "        font-size: 15px;\n",
    "        color: #333333;\n",
    "        border: 1px solid #CCCCCC;\n",
    "        border-radius: 8px;\n",
    "        margin-top: 10px;\n",
    "        padding: 10px;\n",
    "    }\n",
    "    QTextEdit {\n",
    "        background-color: #f0f0f0;\n",
    "        border: 1px solid #CCCCCC;\n",
    "        border-radius: 5px;\n",
    "        padding: 5px;\n",
    "    }\n",
    "    QTableWidget {\n",
    "        background-color: #FFFFFF;\n",
    "        border: 1px solid #CCCCCC;\n",
    "        border-radius: 5px;\n",
    "        padding: 2px;\n",
    "        gridline-color: #E0E0E0;\n",
    "    }\n",
    "    QTableWidget::item {\n",
    "        padding: 5px;\n",
    "        border-bottom: 1px solid #E0E0E0;\n",
    "    }\n",
    "    QHeaderView::section {\n",
    "        background-color: #f0f0f0;\n",
    "        padding: 5px;\n",
    "        border: 1px solid #CCCCCC;\n",
    "        font-weight: bold;\n",
    "    }\n",
    "\n",
    "    QLabel {\n",
    "        margin-left: 20px;\n",
    "        background-color: transparent;\n",
    "        border: none;\n",
    "        font-size: 14px;\n",
    "        font-family: Arial, sans-serif;\n",
    "    }\n",
    "    \n",
    "\"\"\")\n",
    "\n",
    "    window = SearchApp()\n",
    "    window.show()\n",
    "    sys.exit(app.exec_())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
