{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import BIB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyQt5.QtWidgets import (\n",
    "    QApplication, QMainWindow, QWidget, QVBoxLayout, QHBoxLayout,\n",
    "    QLineEdit, QPushButton, QRadioButton, QLabel, QGroupBox,\n",
    "    QTableWidget, QTableWidgetItem, QScrollArea, QTextEdit, QStackedWidget, QGridLayout,\n",
    "    QMessageBox,QHeaderView,QComboBox\n",
    ")\n",
    "from PyQt5.QtGui import QIcon\n",
    "from PyQt5.QtCore import Qt\n",
    "import sys\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.1.3 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\asyncio\\base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\asyncio\\base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\ryan\\AppData\\Local\\Temp\\ipykernel_5396\\2050946007.py\", line 1, in <module>\n",
      "    import nltk\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\nltk\\__init__.py\", line 133, in <module>\n",
      "    from nltk.collocations import *\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\nltk\\collocations.py\", line 36, in <module>\n",
      "    from nltk.metrics import (\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\nltk\\metrics\\__init__.py\", line 18, in <module>\n",
      "    from nltk.metrics.association import (\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\nltk\\metrics\\association.py\", line 26, in <module>\n",
      "    from scipy.stats import fisher_exact\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\scipy\\stats\\__init__.py\", line 606, in <module>\n",
      "    from ._stats_py import *\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\scipy\\stats\\_stats_py.py\", line 37, in <module>\n",
      "    from scipy import sparse\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\scipy\\__init__.py\", line 134, in __getattr__\n",
      "    return _importlib.import_module(f'scipy.{name}')\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\importlib\\__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\scipy\\sparse\\__init__.py\", line 295, in <module>\n",
      "    from ._csr import *\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\scipy\\sparse\\_csr.py\", line 11, in <module>\n",
      "    from ._sparsetools import (csr_tocsc, csr_tobsr, csr_count_blocks,\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.1.3 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\asyncio\\base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\asyncio\\base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\ryan\\AppData\\Local\\Temp\\ipykernel_5396\\2050946007.py\", line 1, in <module>\n",
      "    import nltk\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\nltk\\__init__.py\", line 133, in <module>\n",
      "    from nltk.collocations import *\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\nltk\\collocations.py\", line 36, in <module>\n",
      "    from nltk.metrics import (\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\nltk\\metrics\\__init__.py\", line 38, in <module>\n",
      "    from nltk.metrics.scores import (\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\nltk\\metrics\\scores.py\", line 15, in <module>\n",
      "    from scipy.stats.stats import betai\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\scipy\\stats\\__init__.py\", line 606, in <module>\n",
      "    from ._stats_py import *\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\scipy\\stats\\_stats_py.py\", line 37, in <module>\n",
      "    from scipy import sparse\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\scipy\\__init__.py\", line 134, in __getattr__\n",
      "    return _importlib.import_module(f'scipy.{name}')\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\importlib\\__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\scipy\\sparse\\__init__.py\", line 295, in <module>\n",
      "    from ._csr import *\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\scipy\\sparse\\_csr.py\", line 11, in <module>\n",
      "    from ._sparsetools import (csr_tocsc, csr_tobsr, csr_count_blocks,\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.1.3 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\asyncio\\base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\asyncio\\base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\ryan\\AppData\\Local\\Temp\\ipykernel_5396\\2050946007.py\", line 1, in <module>\n",
      "    import nltk\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\nltk\\__init__.py\", line 146, in <module>\n",
      "    from nltk.chunk import *\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\nltk\\chunk\\__init__.py\", line 155, in <module>\n",
      "    from nltk.chunk.api import ChunkParserI\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\nltk\\chunk\\api.py\", line 15, in <module>\n",
      "    from nltk.parse import ParserI\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\nltk\\parse\\__init__.py\", line 100, in <module>\n",
      "    from nltk.parse.transitionparser import TransitionParser\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\nltk\\parse\\transitionparser.py\", line 17, in <module>\n",
      "    from scipy import sparse\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\scipy\\__init__.py\", line 134, in __getattr__\n",
      "    return _importlib.import_module(f'scipy.{name}')\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\importlib\\__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\scipy\\sparse\\__init__.py\", line 295, in <module>\n",
      "    from ._csr import *\n",
      "  File \"c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\scipy\\sparse\\_csr.py\", line 11, in <module>\n",
      "    from ._sparsetools import (csr_tocsc, csr_tobsr, csr_count_blocks,\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import os\n",
    "import math\n",
    "from collections import defaultdict\n",
    "from nltk import FreqDist\n",
    "\n",
    "\n",
    "STOPWORDS = set(nltk.corpus.stopwords.words('english'))\n",
    "PORTER_STEMMER = nltk.PorterStemmer()\n",
    "LANCASTER_STEMMER = nltk.LancasterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_processing_args():\n",
    "    tokenization = \"Split\"\n",
    "    normalization = \"None\",\n",
    "    file_type = \"TPD\"\n",
    "    return tokenization, normalization, file_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(doc_path, tokenization, normalization):\n",
    "    with open(doc_path, 'r') as file:\n",
    "        text = file.read()\n",
    "        \n",
    "    # Tokenization\n",
    "    if tokenization == \"Split\":\n",
    "        tokens = text.split()\n",
    "    else:\n",
    "        exp_reg = nltk.RegexpTokenizer(r'\\d+(?:\\.\\d+)?x\\d+|\\d+(?:\\.\\d+)|\\w+(?:-\\w+)*|(?:[A-Z]\\.)+|\\w+')\n",
    "        tokens = exp_reg.tokenize(text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    tokens = [term for term in tokens if term.lower() not in STOPWORDS]\n",
    "\n",
    "    # Normalization\n",
    "    if normalization == \"Porter\":\n",
    "        tokens = [PORTER_STEMMER.stem(term) for term in tokens]\n",
    "    elif normalization == \"Lancaster\":\n",
    "        tokens = [LANCASTER_STEMMER.stem(term) for term in tokens]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_global_term_frequencies(tokenization, normalization):\n",
    "    global_term_frequencies = defaultdict(int)\n",
    "\n",
    "    for doc_name in os.listdir('Collections'):\n",
    "        doc_path = os.path.join('Collections', doc_name)\n",
    "        tokens = preprocessing(doc_path, tokenization, normalization)\n",
    "        unique_terms = set(tokens)\n",
    "\n",
    "        for term in unique_terms:\n",
    "            global_term_frequencies[term] += 1\n",
    "            \n",
    "    return global_term_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TPD_result(query, terms_freq, global_term_frequencies, N):\n",
    "    max_freq = max(terms_freq.values())\n",
    "    results=[]\n",
    "    for idx, (term, freq) in enumerate(terms_freq.items(), start=1):\n",
    "        poids = (freq / max_freq) * math.log10((N / global_term_frequencies[term]) + 1)\n",
    "        results.append((idx, term, query, freq, round(poids, 4)))\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nb_termes_glob(tokenization, normalization):\n",
    "    nb_termes_global = []\n",
    "    for doc_name in os.listdir('Collections'):\n",
    "        doc_path = os.path.join('Collections', doc_name)\n",
    "        # Appliquer le prétraitement pour obtenir les tokens du document\n",
    "        tokens = preprocessing(doc_path, tokenization, normalization)\n",
    "        \n",
    "        # Ajouter les tokens du document à la liste globale\n",
    "        nb_termes_global.extend(tokens)\n",
    "\n",
    "    # Obtenir le nombre de termes uniques\n",
    "    termes_uniques = np.unique(nb_termes_global)\n",
    "    print(\"Termes uniques : \", termes_uniques)  # Optionnel : pour visualiser les termes uniques\n",
    "    \n",
    "    return len(termes_uniques)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Termes uniques :  ['0.1' '1' '1.5' '1.6x10' '10' '101' '3' '3.2x10' '300' '5' '704'\n",
      " 'accuracy' 'accurate' 'actual' 'aerodynamic' 'aerodynamic-centre'\n",
      " 'aerodynamics' 'agree' 'algebraic' 'almost' 'also' 'angles' 'another'\n",
      " 'applicability' 'applied' 'approximate' 'approximation' 'arbitrary'\n",
      " 'area' 'areas' 'arises' 'attack' 'axial' 'axially' 'based' 'basis'\n",
      " 'behind' 'blunt-nosed' 'bodies' 'body' 'boundaries' 'boundary'\n",
      " 'boundary-layer' 'boundary-layer-control' 'boundary-value' 'bow'\n",
      " 'calculated' 'calculation' 'capable' 'case' 'cent' 'center'\n",
      " 'characteristics' 'check' 'circulatory' 'classical' 'coefficient'\n",
      " 'coefficients' 'comparative' 'comparison' 'compression' 'computations'\n",
      " 'computed' 'concept' 'concluded' 'cone' 'cones' 'configuration'\n",
      " 'consequently' 'consider' 'considered' 'constant' 'conventional'\n",
      " 'coordinate' 'curved' 'curves' 'dealing' 'define' 'defined' 'depend'\n",
      " 'depending' 'destalling' 'determine' 'developed' 'differ' 'different'\n",
      " 'dimensional' 'diminish' 'discussed' 'discussion' 'distribution'\n",
      " 'disturbance' 'drag' 'due' 'dynamic' 'ease' 'edge' 'edpm' 'effect'\n",
      " 'effects' 'efficient' 'either' 'embedded' 'emitting' 'empirical' 'enough'\n",
      " 'entire' 'entropy' 'equation' 'equations' 'evaluation' 'evidence' 'exact'\n",
      " 'examination' 'example' 'examples' 'exists' 'expected' 'experiment'\n",
      " 'experimental' 'factors' 'feature' 'ferri' 'field' 'fields' 'first'\n",
      " 'flap' 'flare' 'flat' 'flight' 'flow' 'flows' 'fluid' 'found' 'fredholm'\n",
      " 'free' 'free-stream' 'friction' 'general' 'generates' 'give' 'given'\n",
      " 'greater' 'growth' 'high-speed' 'hours' 'hypersonic' 'ibm' 'impact'\n",
      " 'incident' 'incompressible' 'increase' 'increasing' 'increment' 'initial'\n",
      " 'inlet' 'instance' 'integral' 'integrated' 'intended' 'interesting'\n",
      " 'interference' 'internal' 'investigated' 'investigation' 'inviscid'\n",
      " 'involving' 'irrotational' 'karman-pohlhausen' 'kind' 'known' 'laminar'\n",
      " 'large-angled' 'layer' 'layers' 'leading' 'leads' 'libby' 'lift' 'linear'\n",
      " 'little' 'loading' 'local' 'locally' 'located' 'location' 'low'\n",
      " 'low-speed' 'lower' 'made' 'main' 'maximum' 'may' 'measurements'\n",
      " 'mentioned' 'method' 'methods' 'minutes' 'modified' 'mounted' 'must'\n",
      " 'necessary' 'need' 'needs' 'neumann' 'newtonian' 'non' 'nonuniformity'\n",
      " 'normal-force' 'nose' 'novel' 'number' 'numbers' 'obtained' 'occur' 'one'\n",
      " 'order' 'original' 'outside' 'paper' 'part' 'past' 'per'\n",
      " 'pitching-moment' 'plane' 'plate' 'points' 'possible' 'potential'\n",
      " 'prandtl' 'predict' 'predicts' 'presence' 'present' 'presented'\n",
      " 'pressure' 'pressures' 'previously' 'problem' 'problems' 'process'\n",
      " 'produced' 'profile' 'programed' 'propeller' 'protrude' 'provided'\n",
      " 'purely' 'rae' 'ramp' 'range' 'ratio' 'ratios' 'recently' 'refinement'\n",
      " 'region' 'remaining' 'require' 'restricted' 'result' 'results' 'revert'\n",
      " 'revolution' 'reynolds' 'rotational' 'satisfactorily' 'scope' 'second'\n",
      " 'secondary' 'section' 'seidel' 'set' 'several' 'shape' 'shear' 'shock'\n",
      " 'show' 'showed' 'shown' 'simple' 'simplest' 'single' 'situation' 'skin'\n",
      " 'slipstream' 'slopes' 'small' 'solid' 'solid-body' 'solution' 'solutions'\n",
      " 'solve' 'solved' 'solving' 'somewhat' 'source' 'span' 'spanwise'\n",
      " 'specific' 'speed' 'stabilizer' 'stabilizers' 'steady' 'still' 'stream'\n",
      " 'streams' 'study' 'substantial' 'subtracting' 'suction' 'suggests'\n",
      " 'supersonic' 'supporting' 'surface' 'symmetric' 'symmetry' 'takes'\n",
      " 'technique' 'theoretical' 'theory' 'thickness' 'thin' 'things' 'three'\n",
      " 'time' 'together' 'traverses' 'treated' 'treatments' 'turbulent' 'two'\n",
      " 'two-dimensional' 'uniform' 'upon' 'used' 'useful' 'usually' 'value'\n",
      " 'values' 'variable' 'vary' 'velocities' 'velocity' 'viewed' 'viscosity'\n",
      " 'viscous' 'vorticity' 'wave' 'wedges' 'well' 'whose' 'wing' 'within']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "353"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb = nb_termes_glob(\"split\" ,None)\n",
    "nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_input(query, normalization):\n",
    "    # appliquer le traitement sur la requete\n",
    "    if normalization == \"Porter\":\n",
    "        query = PORTER_STEMMER.stem(query) \n",
    "    elif normalization == \"Lancaster\":\n",
    "        query = LANCASTER_STEMMER.stem(query) \n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_processing(query,tokenization, normalization, file_type):\n",
    "    # tokenization, normalization, file_type = get_processing_args()\n",
    "    nb_terms = 0\n",
    "    global_term_frequencies = build_global_term_frequencies(tokenization, normalization)  # Calculate global term frequencies\n",
    "    N = len(os.listdir('Collections'))\n",
    "    results =[]\n",
    "    if file_type == \"TPD\":\n",
    "        doc_path = os.path.join('Collections', f\"{query}.txt\")\n",
    "        tokens = preprocessing(doc_path, tokenization, normalization)\n",
    "        nb_terms = len(np.unique(tokens))\n",
    "        terms_freq = FreqDist(tokens)\n",
    "        \n",
    "        result = TPD_result(query, terms_freq, global_term_frequencies, N)\n",
    "        \n",
    "        return result , nb_terms\n",
    "        \n",
    "    else :\n",
    "        query = process_input(query, normalization)\n",
    "        i=0\n",
    "        for doc_name in os.listdir('Collections'):\n",
    "            doc_path = os.path.join('Collections', doc_name)\n",
    "            Tokens = preprocessing(doc_path, tokenization, normalization)\n",
    "            terms_freq = FreqDist(Tokens)\n",
    "\n",
    "            max_freq = max(terms_freq.values())\n",
    "            for term, freq in terms_freq.items():  \n",
    "                if term == query:  # Check if the term is the specific query term\n",
    "                    poids = ((freq / max_freq) * math.log10((N / global_term_frequencies[term]) + 1))\n",
    "                    i+=1\n",
    "                    results.append((i, term, os.path.splitext(doc_name)[0], freq, round(poids, 4)))\n",
    "        \n",
    "        return results , nb_terms\n",
    "   \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(query):\n",
    "    # if raw:\n",
    "    doc_path = os.path.join('Collections', f\"{query}.txt\")\n",
    "    with open(doc_path, 'r') as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "    # elif processed:\n",
    "    #     text_processing(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculer_relevance(query, tokenization, normalization, file_type , path ):\n",
    "    # Créer une liste des documents à partir des fichiers dans le dossier\n",
    "    docs = [doc.split('.')[0] for doc in os.listdir(path)]\n",
    "    \n",
    "\n",
    "    relevance_dict = {doc: 0 for doc in docs}\n",
    "    for term in query.split():\n",
    "        occurrence, _ = text_processing(term, tokenization, normalization, file_type)\n",
    "        print(\"Occurrences:\", occurrence)\n",
    "        # exemple de occurrence : [(1, 'effect', 'D1', 1, 0.0795), (2, 'effect', 'D3', 1, 0.1193), (3, 'effect', 'D6', 1, 0.1193)]\n",
    "        \n",
    "        for occ in occurrence:\n",
    "            doc_name = occ[2]  # Nom du document contenant le terme\n",
    "            poids_terme = occ[4]  # Poids du terme dans ce document\n",
    "            relevance_dict[doc_name] += poids_terme\n",
    "            \n",
    "    return relevance_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "\n",
    "def calculer_relevance_cosinus(query, tokenization, normalization, file_type, path):\n",
    "    # Créer une liste des documents à partir des fichiers dans le dossier\n",
    "    docs = [doc.split('.')[0] for doc in os.listdir(path)]\n",
    "    \n",
    "    # Initialiser un dictionnaire pour stocker les poids des termes pour chaque document\n",
    "    doc_vectors = {doc: {} for doc in docs}\n",
    "\n",
    "    # Remplir les vecteurs des documents\n",
    "    for term in query.split():\n",
    "        # Récupérer les occurrences du terme dans les documents\n",
    "        occurrence, _ = text_processing(term, tokenization, normalization, file_type)\n",
    "        print(f\"Occurrences pour le terme '{term}':\", occurrence)\n",
    "        \n",
    "        # Ajouter les poids des termes aux vecteurs des documents\n",
    "        for occ in occurrence:\n",
    "            doc_name = occ[2]  # Nom du document\n",
    "            poids_terme = occ[4]  # Poids du terme dans ce document\n",
    "            doc_vectors[doc_name][term] = poids_terme\n",
    "            \n",
    "            \n",
    "    print(doc_vectors)\n",
    "    # exemple : {'D1': {'effect': 0.0795, 'distribution': 0.0663}, 'D2': {}, 'D3': {'effect': 0.1193, 'distribution': 0.0995}, 'D4': {'distribution': 0.0332}, 'D5': {}, 'D6': {'effect': 0.1193, 'distribution': 0.0995}}\n",
    "    \n",
    "    # Calculer la similarité cosinus entre la requête et chaque document\n",
    "    relevance_dict = {}\n",
    "    for doc_name, doc_vector in doc_vectors.items():\n",
    "     \n",
    "        # Produit scalaire (query_vector · doc_vector), où chaque poids de query est 1\n",
    "        dot_product = sum(doc_vector.get(term, 0) for term in query.split())\n",
    "        \n",
    "        # Norme du vecteur de la requête (sqrt(nombre de termes))\n",
    "        # utilisation de len parceque le vecteur norm est un vecteur boolean\n",
    "        norm_query = math.sqrt(len(doc_vector))\n",
    "        \n",
    "        \n",
    "        # Norme du vecteur du document\n",
    "        norm_doc = math.sqrt(sum(weight ** 2 for weight in doc_vector.values()))\n",
    "        \n",
    "        # Similarité cosinus (éviter la division par zéro)\n",
    "        if norm_query > 0 and norm_doc > 0:\n",
    "            similarity = dot_product / (norm_query * norm_doc)\n",
    "\n",
    "        else:\n",
    "            similarity = 0\n",
    "        \n",
    "        # Ajouter le résultat à la pertinence pour ce document\n",
    "        relevance_dict[doc_name] = similarity\n",
    "\n",
    "    return relevance_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculer_jaccard_similarity1(query, tokenization, normalization, file_type, path):\n",
    "    docs = [doc.split('.')[0] for doc in os.listdir(path)]\n",
    "    relevance_dict = {doc: 0 for doc in docs}\n",
    "    \n",
    "    # Construction des vecteurs de poids pour chaque document et pour la requête\n",
    "    doc_vectors = {doc: {} for doc in docs}\n",
    "    query_vector = {}\n",
    "\n",
    "    for term in query.split():\n",
    "        occurrence, _ = text_processing(term, tokenization, normalization, file_type)\n",
    "        print(\"Occurrences:\", occurrence)\n",
    "        \n",
    "        # Remplissage des vecteurs des documents et de la requête\n",
    "        for occ in occurrence:\n",
    "            doc_name = occ[2]\n",
    "            poids_terme = occ[4]\n",
    "            doc_vectors[doc_name][term] = poids_terme\n",
    "            query_vector[term] = query_vector.get(term, 0) + poids_terme\n",
    "\n",
    "    # Calcul de la mesure de Jaccard pour chaque document\n",
    "    jaccard_similarity = {}\n",
    "    for doc, term_weights in doc_vectors.items():\n",
    "        intersection = sum(query_vector.get(term, 0) * weight for term, weight in term_weights.items())\n",
    "        sum_query_squares = sum(weight**2 for weight in query_vector.values())\n",
    "        sum_doc_squares = sum(weight**2 for weight in term_weights.values())\n",
    "        union = sum_query_squares + sum_doc_squares - intersection\n",
    "\n",
    "        if union == 0:\n",
    "            jaccard_similarity[doc] = 0  # Pour éviter une division par zéro\n",
    "        else:\n",
    "            jaccard_similarity[doc] = intersection / union\n",
    "    \n",
    "    return jaccard_similarity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculer_jaccard_similarity(query, tokenization, normalization, file_type, path):\n",
    "    docs = [doc.split('.')[0] for doc in os.listdir(path)]\n",
    "    relevance_dict = {doc: 0 for doc in docs}\n",
    "    \n",
    "    # Construction des vecteurs de poids pour chaque document et pour la requête\n",
    "    doc_vectors = {doc: {} for doc in docs}\n",
    "    query_vector = {}\n",
    "\n",
    "    for term in query.split():\n",
    "        occurrence, _ = text_processing(term, tokenization, normalization, file_type)\n",
    "        print(\"Occurrences:\", occurrence)\n",
    "        \n",
    "        # Remplissage des vecteurs des documents et de la requête\n",
    "        for occ in occurrence:\n",
    "            doc_name = occ[2]\n",
    "            poids_terme = occ[4]\n",
    "            doc_vectors[doc_name][term] = poids_terme\n",
    "            query_vector[term] = query_vector.get(term, 0) + poids_terme\n",
    "            \n",
    "    print(f\" doc vectors : {doc_vectors}\")\n",
    "    print(f\"vecteur query {query_vector}\")\n",
    "    # Calcul de la mesure de Jaccard pour chaque document\n",
    "    jaccard_similarity = {}\n",
    "    for doc, term_weights in doc_vectors.items():\n",
    "        # intersection = sum(query_vector.get(term, 0) * weight for term, weight in term_weights.items())\n",
    "        intersection = sum( weight for term, weight in term_weights.items())\n",
    "        \n",
    "        # sum_query_squares = sum(weight**2 for weight in query_vector.values())\n",
    "        sum_query_squares = math.sqrt(len(term_weights))\n",
    "        sum_doc_squares = sum(weight**2 for weight in term_weights.values())\n",
    "        union = sum_query_squares + sum_doc_squares - intersection\n",
    "\n",
    "        if union == 0:\n",
    "            jaccard_similarity[doc] = 0  # Pour éviter une division par zéro\n",
    "        else:\n",
    "            jaccard_similarity[doc] = intersection / union\n",
    "    \n",
    "    return jaccard_similarity\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SearchApp(QMainWindow):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.path = 'Collections'\n",
    "        self.setWindowTitle(\"Document Search and Processing\")\n",
    "        self.setGeometry(100, 100, 1050, 700) #8,6 => 900?700 \n",
    "        self.setWindowIcon(QIcon(\"./icons/interface_icon.png\")) \n",
    "        self.setFixedSize(1050, 700) #900:700\n",
    "        \n",
    "        \n",
    "        # Layout principal\n",
    "        central_widget = QWidget()\n",
    "        self.setCentralWidget(central_widget)\n",
    "        self.main_layout = QVBoxLayout(central_widget)\n",
    "\n",
    "        # Barre de recherche\n",
    "        search_layout = QHBoxLayout()\n",
    "        query_label = QLabel(\"Query: \", self)\n",
    "        search_layout.addWidget(query_label)\n",
    "        \n",
    "        self.search_bar = QLineEdit(self)\n",
    "        self.search_bar.setPlaceholderText(\"Enter document name...\")\n",
    "        self.search_button = QPushButton(\"Search\", self)\n",
    "        \n",
    "        search_layout.addWidget(self.search_bar)\n",
    "        search_layout.addWidget(self.search_button)\n",
    "        self.main_layout.addLayout(search_layout)\n",
    "\n",
    "        # Options de radio\n",
    "        radio_layout = QHBoxLayout()\n",
    "        self.raw_text_radio = QRadioButton(\"Raw Text\", self)\n",
    "        self.processed_text_radio = QRadioButton(\"Processed Text\", self)\n",
    "        radio_layout.addWidget(self.raw_text_radio)\n",
    "        radio_layout.addWidget(self.processed_text_radio)\n",
    "        self.main_layout.addLayout(radio_layout)\n",
    "        \n",
    "        # Section Tokenization\n",
    "        tokenization_box = QGroupBox(\"Tokenization\")\n",
    "        tokenization_layout = QVBoxLayout()\n",
    "        self.split_radio = QRadioButton(\"Split\", self)\n",
    "        self.regex_radio = QRadioButton(\"Regex\", self)\n",
    "        tokenization_layout.addWidget(self.split_radio)\n",
    "        tokenization_layout.addWidget(self.regex_radio)\n",
    "        tokenization_box.setLayout(tokenization_layout)\n",
    "        \n",
    "        # Section Normalization\n",
    "        normalization_box = QGroupBox(\"Normalization\")\n",
    "        normalization_layout = QVBoxLayout()\n",
    "        self.no_stem_radio = QRadioButton(\"No Stem\", self)\n",
    "        self.porter_radio = QRadioButton(\"Porter\", self)\n",
    "        self.lancaster_radio = QRadioButton(\"Lancaster\", self)\n",
    "        normalization_layout.addWidget(self.no_stem_radio)\n",
    "        normalization_layout.addWidget(self.porter_radio)\n",
    "        normalization_layout.addWidget(self.lancaster_radio)\n",
    "        normalization_box.setLayout(normalization_layout)\n",
    "        \n",
    "        # Section Indexation\n",
    "        indexation_box = QGroupBox(\"Indexation\")\n",
    "        indexation_layout = QVBoxLayout()\n",
    "        self.doc_per_term_radio = QRadioButton(\"Documents per Term\", self)\n",
    "        self.term_per_doc_radio = QRadioButton(\"Terms per Document\", self)\n",
    "        indexation_layout.addWidget(self.doc_per_term_radio)\n",
    "        indexation_layout.addWidget(self.term_per_doc_radio)\n",
    "        indexation_box.setLayout(indexation_layout)\n",
    "        \n",
    "        # Section Matching\n",
    "        matching_box = QGroupBox(\"Matching\")\n",
    "\n",
    "        # Création des options de sélection\n",
    "        matching_layout = QVBoxLayout()\n",
    "        \n",
    "        self.vector_space_radio = QRadioButton(\"Vector Space Model\")\n",
    "        matching_layout.addWidget(self.vector_space_radio)\n",
    "        \n",
    "        self.matching_options = QComboBox()\n",
    "        self.matching_options.addItems([\"Produit Scalaire\", \"Similarité Cosinus\", \"Indice de Jaccard\"])\n",
    "\n",
    "        # Ajout du menu déroulant au layout de la section\n",
    "        matching_layout.addWidget(self.matching_options)\n",
    "        matching_box.setLayout(matching_layout)\n",
    "        \n",
    "        \n",
    "        # Disposition des sections\n",
    "        sections_layout = QHBoxLayout()\n",
    "        sections_layout.addWidget(tokenization_box)\n",
    "        sections_layout.addWidget(normalization_box)\n",
    "        sections_layout.addWidget(indexation_box)\n",
    "        sections_layout.addWidget(matching_box) \n",
    "        self.main_layout.addLayout(sections_layout)\n",
    "        \n",
    "        # Zone de résultats (QStackedWidget pour alterner entre texte et tableau)\n",
    "        self.result_label = QLabel(\"Result: \", self)\n",
    "        self.main_layout.addWidget(self.result_label)\n",
    "\n",
    "        self.result_area = QStackedWidget(self)\n",
    "        self.result_area.setFixedHeight(400)  # Taille fixe pour éviter d'étendre la mise en page\n",
    "        self.result_area.setFixedWidth(600)  # Ajustez selon la largeur désirée\n",
    "        \n",
    "        # Widget pour afficher le texte brut\n",
    "        self.raw_text_widget = QTextEdit(self)\n",
    "        self.raw_text_widget.setReadOnly(True)  # Rendre le texte en lecture seule\n",
    "        self.result_area.addWidget(self.raw_text_widget)\n",
    "        \n",
    "        # Widget pour afficher le tableau\n",
    "        self.table = QTableWidget(0, 5, self)  # 5 colonnes pour N°, N° doc, terme, fréquence, poids\n",
    "        self.table.setHorizontalHeaderLabels([\"N°\", \"N° doc\", \"Term\", \"Frequency\", \"Weight\"])\n",
    "        self.table.setShowGrid(False)  # Masquer la grille du tableau\n",
    "        \n",
    "        # Faire en sorte que les colonnes s'étendent pour couvrir toute la largeur\n",
    "        header = self.table.horizontalHeader()\n",
    "        header.setSectionResizeMode(QHeaderView.Stretch)\n",
    "        self.result_area.addWidget(self.table)\n",
    "        \n",
    "        self.main_layout.addWidget(self.result_area)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #    //////////////////////////////////////\n",
    "        Total_terms_layout = QHBoxLayout()\n",
    "        \n",
    "        # Création et configuration des QLabel\n",
    "        self.terms_per_doc = QLabel(self)\n",
    "        self.terms_all_doc = QLabel(self)\n",
    "        \n",
    "        # Appliquer les styles pour enlever le fond et les bordures\n",
    "        style = \"\"\"\n",
    "            QLabel {\n",
    "                margin-left: 20px;\n",
    "                background-color: transparent;\n",
    "                border: none;\n",
    "                font-size: 14px;\n",
    "                font-family: Arial, sans-serif;\n",
    "            }\n",
    "        \"\"\"\n",
    "        self.terms_all_doc.setStyleSheet(style)\n",
    "        self.terms_per_doc.setStyleSheet(style)\n",
    "        \n",
    "        \n",
    "        # Ajout des QLabel au layout horizontal\n",
    "        Total_terms_layout.addWidget(self.terms_per_doc)\n",
    "        Total_terms_layout.addWidget(self.terms_all_doc)\n",
    "        \n",
    "        # Ajout du layout horizontal dans le layout principal\n",
    "        self.main_layout.addLayout(Total_terms_layout)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # Ajustements dans le code principal\n",
    "        self.main_layout.setContentsMargins(2, 2, 2, 2)  # Réduire les marges globales\n",
    "        self.main_layout.setSpacing(8)  # Diminuer l'espace entre les sections\n",
    "        self.result_area.setContentsMargins(2, 0, 2, 0)  # Marges gauche et droite de 2px pour le tableau\n",
    "        self.result_area.setFixedWidth(self.width() - 4) \n",
    "                \n",
    "                \n",
    "        # events\n",
    "        self.search_button.clicked.connect(self.process_search)\n",
    "        self.raw_text_radio.clicked.connect(self.raw_text_radio_process)\n",
    "        self.processed_text_radio.clicked.connect(self.processed_text_radio_process)\n",
    "        self.vector_space_radio.toggled.connect(self.toggle_radio_buttons)\n",
    "        \n",
    "    def toggle_radio_buttons(self,state):\n",
    "        self.doc_per_term_radio.setEnabled(not state) \n",
    "        self.term_per_doc_radio.setEnabled(not state)\n",
    "        \n",
    "    def raw_text_radio_process(self):\n",
    "        self.vector_space_radio.setEnabled(False)\n",
    "        self.matching_options.setEnabled(False)\n",
    "        self.split_radio.setEnabled(False) \n",
    "        self.regex_radio.setEnabled(False)\n",
    "        self.lancaster_radio.setEnabled(False)\n",
    "        self.porter_radio.setEnabled(False)\n",
    "        self.doc_per_term_radio.setEnabled(False)\n",
    "        self.term_per_doc_radio.setEnabled(False)\n",
    "        self.no_stem_radio.setEnabled(False)\n",
    "        self.terms_per_doc.setText(\"\")\n",
    "        self.terms_all_doc.setText(\"\")\n",
    "       \n",
    "       \n",
    "    def processed_text_radio_process(self):\n",
    "        self.vector_space_radio.setEnabled(True)\n",
    "        self.matching_options.setEnabled(True)\n",
    "        self.split_radio.setEnabled(True) \n",
    "        self.regex_radio.setEnabled(True)\n",
    "        self.lancaster_radio.setEnabled(True)\n",
    "        self.porter_radio.setEnabled(True)\n",
    "        self.doc_per_term_radio.setEnabled(True)\n",
    "        self.term_per_doc_radio.setEnabled(True)\n",
    "        self.no_stem_radio.setEnabled(True)\n",
    "        \n",
    "         \n",
    "    def display_Total_Terms(self, termes_global, nb_termes ,index):\n",
    "        if nb_termes != 0 and index == 'TPD':\n",
    "            # Afficher le nombre de termes par document\n",
    "            self.terms_per_doc.setText(f\"Terms per document : {nb_termes}\")\n",
    "        else :\n",
    "            self.terms_per_doc.setText(\"\")\n",
    "        self.terms_all_doc.setText(f\"Total terms  : {termes_global}\")\n",
    "\n",
    "        \n",
    "        \n",
    "    def process_search(self):\n",
    "        # Obtenir le numéro de document\n",
    "        document_number = self.search_bar.text()\n",
    "        \n",
    "        if not document_number:\n",
    "            self.show_error(\"Veuillez entrer un numéro de document valide.\")\n",
    "            return\n",
    "\n",
    "        # \n",
    "        # Vérifier le type de texte sélectionné\n",
    "        if self.raw_text_radio.isChecked():\n",
    "            # verification de nom_document\n",
    "            result = get_text(document_number)\n",
    "            self.show_raw_text(result)\n",
    "        \n",
    "        else:\n",
    "            # Obtenir les méthodes sélectionnées\n",
    "            tokenization_method = \"Split\" if self.split_radio.isChecked() else \"Regex\"\n",
    "            if self.porter_radio.isChecked() :\n",
    "                normalization_method = \"Porter\" \n",
    "            elif self.no_stem_radio.isChecked():\n",
    "                normalization_method = \"None\" \n",
    "            else :\n",
    "                normalization_method =\"Lancaster\"\n",
    "            indexation_method = \"DPT\" if self.doc_per_term_radio.isChecked() else \"TPD\"\n",
    "            \n",
    "            if self.vector_space_radio.isChecked():\n",
    "            # Récupération de la méthode de matching sélectionnée\n",
    "                matching_method = self.matching_options.currentText()\n",
    "                # Utilisation\n",
    "                if matching_method == \"Produit Scalaire\":\n",
    "                    print(\"produit scalaire\")\n",
    "                    relevances = calculer_relevance(document_number, tokenization_method, normalization_method, 'DPT',self.path)\n",
    "                    print(relevances)\n",
    "\n",
    "                elif matching_method == \"Similarité Cosinus\":\n",
    "                    print(\"produit cosine\")\n",
    "                    relevances = calculer_relevance_cosinus(document_number, tokenization_method, normalization_method, 'DPT',self.path)\n",
    "                    print(relevances)\n",
    "\n",
    "                elif matching_method == \"Indice de Jaccard\":\n",
    "                    print(\"jacard\")\n",
    "                    relevances = calculer_jaccard_similarity(document_number, tokenization_method, normalization_method, 'DPT',self.path)\n",
    "                    print(relevances)\n",
    "                filtered_relevances = {doc: score for doc, score in relevances.items() if score > 0}\n",
    "                sorted_relevances = sorted(filtered_relevances.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "                # Afficher les relevances triées\n",
    "                self.display_relevance(sorted_relevances)\n",
    "            else :\n",
    "                termes_global = nb_termes_glob(tokenization_method, normalization_method)\n",
    "                # Appeler la fonction pour obtenir les données\n",
    "                data , nb_termes = text_processing(document_number, tokenization_method, normalization_method, indexation_method)\n",
    "                print(data)\n",
    "                self.display_results(data)\n",
    "                self.display_Total_Terms(termes_global , nb_termes , indexation_method)\n",
    "                \n",
    "                     \n",
    "\n",
    "    def reset_table(self, column_headers):\n",
    "        \"\"\"\n",
    "        Réinitialise complètement le tableau avec de nouvelles colonnes et leurs en-têtes.\n",
    "\n",
    "        Parameters:\n",
    "            column_headers (list of str): Liste des noms des colonnes.\n",
    "        \"\"\"\n",
    "        self.table.clear()  # Efface tout le contenu (cellules et en-têtes)\n",
    "        self.table.setRowCount(0)  # Réinitialise le nombre de lignes\n",
    "        self.table.setColumnCount(len(column_headers))  # Définir le nombre de colonnes\n",
    "        self.table.setHorizontalHeaderLabels(column_headers)  # Définir les en-têtes de colonnes\n",
    "\n",
    "        # Ajuster les colonnes pour s'étendre uniformément\n",
    "        header = self.table.horizontalHeader()\n",
    "        header.setSectionResizeMode(QHeaderView.Stretch)\n",
    "\n",
    "\n",
    "    def show_raw_text(self, text):\n",
    "        self.raw_text_widget.setText(text)\n",
    "        self.result_area.setCurrentWidget(self.raw_text_widget)  # Afficher le widget de texte brut\n",
    "\n",
    "        \n",
    "    def display_results(self, data):\n",
    "    # Supprimer l'affichage de l'index de ligne\n",
    "        self.table.verticalHeader().setVisible(False)\n",
    "        # Réinitialiser le tableau avec les colonnes spécifiques\n",
    "        column_headers = [\"N°\", \"N° doc\", \"Term\", \"Frequency\", \"Weight\"]\n",
    "        self.reset_table(column_headers)\n",
    "        \n",
    "        # Nettoyer le tableau et ajouter les résultats\n",
    "        self.table.setRowCount(0)\n",
    "        for index, row_data in enumerate(data):\n",
    "            row_position = self.table.rowCount()\n",
    "            self.table.insertRow(row_position)\n",
    "            for column, value in enumerate(row_data):\n",
    "                item = QTableWidgetItem(str(value))\n",
    "                item.setTextAlignment(Qt.AlignCenter)  # Centrer le texte dans chaque cellule\n",
    "                self.table.setItem(row_position, column, item)\n",
    "        \n",
    "        # Afficher le widget de tableau\n",
    "        self.result_area.setCurrentWidget(self.table)\n",
    "    \n",
    "\n",
    "\n",
    "    def display_relevance(self, sorted_relevances):\n",
    "        \"\"\"\n",
    "        Affiche les relevances triées dans le tableau.\n",
    "\n",
    "        Parameters:\n",
    "            sorted_relevances (list of tuples): Une liste de tuples (document, score de pertinence).\n",
    "        \"\"\"\n",
    "        # Réinitialiser le tableau avec deux colonnes\n",
    "        column_headers = [\"Document\", \"Score de Pertinence\"]\n",
    "        self.reset_table(column_headers)\n",
    "\n",
    "        # Ajouter les données triées dans le tableau\n",
    "        for doc_id, score in sorted_relevances:\n",
    "            row_position = self.table.rowCount()\n",
    "            self.table.insertRow(row_position)\n",
    "\n",
    "            # Ajouter l'identifiant du document\n",
    "            doc_item = QTableWidgetItem(doc_id)\n",
    "            doc_item.setTextAlignment(Qt.AlignCenter)\n",
    "            self.table.setItem(row_position, 0, doc_item)\n",
    "\n",
    "            # Ajouter le score de pertinence\n",
    "            score_item = QTableWidgetItem(f\"{score:.4f}\")\n",
    "            score_item.setTextAlignment(Qt.AlignCenter)\n",
    "            self.table.setItem(row_position, 1, score_item)\n",
    "\n",
    "        # Afficher le tableau\n",
    "        self.result_area.setCurrentWidget(self.table)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def show_error(self, message):\n",
    "        error_dialog = QMessageBox(self)\n",
    "        error_dialog.setIcon(QMessageBox.Critical)\n",
    "        error_dialog.setWindowTitle(\"Erreur\")\n",
    "        error_dialog.setText(message)\n",
    "        error_dialog.exec_()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Termes uniques :  ['.' '..' '/destalling/' '0.1' '1' '1.5' '1.6x10' '10' '101' '3,.'\n",
      " '3.2x10' '300' '5' '704' 'accuracy' 'accurate' 'actual' 'aerodynamic'\n",
      " 'aerodynamic-centre' 'aerodynamics' 'agree' 'algebraic' 'almost' 'also'\n",
      " 'angles' 'another,' 'applicability' 'applied' 'approximate'\n",
      " 'approximation' 'approximation,' 'arbitrary' 'area' 'areas' 'arises,'\n",
      " 'attack' 'axial' 'axially' 'based' 'basis' 'behind' 'blunt-nosed'\n",
      " 'bodies' 'body' 'boundaries' 'boundary' 'boundary-' 'boundary-layer'\n",
      " 'boundary-layer-control' 'boundary-value' 'bow' 'calculated'\n",
      " 'calculation' 'capable' 'case' 'cent' 'center,.but' 'characteristics'\n",
      " 'characteristics,' 'check' 'circulatory' 'classical' 'coefficient'\n",
      " 'coefficients' 'comparative' 'comparison' 'compression' 'computations'\n",
      " 'computed' 'concept' 'concluded' 'cone,' 'cones' 'configuration'\n",
      " 'consequently' 'consequently,' 'consider' 'considered' 'constant'\n",
      " 'conventional' 'coordinate' 'curved' 'curves' 'curves,' 'dealing'\n",
      " 'define' 'defined' 'depend' 'depending' 'destalling' 'determine'\n",
      " 'developed' 'differ' 'different' 'dimensional' 'diminish' 'discussed'\n",
      " 'discussion' 'distribution' 'distribution,' 'disturbance' 'drag' 'due'\n",
      " 'dynamic' 'ease' 'edge' 'edpm' 'effect' 'effects' 'efficient' 'either'\n",
      " 'embedded' 'emitting' 'empirical' 'enough' 'entire' 'entropy' 'equation'\n",
      " 'equations' 'equations,' 'evaluation' 'evidence,' 'exact' 'examination'\n",
      " 'example,' 'examples' 'exists' 'expected' 'experiment' 'experimental'\n",
      " 'factors' 'feature' 'ferri' 'field' 'fields' 'first,' 'flap' 'flare'\n",
      " 'flat' 'flight' 'flow' 'flows' 'fluid' 'found' 'fredholm' 'free'\n",
      " 'free-stream' 'friction,' 'general' 'generates' 'give' 'given' 'greater'\n",
      " 'growth' 'high-speed' 'hours,' 'hypersonic' 'ibm' 'impact' 'incident'\n",
      " 'incompressible' 'increase' 'increasing' 'increment' 'increment,'\n",
      " 'initial' 'inlet,' 'instance,' 'integral' 'integrated' 'intended'\n",
      " 'interesting' 'interference' 'internal' 'investigated' 'investigation'\n",
      " 'inviscid' 'involving' 'irrotational' 'is,' 'karman-pohlhausen' 'kind,'\n",
      " 'known' 'laminar' 'large-angled' 'layer' 'layers' 'leading' 'leads'\n",
      " 'libby' 'lift' 'lift,' 'linear' 'little' 'loading' 'local' 'locally'\n",
      " 'located' 'location' 'low' 'low-speed' 'lower' 'made' 'main' 'maximum'\n",
      " 'may' 'measurements' 'mentioned' 'method' 'methods' 'minutes' 'modified'\n",
      " 'mounted' 'must' 'necessary' 'need' 'needs' 'neumann' 'newtonian' 'non-'\n",
      " 'nonuniformity' 'normal-force' 'nose' 'novel' 'number' 'numbers'\n",
      " 'obtained' 'occur' 'one' 'order' 'original' 'outside' 'paper,' 'part'\n",
      " 'past' 'per' 'pitching-moment' 'plane' 'plate' 'points' 'possible'\n",
      " 'potential' \"prandtl's\" 'predict' 'predicts' 'presence' 'present'\n",
      " 'presented,' 'pressure' 'pressures' 'previously' 'problem' 'problems'\n",
      " 'process' 'produced' 'profile' 'programed' 'propeller' 'protrude'\n",
      " 'provided' 'purely' 'rae' 'ramp' 'range' 'ratio' 'ratios' 'recently'\n",
      " 'refinement' 'region' 'remaining' 'require' 'restricted' 'result'\n",
      " 'results' 'revert' 'revolution,' 'reynolds' 'rotational' 'satisfactorily'\n",
      " 'scope' 'second' 'secondary' 'section' 'seidel' 'set' 'several' 'shape'\n",
      " 'shape,' 'shear' 'shock' 'show' 'showed' 'shown' 'simple' 'simplest'\n",
      " 'single' 'situation' 'skin' 'slipstream' 'slopes' 'small' 'solid,'\n",
      " 'solid-body,' 'solution' 'solutions' 'solve' 'solved' 'solving'\n",
      " 'somewhat' 'source' 'span' 'spanwise' 'specific' 'speed' 'stabilizer'\n",
      " 'stabilizers' 'steady' 'still' 'stream' 'stream,' 'streams' 'study'\n",
      " 'substantial' 'subtracting' 'suction' 'suggests' 'supersonic'\n",
      " 'supporting' 'surface' 'symmetric' 'symmetry' 'takes' 'technique'\n",
      " 'theoretical' 'theory' 'thickness,' 'thin' 'things' 'three' 'time'\n",
      " 'together' 'traverses' 'treated' 'treatments' 'turbulent' 'two'\n",
      " 'two-dimensional' 'uniform' 'upon' 'used' 'useful' 'usually' 'value'\n",
      " 'values' 'variable' 'vary' 'velocities' 'velocity' 'viewed' 'viscosity'\n",
      " 'viscous' 'vorticity' 'wave' 'wave,' 'wedges' 'well' 'whose' 'wing'\n",
      " 'within']\n",
      "[(1, 'effect', 'D1', 1, 0.0795), (2, 'effect', 'D3', 1, 0.1193), (3, 'effect', 'D6', 1, 0.1193)]\n",
      "Termes uniques :  ['.' '..' '/destalling/' '0.1' '1' '1.5' '1.6x10' '10' '101' '3,.'\n",
      " '3.2x10' '300' '5' '704' 'accuracy' 'accurate' 'actual' 'aerodynamic'\n",
      " 'aerodynamic-centre' 'aerodynamics' 'agree' 'algebraic' 'almost' 'also'\n",
      " 'angles' 'another,' 'applicability' 'applied' 'approximate'\n",
      " 'approximation' 'approximation,' 'arbitrary' 'area' 'areas' 'arises,'\n",
      " 'attack' 'axial' 'axially' 'based' 'basis' 'behind' 'blunt-nosed'\n",
      " 'bodies' 'body' 'boundaries' 'boundary' 'boundary-' 'boundary-layer'\n",
      " 'boundary-layer-control' 'boundary-value' 'bow' 'calculated'\n",
      " 'calculation' 'capable' 'case' 'cent' 'center,.but' 'characteristics'\n",
      " 'characteristics,' 'check' 'circulatory' 'classical' 'coefficient'\n",
      " 'coefficients' 'comparative' 'comparison' 'compression' 'computations'\n",
      " 'computed' 'concept' 'concluded' 'cone,' 'cones' 'configuration'\n",
      " 'consequently' 'consequently,' 'consider' 'considered' 'constant'\n",
      " 'conventional' 'coordinate' 'curved' 'curves' 'curves,' 'dealing'\n",
      " 'define' 'defined' 'depend' 'depending' 'destalling' 'determine'\n",
      " 'developed' 'differ' 'different' 'dimensional' 'diminish' 'discussed'\n",
      " 'discussion' 'distribution' 'distribution,' 'disturbance' 'drag' 'due'\n",
      " 'dynamic' 'ease' 'edge' 'edpm' 'effect' 'effects' 'efficient' 'either'\n",
      " 'embedded' 'emitting' 'empirical' 'enough' 'entire' 'entropy' 'equation'\n",
      " 'equations' 'equations,' 'evaluation' 'evidence,' 'exact' 'examination'\n",
      " 'example,' 'examples' 'exists' 'expected' 'experiment' 'experimental'\n",
      " 'factors' 'feature' 'ferri' 'field' 'fields' 'first,' 'flap' 'flare'\n",
      " 'flat' 'flight' 'flow' 'flows' 'fluid' 'found' 'fredholm' 'free'\n",
      " 'free-stream' 'friction,' 'general' 'generates' 'give' 'given' 'greater'\n",
      " 'growth' 'high-speed' 'hours,' 'hypersonic' 'ibm' 'impact' 'incident'\n",
      " 'incompressible' 'increase' 'increasing' 'increment' 'increment,'\n",
      " 'initial' 'inlet,' 'instance,' 'integral' 'integrated' 'intended'\n",
      " 'interesting' 'interference' 'internal' 'investigated' 'investigation'\n",
      " 'inviscid' 'involving' 'irrotational' 'is,' 'karman-pohlhausen' 'kind,'\n",
      " 'known' 'laminar' 'large-angled' 'layer' 'layers' 'leading' 'leads'\n",
      " 'libby' 'lift' 'lift,' 'linear' 'little' 'loading' 'local' 'locally'\n",
      " 'located' 'location' 'low' 'low-speed' 'lower' 'made' 'main' 'maximum'\n",
      " 'may' 'measurements' 'mentioned' 'method' 'methods' 'minutes' 'modified'\n",
      " 'mounted' 'must' 'necessary' 'need' 'needs' 'neumann' 'newtonian' 'non-'\n",
      " 'nonuniformity' 'normal-force' 'nose' 'novel' 'number' 'numbers'\n",
      " 'obtained' 'occur' 'one' 'order' 'original' 'outside' 'paper,' 'part'\n",
      " 'past' 'per' 'pitching-moment' 'plane' 'plate' 'points' 'possible'\n",
      " 'potential' \"prandtl's\" 'predict' 'predicts' 'presence' 'present'\n",
      " 'presented,' 'pressure' 'pressures' 'previously' 'problem' 'problems'\n",
      " 'process' 'produced' 'profile' 'programed' 'propeller' 'protrude'\n",
      " 'provided' 'purely' 'rae' 'ramp' 'range' 'ratio' 'ratios' 'recently'\n",
      " 'refinement' 'region' 'remaining' 'require' 'restricted' 'result'\n",
      " 'results' 'revert' 'revolution,' 'reynolds' 'rotational' 'satisfactorily'\n",
      " 'scope' 'second' 'secondary' 'section' 'seidel' 'set' 'several' 'shape'\n",
      " 'shape,' 'shear' 'shock' 'show' 'showed' 'shown' 'simple' 'simplest'\n",
      " 'single' 'situation' 'skin' 'slipstream' 'slopes' 'small' 'solid,'\n",
      " 'solid-body,' 'solution' 'solutions' 'solve' 'solved' 'solving'\n",
      " 'somewhat' 'source' 'span' 'spanwise' 'specific' 'speed' 'stabilizer'\n",
      " 'stabilizers' 'steady' 'still' 'stream' 'stream,' 'streams' 'study'\n",
      " 'substantial' 'subtracting' 'suction' 'suggests' 'supersonic'\n",
      " 'supporting' 'surface' 'symmetric' 'symmetry' 'takes' 'technique'\n",
      " 'theoretical' 'theory' 'thickness,' 'thin' 'things' 'three' 'time'\n",
      " 'together' 'traverses' 'treated' 'treatments' 'turbulent' 'two'\n",
      " 'two-dimensional' 'uniform' 'upon' 'used' 'useful' 'usually' 'value'\n",
      " 'values' 'variable' 'vary' 'velocities' 'velocity' 'viewed' 'viscosity'\n",
      " 'viscous' 'vorticity' 'wave' 'wave,' 'wedges' 'well' 'whose' 'wing'\n",
      " 'within']\n",
      "[(1, 'distribution', 'D1', 1, 0.0663), (2, 'distribution', 'D3', 1, 0.0995), (3, 'distribution', 'D4', 1, 0.0332), (4, 'distribution', 'D6', 1, 0.0995)]\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ryan\\anaconda3\\envs\\ML\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    app = QApplication(sys.argv)\n",
    "    app.setStyleSheet(\"\"\"\n",
    "    QMainWindow {\n",
    "        background-color: #f5f5f5;\n",
    "    }\n",
    "    QLabel {\n",
    "        color: #333333;\n",
    "        font-size: 14px;\n",
    "    }\n",
    "    QLineEdit {\n",
    "        border: 1px solid #CCCCCC;\n",
    "        border-radius: 5px;\n",
    "        padding: 5px;\n",
    "    }\n",
    "    QPushButton {\n",
    "        background-color: #4CAF50;\n",
    "        color: white;\n",
    "        font-size: 14px;\n",
    "        padding: 5px 10px;\n",
    "        border-radius: 5px;\n",
    "    }\n",
    "    QPushButton:hover {\n",
    "        background-color: #45a049;\n",
    "    }\n",
    "    \n",
    "    QRadioButton {\n",
    "        font-size: 13px;\n",
    "    }\n",
    "    QGroupBox {\n",
    "        font-size: 15px;\n",
    "        color: #333333;\n",
    "        border: 1px solid #CCCCCC;\n",
    "        border-radius: 8px;\n",
    "        margin-top: 10px;\n",
    "        padding: 10px;\n",
    "    }\n",
    "    QTextEdit {\n",
    "        background-color: #f0f0f0;\n",
    "        border: 1px solid #CCCCCC;\n",
    "        border-radius: 5px;\n",
    "        padding: 5px;\n",
    "    }\n",
    "    QTableWidget {\n",
    "        background-color: #FFFFFF;\n",
    "        border: 1px solid #CCCCCC;\n",
    "        border-radius: 5px;\n",
    "        padding: 2px;\n",
    "        gridline-color: #E0E0E0;\n",
    "    }\n",
    "    QTableWidget::item {\n",
    "        padding: 5px;\n",
    "        border-bottom: 1px solid #E0E0E0;\n",
    "    }\n",
    "    QHeaderView::section {\n",
    "        background-color: #f0f0f0;\n",
    "        padding: 5px;\n",
    "        border: 1px solid #CCCCCC;\n",
    "        font-weight: bold;\n",
    "    }\n",
    "\n",
    "    QLabel {\n",
    "        margin-left: 20px;\n",
    "        background-color: transparent;\n",
    "        border: none;\n",
    "        font-size: 14px;\n",
    "        font-family: Arial, sans-serif;\n",
    "    }\n",
    "    \n",
    "\"\"\")\n",
    "\n",
    "    window = SearchApp()\n",
    "    window.show()\n",
    "    sys.exit(app.exec_())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
